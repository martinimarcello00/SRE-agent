# 2025-09-08 12:03:09 - Hotel Missconf fetch incident (already in RAG)

    # üìù Results of the Analysis

## ü©∫ Symptoms
- Pod test-hotel-reservation/geo-6b4b89b5f5-d74mv in namespace test-hotel-reservation is CrashLoopBackOff (0/1) with ~17 restarts
- Container hotel-reserv-geo repeatedly terminates shortly after start (~11s uptime) with Exit Code 2
- Pod scheduled successfully (node kind-worker) and image pulled ‚Äî not a scheduling/image issue
- Application logs show panic: "no reachable servers" while attempting to connect to mongodb-geo:27777
- Initial log fetch failed due to wrong pod name but subsequent logs from geo-6b4b89b5f5-d74mv confirmed the DB connection error
- Incident store document indicates MongoDB actually listens on port 27017 and requires authentication (port/auth mismatch)

## üîç Steps Performed
- Executed kubectl_get to list all pods across namespaces (kubectl get pods --all-namespaces -o wide) and collected pod statuses.
- Ran kubectl_describe on pod 'geo' in namespace 'test-hotel-reservation' (kubectl_describe) to inspect container state and events.
- Ran kubectl_logs to tail 500 lines from container 'hotel-reserv-geo' in pod 'geo' (namespace test-hotel-reservation); the command failed with "Resource pod geo not found".
- Ran kubectl_get (kubectl get pods --all-namespaces -o wide) to list pods and obtain the exact pod name and status.
- Ran kubectl_describe on pod 'geo-6b4b89b5f5-d74mv' in namespace 'test-hotel-reservation' to inspect container state and events.
- Ran kubectl_logs to fetch the last 500 lines from pod geo-6b4b89b5f5-d74mv in namespace test-hotel-reservation.
- Queried the incident store with chroma_query_documents to retrieve a stored analysis confirming the MongoDB port/auth mismatch and suggested mitigations.

## üí° Insights Gathered
- The pod test-hotel-reservation/geo is in CrashLoopBackOff (0/1) with 16 restarts (last restart ~4m52s ago) ‚Äî the only failing pod in the cluster, indicating a repeated startup/crash symptom likely due to an application or init error.
- The container hotel-reserv-geo is repeatedly crashing with LastState Terminated (Reason: Error) Exit Code 2, producing a CrashLoopBackOff despite successful image pulls ‚Äî indicates an application startup/runtime error (not an image or scheduling issue); next step: fetch container logs to see the error output.
- Attempt to fetch container logs failed: kubectl returned "Resource pod geo not found" ‚Äî the pod cannot be located (may have been deleted/restarted with a different name), so we currently cannot access the crashing container's logs.
- The failing pod is present as test-hotel-reservation/geo-6b4b89b5f5-d74mv (0/1, CrashLoopBackOff) with 17 restarts (last restart ~20s ago) ‚Äî an active crashloop. We can now fetch logs using that exact pod name to diagnose the startup error.
- The container hotel-reserv-geo in pod geo-6b4b89b5f5-d74mv is repeatedly crashing very quickly: Last start ~11:59:58 and finish ~12:00:09 (~11s uptime) with Exit Code 2 and RestartCount 17, now in CrashLoopBackOff. Image pull succeeded and the pod is Running on node kind-worker ‚Äî this points to an application startup/runtime failure (likely bad config, missing dependency, or runtime error).
- Application log shows a panic: "no reachable servers" while initializing the database connection to mongodb-geo:27777 ‚Äî the geo service cannot reach its MongoDB, causing the CrashLoopBackOff.
- Application is crashing on startup because it cannot reach MongoDB: logs show a panic with 'no reachable servers' while trying mongodb-geo:27777. A retrieved incident document confirms MongoDB actually listens on 27017 and requires authentication ‚Äî the wrong port/credentials are the root cause of the CrashLoopBackOff.

## üö® Final Report (Root Cause)
> Diagnosis:
The geo service is crashing on startup because it is misconfigured to connect to MongoDB on the wrong port (27777) and lacking proper authentication/credentials; this causes connection failures (no reachable servers) and the application to panic and exit with code 2, resulting in a CrashLoopBackOff.

Reasoning:
Kubernetes pod status and restart counts show the geo container repeatedly fails on startup with Exit Code 2, which points to an application runtime error. The pod is scheduled and the image is pulled successfully, so it's not a scheduling or image issue. Application logs explicitly show a panic 'no reachable servers' while attempting to connect to mongodb-geo:27777. A prior incident document retrieved from the incident store confirms MongoDB listens on the default 27017 and requires authentication. Combining these facts indicates the geo service is configured with the wrong MongoDB port and likely missing credentials; the service cannot establish a DB connection, panics during initialization, and crashes repeatedly, producing the observed CrashLoopBackOff.

## üõ†Ô∏è Mitigation Plan Strategy
Fix geo's MongoDB connection by correcting the port to 27017 and supplying credentials via a Kubernetes Secret; perform a rolling restart and verify. If immediate config change is not possible, use a short-lived service port mapping (mongodb-geo:27777 ‚Üí 27017) as a temporary workaround. After recovery, apply remediation: Secrets, RBAC, CI checks, and improved startup resilience.

## üìã Detailed Mitigation Steps
1. Confirm namespace and current state: kubectl get pods,svc,deploy -n <NAMESPACE> -o wide; identify geo and mongodb-geo pod names.
2. Inspect failing geo pod and logs: kubectl describe pod <geo-pod-name> -n <NAMESPACE>; kubectl logs <geo-pod-name> -n <NAMESPACE> -c hotel-reserv-geo --tail=200.
3. Verify MongoDB pod/service: kubectl get pod -l app=mongodb-geo -n <NAMESPACE> -o wide; kubectl describe pod <mongodb-pod-name> -n <NAMESPACE>; kubectl get svc mongodb-geo -n <NAMESPACE> -o yaml; kubectl logs <mongodb-pod-name> -n <NAMESPACE> --tail=200.
4. Inspect geo Deployment/ConfigMap to find how DB connection is configured: kubectl get deployment geo -n <NAMESPACE> -o yaml; kubectl get configmap -n <NAMESPACE> | grep -i geo.
5. Create or verify Kubernetes Secret for MongoDB credentials (do NOT store creds in ConfigMap): kubectl create secret generic mongodb-geo-creds --from-literal=MONGO_USER='<user>' --from-literal=MONGO_PASSWORD='<pass>' -n <NAMESPACE> OR --from-literal=MONGO_URI='mongodb://<user>:<pass>@mongodb-geo:27017/<db>?authSource=admin'.
6. Update geo deployment to use correct MongoDB host/port and inject credentials from the Secret: set MONGO_HOST=mongodb-geo and MONGO_PORT=27017 or set MONGO_URI via secretKeyRef in deployment env (kubectl edit deployment geo -n <NAMESPACE> or use kubectl patch/set env).
7. Perform a rolling restart to apply config/secret changes: kubectl rollout restart deployment/geo -n <NAMESPACE>; monitor rollout: kubectl rollout status deployment/geo -n <NAMESPACE>.
8. Monitor new geo pods and logs for successful DB connection and absence of panic: kubectl get pods -l app=geo -n <NAMESPACE> -w; kubectl logs <new-geo-pod> -n <NAMESPACE> -c hotel-reserv-geo --follow.
9. Test connectivity and credentials from a debug pod if needed: kubectl run --rm -it mongo-client --image=mongo --namespace=<NAMESPACE> -- bash then mongo --host mongodb-geo --port 27017 -u <user> -p '<pass>' --authenticationDatabase admin OR run nc from a pod to test TCP connectivity (nc -vz mongodb-geo 27017).
10. Temporary workaround (only if you cannot change geo config quickly): add a short-lived service port mapping so mongodb-geo:27777 maps to targetPort 27017: kubectl patch svc mongodb-geo -n <NAMESPACE> --type='json' -p='[{"op":"add","path":"/spec/ports/-","value":{"name":"mongo-alt","port":27777,"targetPort":27017,"protocol":"TCP"}}]'. Remove this mapping once geo is fixed.
11. If authentication failures persist, verify/create the MongoDB user with appropriate roles using the mongo shell in the mongodb pod (follow your security process). Example: kubectl exec -it <mongodb-pod> -n <NAMESPACE> -- mongo admin -u <admin> -p '<admin-pass>' --eval "db.getSiblingDB('<db>').createUser({user:'<user>',pwd:'<pass>',roles:[{role:'readWrite',db:'<db>'}]})".
12. If a change causes regressions, rollback geo: kubectl rollout undo deployment/geo -n <NAMESPACE>. Revert any temporary svc modifications by re-applying original service manifest or removing the temp port.
13. Validate recovery: ensure geo pods are Running and Ready with no CrashLoopBackOff: kubectl get pods -n <NAMESPACE> | grep geo; verify readiness/liveness endpoints and application-level functionality.
14. Post-incident remediation: move DB creds to Secrets (not ConfigMaps), restrict Secret RBAC, add CI/manifest checks for required DB env vars and secret usage, and harden geo startup (retries/backoff instead of panic on DB failure).
15. Rotate credentials if they were exposed in logs or ConfigMaps during the incident and document the incident steps, root cause, remediation, and timeline in your incident tracker.
