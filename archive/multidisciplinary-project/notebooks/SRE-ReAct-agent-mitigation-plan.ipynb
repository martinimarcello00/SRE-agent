{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# SRE ReAct agent with custom schema and RAG (to store mitigation plans)\n",
    "\n",
    "This notebook implements an intelligent Site Reliability Engineering (SRE) agent that automatically diagnoses Kubernetes incidents and generates mitigation plans. The agent uses a ReAct (Reasoning + Acting) framework powered by LangGraph to:\n",
    "\n",
    "- **Investigate incidents** using kubectl tools through MCP (Model Context Protocol) servers\n",
    "- **Analyze cluster state** and extract meaningful insights from Kubernetes resources\n",
    "- **Generate structured diagnosis** with detailed reasoning and root cause analysis\n",
    "- **Create mitigation plans** by leveraging ChromaDB RAG to search for similar past incidents\n",
    "- **Provide comprehensive reports** with step-by-step remediation guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Set env variables from file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromaDB_path = os.environ.get(\"CHROMADB_STORAGE_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Testing the RAG (ChromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import PersistentClient\n",
    "\n",
    "chroma_client = PersistentClient(path=chromaDB_path)\n",
    "\n",
    "incidents_collection = chroma_client.get_or_create_collection(\"incidents\")\n",
    "\n",
    "# Get all the docs in the collection\n",
    "incidents_collection.peek()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Building SRE ReAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "gpt5 = ChatOpenAI(model=\"gpt-5\")\n",
    "gpt5mini = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    google_api_key=os.getenv(\"GOOGLEAI_API_KEY\") # Google AI Studio free api key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "chromaDB_path = os.environ.get(\"CHROMADB_STORAGE_PATH\")\n",
    "\n",
    "prometheus_URL = os.environ.get(\"PROMETHEUS_SERVER_URL\")\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"kubernetes\" : { # https://github.com/Flux159/mcp-server-kubernetes\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\"mcp-server-kubernetes\"],\n",
    "            \"transport\": \"stdio\",\n",
    "            \"env\": {\n",
    "                \"ALLOW_ONLY_NON_DESTRUCTIVE_TOOLS\": \"true\"\n",
    "            }\n",
    "        },\n",
    "        \"chroma\": { # https://github.com/chroma-core/chroma-mcp\n",
    "            \"command\": \"uvx\",\n",
    "            \"transport\": \"stdio\",\n",
    "            \"args\": [\n",
    "                \"chroma-mcp\",\n",
    "                \"--client-type\",\n",
    "                \"persistent\",\n",
    "                \"--data-dir\",\n",
    "                str(chromaDB_path)\n",
    "            ]\n",
    "        },\n",
    "        \"prometheus\": { # https://github.com/idanfishman/prometheus-mcp\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\"prometheus-mcp@latest\", \"stdio\"],\n",
    "            \"transport\": \"stdio\",\n",
    "            \"env\": {\n",
    "                \"PROMETHEUS_URL\": str(prometheus_URL)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# NB: The cluster has to be running otherwise the tools won't be available\n",
    "mcp_tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all the tools\n",
    "for tool in mcp_tools:\n",
    "    print(f\"üîß {tool.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools allowed\n",
    "tools_allowed = [\"kubectl_get\", \"kubectl_describe\", \"kubectl_logs\", \"explain_resource\", \"list_api_resources\", \"ping\"]\n",
    "\n",
    "k8s_tools = []\n",
    "chroma_tools = []\n",
    "prometheus_tools = []\n",
    "\n",
    "for tool in mcp_tools:\n",
    "    # Remove not allowed MCP tools\n",
    "    if tool.name in tools_allowed:\n",
    "        k8s_tools.append(tool)\n",
    "    # Create a custom list for ChromaDB tools\n",
    "    elif \"chroma\" in  tool.name:\n",
    "        chroma_tools.append(tool)\n",
    "    # Create a custom list of Prometheus tools\n",
    "    elif \"prometheus\" in tool.name:\n",
    "        prometheus_tools.append(tool)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of kubernetes tools\n",
    "for tool in k8s_tools:\n",
    "    print(f\"üîß {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of chromaDB tools\n",
    "for tool in chroma_tools:\n",
    "    print(f\"üîß {tool.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of prometheus tools\n",
    "for tool in prometheus_tools:\n",
    "    print(f\"üîß {tool.name} : {tool.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Literal, Annotated\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "import operator\n",
    "\n",
    "class SREAgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    app_summary: str\n",
    "    insights: Annotated[list[str], operator.add]\n",
    "    prev_steps: Annotated[list[str], operator.add]\n",
    "    response: str\n",
    "    final_output: str\n",
    "    mitigation_plan_overview: str\n",
    "    mitigation_steps: List[str]\n",
    "    has_already_happened: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sre_agent_prompt = \"\"\"\n",
    "    You are an expert DevOps engineer who has been tasked with detecting anomalies in a deployed service.\n",
    "\n",
    "    The service you are working with today is described below:\n",
    "    {app_summary}\n",
    "\n",
    "    You will use an MCP server which will provide you access to the Kubernetes cluster and prometheus server.\n",
    "\n",
    "    Context:\n",
    "\n",
    "    *Previous Steps:*\n",
    "    {prev_steps}\n",
    "\n",
    "    *Insights:*\n",
    "    {insights}\n",
    "\n",
    "    Your task:\n",
    "        1. Begin by analyzing the service's state and telemetry using kubectl and prometheus tools\n",
    "        2. When you have identified the issue, call the submit_final_diagnosis tool with:\n",
    "            - diagnosis: Describe the issue you have identified (without fixing it)\n",
    "            - reasoning: Explain your reasoning and thought process behind the solution\n",
    "\n",
    "    IMPORTANT: You must call submit_final_diagnosis when you're ready to conclude your investigation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_prompt = \"\"\"\n",
    "    You are an autonomous SRE agent for Kubernetes incident diagnosis.\n",
    "\n",
    "    Context:\n",
    "\n",
    "    Previous Insights: \n",
    "    {insights}\n",
    "    \n",
    "    Previous Steps:\n",
    "    {prev_steps}\n",
    "\n",
    "    Below are the latest two messages:\n",
    "    {last_two_messages}\n",
    "\n",
    "    Instructions:\n",
    "    1. From the latest two messages, extract the most important new insight relevant for incident diagnosis or mitigation. Summarize it concisely.\n",
    "    2. Write a concise description of only the most recent action taken including the tool used (not the whole list).  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_insights_str(state):\n",
    "    \"\"\"Return a string with the formatted list of insights gathered during exploration\"\"\"\n",
    "    if len(state[\"insights\"]) > 0:\n",
    "        return \"\\n- \".join([\"\"] + state[\"insights\"])\n",
    "    else:\n",
    "        return \"No insights yet\"\n",
    "    \n",
    "def get_prev_steps_str(state):\n",
    "    \"\"\"Return a string with the formatted list of previous steps performed during exploration\"\"\"\n",
    "    if len(state[\"prev_steps\"]) > 0:\n",
    "        return \"\\n- \".join([\"\"] + state[\"prev_steps\"])\n",
    "    else:\n",
    "        return \"No previous steps yet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Extract insights node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class UpdateAgentData(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a step performed by the SRE agent.\n",
    "    \"\"\"\n",
    "    insight: str = Field(..., description=\"Most important new finding\")\n",
    "    prev_step: str = Field(..., description=\"Concise description of the most recent action taken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_struct_output = gpt5mini.with_structured_output(UpdateAgentData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Node used to summarise the infos given the two previous messages\n",
    "async def summarise(state: SREAgentState):\n",
    "\n",
    "    # Gather last two messages (tool call + tool response)\n",
    "    last_messages = state[\"messages\"][-2:]\n",
    "\n",
    "    insights_str = get_insights_str(state)\n",
    "    prev_step_str = get_prev_steps_str(state)\n",
    "\n",
    "    prompt = HumanMessage(content=summarise_prompt.format(\n",
    "        prev_steps = prev_step_str,\n",
    "        insights=insights_str,\n",
    "        last_two_messages=last_messages))\n",
    "\n",
    "    data = llm_with_struct_output.invoke([prompt])\n",
    "\n",
    "    return {\"insights\" : [data.insight], \"prev_steps\" : [data.prev_step]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### SRE Agent node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "\n",
    "#¬†Tool used to submit the final response\n",
    "@tool\n",
    "def submit_final_diagnosis(\n",
    "    diagnosis: str, \n",
    "    reasoning: str,\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> Command:\n",
    "    \"\"\"\n",
    "    Submit the final diagnosis when investigation is complete.\n",
    "    \n",
    "    Args:\n",
    "        diagnosis: The issue you have identified (without fixing it)\n",
    "        reasoning: Your reasoning and thought process behind the diagnosis\n",
    "    \n",
    "    Returns:\n",
    "        Command to update state and end workflow\n",
    "    \"\"\"\n",
    "    final_response = f\"Diagnosis:\\n{diagnosis}\\n\\nReasoning:\\n{reasoning}\"\n",
    "    \n",
    "    return Command(\n",
    "        update={\n",
    "            \"response\": final_response, # Add in the final graph state the final answer\n",
    "            \"messages\": [\n",
    "                ToolMessage(\n",
    "                    content=\"Final diagnosis submitted successfully. Investigation complete.\",\n",
    "                    tool_call_id=tool_call_id\n",
    "                )\n",
    "            ]\n",
    "        },\n",
    "        goto=\"format-output\" # End the loop cycle\n",
    "    )\n",
    "\n",
    "# Append the tool for submission to the list of tools (MCP servers)\n",
    "completion_tool = submit_final_diagnosis\n",
    "sre_agent_tools = k8s_tools + prometheus_tools + [completion_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def sreAgent(state: SREAgentState):\n",
    "\n",
    "    insights_str = get_insights_str(state)\n",
    "    prev_step_str = get_prev_steps_str(state)\n",
    "\n",
    "    prompt = HumanMessage(content=sre_agent_prompt.format(\n",
    "        prev_steps=prev_step_str, \n",
    "        insights=insights_str, \n",
    "        app_summary=state[\"app_summary\"]\n",
    "    ))\n",
    "\n",
    "    # Use tools with completion (for the submission)\n",
    "    llm_with_completion_tools = gpt5mini.bind_tools(sre_agent_tools, parallel_tool_calls=False)\n",
    "    return {\"messages\": [llm_with_completion_tools.invoke([prompt])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sre_agent_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def format_response(state: SREAgentState):\n",
    "    insights_str = get_insights_str(state)\n",
    "    prev_step_str = get_prev_steps_str(state)\n",
    "\n",
    "    message = \"# üìù Results of the Analysis\\n\\n\"\n",
    "\n",
    "    # Steps performed\n",
    "    message += \"## üîç Steps Performed\\n\"\n",
    "    message += prev_step_str.strip() + \"\\n\\n\"\n",
    "\n",
    "    # Insights\n",
    "    message += \"## üí° Insights Gathered\\n\"\n",
    "    message += insights_str.strip() + \"\\n\\n\"\n",
    "\n",
    "    # Final root cause\n",
    "    message += \"## üö® Final Report (Root Cause)\\n\"\n",
    "    message += f\"> {state['response'].strip()}\\n\\n\"\n",
    "\n",
    "    # Mitigation overview\n",
    "    message += \"## üõ†Ô∏è Mitigation Plan Strategy\\n\"\n",
    "    message += f\"{state['mitigation_plan_overview'].strip()}\\n\\n\"\n",
    "\n",
    "    # Detailed mitigation steps\n",
    "    message += \"## üìã Detailed Mitigation Steps\\n\"\n",
    "    for i, step in enumerate(state[\"mitigation_steps\"], start=1):\n",
    "        message += f\"{i}. {step}\\n\"\n",
    "\n",
    "    return {\"final_output\": message}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Elaborate mitigation plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MitigationPlanResponse(BaseModel):\n",
    "    mitigation_steps: List[str] = Field(..., description=\"List of steps to be executed to mitigate the current incident\")\n",
    "    mitigation_plan_overview: str = Field(..., description=\"Short overview of the mitigation plan to be executed\")\n",
    "    is_previous_incident: bool = Field(..., description=\"True if the mitigation plan was found in the incidetns colletionc (incident already happened)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigation_planner_prompt = \"\"\"\n",
    "    You're a React agent developed using the LangGraph prebuilt agent framework, in charge of creating a mitigation plan to solve an incident in a deployed Kubernetes infrastructure.\n",
    "    \n",
    "    Important: do not execute the mitigation plan, just explain all the steps.\n",
    "\n",
    "    The root cause will be provided and then you should:\n",
    "        1. Look using the ChromaDB tools if you find any similar incident in the 'incidents' collection.\n",
    "        2. If so, set \"is_previous_incident\" to True and return the mitigation plan previously discovered if it fits for the current problem.\n",
    "        3. If not, create a custom mitigation plan. Use Kubernetes tools only to check for more information, but try to avoid using them unless necessary.\n",
    "    \n",
    "    Report of the incident:\n",
    "    {incident_report}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigation_agent_tools = k8s_tools + chroma_tools\n",
    "mitigation_agent_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "async def mitigation_planner(state: SREAgentState):\n",
    "\n",
    "    # Create a React agent\n",
    "    mitigation_agent = create_react_agent(\n",
    "        name = \"MitigationPlanGenerator\",\n",
    "        model = gpt5mini,\n",
    "        tools = mitigation_agent_tools,\n",
    "        prompt= mitigation_planner_prompt.format(incident_report = state[\"response\"]),\n",
    "        response_format = MitigationPlanResponse\n",
    "    )\n",
    "\n",
    "    mitigation_response = await mitigation_agent.ainvoke({})\n",
    "\n",
    "    return {\n",
    "        \"mitigation_plan_overview\" : mitigation_response['structured_response'].mitigation_plan_overview,\n",
    "        \"mitigation_steps\" : mitigation_response['structured_response'].mitigation_steps,\n",
    "        \"has_already_happened\" : mitigation_response['structured_response'].is_previous_incident\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def store_incident_report(state: SREAgentState):\n",
    "    \"\"\"Store the incident report with the mitigation plan in chromaDB incidents collection\"\"\"\n",
    "\n",
    "    from chromadb import PersistentClient\n",
    "    import uuid\n",
    "\n",
    "    chroma_client = PersistentClient(path=chromaDB_path)\n",
    "\n",
    "    incidents_collection = chroma_client.get_or_create_collection(\"incidents\")\n",
    "\n",
    "    incidents_collection.add(\n",
    "        ids=[str(uuid.uuid4())],\n",
    "        documents=state[\"final_output\"],\n",
    "    )\n",
    "\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Assemble the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(SREAgentState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"sre-agent\", sreAgent)\n",
    "builder.add_node(\"tools\", ToolNode(sre_agent_tools)) # Tool node is executing the tool called in the previous message\n",
    "builder.add_node(\"summarise\", summarise) # Node to reduce the raw data into a schema\n",
    "builder.add_node(\"generate-mitigation-plan\", mitigation_planner)\n",
    "builder.add_node(\"format-output\", format_response)\n",
    "builder.add_node(\"store-incident-report\", store_incident_report)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"sre-agent\")\n",
    "\n",
    "# Conditional edge from sre-agent\n",
    "builder.add_conditional_edges(\n",
    "    \"sre-agent\",\n",
    "    #Use in the conditional_edge to route to the ToolNode if the last message has tool calls. Otherwise, route to the end.\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "# After tools, decide whether to summarise or end\n",
    "def after_tools_condition(state: SREAgentState):\n",
    "    # If response is filled, investigation is complete (end of the workflow)\n",
    "    if state.get(\"response\"):\n",
    "        return \"generate-mitigation-plan\"\n",
    "    return \"summarise\"\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"tools\",\n",
    "    after_tools_condition,\n",
    "    {\n",
    "        \"summarise\": \"summarise\",\n",
    "        \"generate-mitigation-plan\": \"generate-mitigation-plan\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# If is a new incident, store in VectorDB\n",
    "def has_already_happened(state: SREAgentState):\n",
    "    return bool(state['has_already_happened'])\n",
    "    \n",
    "builder.add_conditional_edges(\n",
    "    \"format-output\",\n",
    "    has_already_happened,\n",
    "    {\n",
    "        True: END,\n",
    "        False: \"store-incident-report\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# After summarise, continue investigation (go to sre-agent)\n",
    "builder.add_edge(\"summarise\", \"sre-agent\")\n",
    "builder.add_edge(\"generate-mitigation-plan\", \"format-output\")\n",
    "builder.add_edge(\"format-output\", END)\n",
    "builder.add_edge(\"store-incident-report\", END)\n",
    "\n",
    "# Compile the graph\n",
    "structured_graph = builder.compile()\n",
    "\n",
    "# Show the graph\n",
    "display(Image(structured_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(structured_graph.get_graph(xray=True).draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def save_markdown_file(structured_result, trace_name):\n",
    "    \n",
    "    # Get the output path from environment variable\n",
    "    output_path = os.getenv(\"RESULTS_PATH\")\n",
    "    if not output_path:\n",
    "        raise ValueError(\"Environment variable not set\")\n",
    "    \n",
    "\n",
    "    # Generate timestamp for filename and title\n",
    "    now = datetime.now()\n",
    "    filename_timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    title_timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Create filename\n",
    "    filename = f\"{filename_timestamp}.md\"\n",
    "    filepath = output_path + \"/\" + filename\n",
    "    \n",
    "    # Get markdown content\n",
    "    markdown_content = structured_result.get(\"final_output\", \"\")\n",
    "    \n",
    "    # Create the complete file content with title header\n",
    "    file_content = f\"\"\"# {title_timestamp} - {trace_name}\n",
    "\n",
    "    {markdown_content}\"\"\"\n",
    "    \n",
    "    # Write to file\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(file_content)\n",
    "    \n",
    "    print(f\"Markdown file saved: {filepath}\")\n",
    "    return str(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "async def test_structured_graph(graph, app_summary:str, human_message: str = \"\", trace_name: str = None):\n",
    "    \"\"\"Test the structured graph with SREAgentState\"\"\"\n",
    "    # Create initial state with SREAgentState structure\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=human_message)] if human_message else [],\n",
    "        \"insights\": [],\n",
    "        \"prev_steps\": [],\n",
    "        \"response\": \"\",\n",
    "        \"final_output\" : \"\",\n",
    "        \"app_summary\" : app_summary,\n",
    "        \"mitigation_plan_overview\" : \"\",\n",
    "        \"mitigation_steps\" : [],\n",
    "        \"has_already_happened\" : False\n",
    "    }\n",
    "    \n",
    "    # Start time tracking\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Configuration for the graph execution\n",
    "    config = {\"recursion_limit\": 50}\n",
    "    if trace_name:\n",
    "        config[\"run_name\"] = trace_name\n",
    "    \n",
    "    # Invoke the graph asynchronously\n",
    "    result = await graph.ainvoke(initial_state, config)\n",
    "\n",
    "    # Calculate execution time\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    save_markdown_file(result, trace_name)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = input(\"Enter experiment name: \")\n",
    "\n",
    "if experiment_name.strip() == \"\":\n",
    "    experiment_name = \"SRE structured format agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_summary = \"\"\"\n",
    "    The application implements a hotel reservation service, build with Go and gRPC, and starting from the open-source project https://github.com/harlow/go-micro-services. The initial project is extended in several ways, including adding back-end in-memory and persistent databases, adding a recommender system for obtaining hotel recommendations, and adding the functionality to place a hotel reservation. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "human = \"\"\n",
    "structured_result = await test_structured_graph(structured_graph, app_summary, human, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(structured_result[\"final_output\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Analysis of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "for i, msg in enumerate(structured_result['messages']):\n",
    "    if isinstance(msg, AIMessage):\n",
    "        print(f'{i}) {msg.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"]}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_result['messages'][12].additional_kwargs[\"tool_calls\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "def count_tool_calls(messages):\n",
    "    \"\"\"\n",
    "    Plots a bar chart of tool call occurrences by tool name from state['messages'].\n",
    "    \"\"\"\n",
    "    # Extract tool names from ToolMessage objects\n",
    "    tool_calls = []\n",
    "    for msg in messages:\n",
    "\n",
    "        if isinstance(msg, AIMessage):\n",
    "            if hasattr(msg, 'additional_kwargs'):\n",
    "                if \"tool_calls\" in msg.additional_kwargs:\n",
    "                    for call in msg.additional_kwargs['tool_calls']:\n",
    "                        if \"function\" in call:\n",
    "                            if \"name\" in call[\"function\"]:\n",
    "                                tool_calls.append(call[\"function\"][\"name\"])\n",
    "\n",
    "    # Count occurrences\n",
    "    counts = Counter(tool_calls)\n",
    "\n",
    "    return dict(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calls = count_tool_calls(structured_result[\"messages\"])\n",
    "tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a better color palette with purple shades\n",
    "colors = ['#7400B8', '#6930C3', '#5E60CE', '#5390D9', '#4EA8DE', '#48BFE3'][:len(tool_calls)]\n",
    "# Create labels with tool name and count\n",
    "labels = [f\"{tool} ({count})\" for tool, count in tool_calls.items()]\n",
    "\n",
    "# Create an explode effect for the largest value\n",
    "values = list(tool_calls.values())\n",
    "explode = [0.1 if v == max(values) else 0 for v in values]\n",
    "\n",
    "# Create the pie chart with improvements\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    values, \n",
    "    explode=explode,\n",
    "    labels=labels, \n",
    "    colors=colors,\n",
    "    autopct='%1.1f%%',\n",
    "    shadow=True,\n",
    "    startangle=90,\n",
    "    textprops={'fontsize': 12}\n",
    ")\n",
    "\n",
    "# Make the percentage text more visible\n",
    "for autotext in autotexts:\n",
    "    autotext.set_color('white')\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "plt.title('Tool Calls Distribution', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in structured_result[\"prev_steps\"]:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for insight in structured_result[\"insights\"]:\n",
    "    print(insight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(structured_result[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(structured_result[\"mitigation_plan_overview\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in structured_result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sre-agent-QNf2KdVb-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
