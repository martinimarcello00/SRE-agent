{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# SRE ReAct Agent\n",
    "\n",
    "This SRE agent uses **LangGraph** to autonomously investigate Kubernetes cluster issues. It connects to a **Model Context Protocol (MCP) server** for Kubernetes access, allowing an **LLM (GPT-5 mini)** to use kubectl commands as tools. The agent follows a **ReAct pattern** (Reason + Act) - it analyzes the hotel reservation service, calls K8s tools when needed, and provides diagnostic reports without human intervention.\n",
    "\n",
    "**Key components:**\n",
    "- **MCP Client**: Provides kubectl tools to the LLM\n",
    "- **LangGraph**: Orchestrates the investigation workflow  \n",
    "- **ReAct Loop**: Agent â†’ Tools â†’ Agent until diagnosis complete\n",
    "\n",
    "The agent automatically starts investigating when triggered by the system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "#gpt5 = ChatOpenAI(model=\"gpt-5\")\n",
    "gpt5mini = ChatOpenAI(model=\"gpt-5-mini\")\n",
    "# gemini = ChatGoogleGenerativeAI(\n",
    "#     model=\"gemini-2.5-flash\",\n",
    "#     google_api_key=os.getenv(\"GOOGLEAI_API_KEY\") # Google AI Studio free api key\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Testing MCP Client for Kubernetes\n",
    "\n",
    "MCP K8s server: [mcp-server-kubernetes](https://github.com/Flux159/mcp-server-kubernetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"kubernetes\" : {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\"mcp-server-kubernetes\"],\n",
    "            \"transport\": \"stdio\",\n",
    "            \"env\": {\n",
    "                \"ALLOW_ONLY_NON_DESTRUCTIVE_TOOLS\": \"true\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# NB: The cluster has to be running otherwise the tools won't be available\n",
    "mcp_tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all the tools\n",
    "for tool in mcp_tools:\n",
    "    print(f\"ðŸ”§ {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools allowed\n",
    "tools_allowed = [\"kubectl_get\", \"kubectl_describe\", \"kubectl_logs\", \"explain_resource\", \"list_api_resources\", \"ping\"]\n",
    "\n",
    "tools = []\n",
    "for tool in mcp_tools:\n",
    "    if tool.name in tools_allowed:\n",
    "        tools.append(tool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool in tools:\n",
    "    print(f\"ðŸ”§ {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "All the previous messages are passed to the LLM for determining the next action\n",
    "\n",
    "This implementation requires a lot of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sytem_prompt = \"\"\"\n",
    "    You are an expert DevOps engineer who has been tasked with detecting anomalies in a deployed service.\n",
    "\n",
    "    The service you are working with today is described below:\n",
    "    {app_summary}\n",
    "\n",
    "    You will use an MCP server which will provide you access to the kubernetes cluster.\n",
    "\n",
    "    You will begin by analyzing the service's state and telemetry, and then submit your solution: describe the issue you have identified without fixing it!\n",
    "    Also explain your reasoning and thought process behind the solution.\n",
    "    Do not add any followup questions nor additional context.\n",
    "    \"\"\"\n",
    "\n",
    "app_summary = \"\"\"\n",
    "    The application implements a hotel reservation service, build with Go and gRPC, and starting from the open-source project https://github.com/harlow/go-micro-services. The initial project is extended in several ways, including adding back-end in-memory and persistent databases, adding a recommender system for obtaining hotel recommendations, and adding the functionality to place a hotel reservation. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = gpt5mini.bind_tools(tools, parallel_tool_calls=False)\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# System message\n",
    "sys_msg = SystemMessage(content=sytem_prompt.format(app_summary=app_summary))\n",
    "\n",
    "# Node\n",
    "async def sreAgent(state: MessagesState):\n",
    "   return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"sre-agent\", sreAgent)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"sre-agent\")\n",
    "builder.add_conditional_edges(\n",
    "    \"sre-agent\",\n",
    "    # If the latest message (result) from sre-agent is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from sre-agent is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"sre-agent\")\n",
    "react_graph = builder.compile()\n",
    "\n",
    "# Show\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(react_graph.get_graph(xray=True).draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "async def test_async_graph(graph , human_message: str, trace_name: str = None):\n",
    "    \"\"\"Test the graph with proper async execution\"\"\"\n",
    "    # Create a human message from the input\n",
    "    messages = [HumanMessage(content=human_message)]\n",
    "    \n",
    "    # Start time tracking\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Configuration for the graph execution\n",
    "    config = {\"recursion_limit\": 50}\n",
    "    if trace_name:\n",
    "        config[\"run_name\"] = trace_name\n",
    "    \n",
    "    # Invoke the graph asynchronously\n",
    "    result = await graph.ainvoke({\"messages\": messages}, config)\n",
    "\n",
    "    # Calculate execution time\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Print results\n",
    "    for m in result['messages']:\n",
    "        m.pretty_print()\n",
    "    \n",
    "    # Calculate token usage\n",
    "    total_tokens = 0\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    \n",
    "    for m in result['messages']:\n",
    "        if hasattr(m, 'usage_metadata'):\n",
    "            if m.usage_metadata:\n",
    "                if 'input_tokens' in m.usage_metadata:\n",
    "                    input_tokens += m.usage_metadata['input_tokens']\n",
    "                if 'output_tokens' in m.usage_metadata:\n",
    "                    output_tokens += m.usage_metadata['output_tokens']\n",
    "                if 'total_tokens' in m.usage_metadata:\n",
    "                    total_tokens += m.usage_metadata['total_tokens']\n",
    "\n",
    "    result[\"total_tokens\"] = total_tokens\n",
    "    result[\"input_tokens\"] = input_tokens\n",
    "    result[\"output_tokens\"] = output_tokens\n",
    "    result[\"execution_time\"] = execution_time\n",
    "    \n",
    "    print(f\"\\nExecution time: {execution_time:.2f} seconds\")\n",
    "    print(f\"Token usage: {total_tokens} total ({input_tokens} input, {output_tokens} output)\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = input(\"Enter experiment name: \")\n",
    "\n",
    "if experiment_name.strip() == \"\":\n",
    "    experiment_name = \"SRE ReAct agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "human = \"\"\n",
    "\n",
    "result_baseline = await test_async_graph(react_graph, human, trace_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final AI response\n",
    "result_baseline[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "def saveExperiment(experiment_name, result):\n",
    "    \n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    results_folder = \"results\"\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "    # Create the full path for the file\n",
    "    file_path = os.path.join(results_folder, f\"{timestamp}.txt\")\n",
    "\n",
    "    # Format and write messages to file\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(f\"{'='*50}\\nEXPERIMENT SUMMARY\\n{'='*50}\\n\\n\")\n",
    "        if experiment_name:\n",
    "            f.write(f\"Experiment Name: {experiment_name}\\n\")\n",
    "        f.write(f\"Total tokens used: {result['total_tokens']}\\n\")\n",
    "        f.write(f\"Input tokens: {result['input_tokens']}\\n\")\n",
    "        f.write(f\"Output tokens: {result['output_tokens']}\\n\")\n",
    "        f.write(f\"Execution time: {result['execution_time']:.2f} seconds\\n\\n\")\n",
    "\n",
    "        f.write(f\"{'='*50}\\nSYSTEM PROMPT\\n{'='*50}\\n\\n\")\n",
    "        f.write(sys_msg.content + \"\\n\\n\")\n",
    "        \n",
    "\n",
    "        for i, message in enumerate(result['messages']):\n",
    "            f.write(f\"{'='*50}\\nMESSAGE {i+1}\\n{'='*50}\\n\")\n",
    "            \n",
    "            if isinstance(message, HumanMessage):\n",
    "                f.write(f\"Type: Human Message\\n\")\n",
    "                f.write(f\"Content: {message.content}\\n\\n\")\n",
    "            \n",
    "            elif isinstance(message, AIMessage):\n",
    "                f.write(f\"Type: AI Message\\n\")\n",
    "                if message.content:\n",
    "                    f.write(f\"Content: {message.content}\\n\\n\")\n",
    "                \n",
    "                if hasattr(message, 'additional_kwargs') and 'tool_calls' in message.additional_kwargs:\n",
    "                    tool_calls = message.additional_kwargs['tool_calls']\n",
    "                    f.write(f\"Tool Calls ({len(tool_calls)}):\\n\")\n",
    "                    \n",
    "                    for idx, tool_call in enumerate(tool_calls):\n",
    "                        f.write(f\"  Tool Call {idx+1}: {tool_call['id']}\\n\")\n",
    "                        if 'function' in tool_call:\n",
    "                            f.write(f\"    Function: {tool_call['function']['name']}\\n\")\n",
    "                            f.write(f\"    Arguments: {tool_call['function']['arguments']}\\n\\n\")\n",
    "            \n",
    "            elif isinstance(message, ToolMessage):\n",
    "                f.write(f\"Type: Tool Response\\n\")\n",
    "                f.write(f\"Tool Call ID: {message.tool_call_id}\\n\")\n",
    "                f.write(f\"Content:\\n{message.content}\\n\\n\")\n",
    "            \n",
    "            else:\n",
    "                f.write(f\"Type: Other Message Type ({type(message).__name__})\\n\")\n",
    "                f.write(f\"Content: {str(message)}\\n\\n\")\n",
    "\n",
    "    print(f\"Messages exported to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveExperiment(experiment_name, result_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "# Improve agent efficiency (in terms of tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Reduce context by passing only three messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MESSAGES = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_reduced_input = \"\"\"\n",
    "    You are an expert DevOps engineer who has been tasked with detecting anomalies in a deployed service.\n",
    "\n",
    "    The service you are working with today is described below:\n",
    "    {app_summary}\n",
    "\n",
    "    Important constraint:\n",
    "    You only have access to the {max_messages} most recent messages in this conversation. \n",
    "    Anything older is not visible to you, so do not reference or assume context beyond those last {max_messages} messages.\n",
    "\n",
    "    Your task:\n",
    "        - Begin by analyzing the service's state and telemetry.\n",
    "        - Then submit your solution: describe the issue you have identified and the root cause (without fixing it).\n",
    "        - Also explain your reasoning and thought process.\n",
    "        - Do not add any followup questions nor additional context.\n",
    "\"\"\"\n",
    "\n",
    "app_summary = \"\"\"\n",
    "    The application implements a hotel reservation service, build with Go and gRPC, and starting from the open-source project https://github.com/harlow/go-micro-services. The initial project is extended in several ways, including adding back-end in-memory and persistent databases, adding a recommender system for obtaining hotel recommendations, and adding the functionality to place a hotel reservation. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = gpt5mini.bind_tools(tools, parallel_tool_calls=False)\n",
    "\n",
    "from langgraph.graph import MessagesState\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage, AIMessage\n",
    "\n",
    "# System message\n",
    "sys_msg_reduced = SystemMessage(content=system_prompt_reduced_input.format(app_summary=app_summary, max_messages=MAX_MESSAGES))\n",
    "\n",
    "# Node which limits the context window \n",
    "async def sreAgent_reducedInput(state: MessagesState):\n",
    "\n",
    "   messages = state[\"messages\"]\n",
    "\n",
    "   if len(state[\"messages\"]) <= MAX_MESSAGES:\n",
    "      return {\"messages\": [llm_with_tools.invoke([sys_msg_reduced] + messages)]}\n",
    "   \n",
    "   # Reduce the context window by taking only the last few messages\n",
    "   context_messages = []\n",
    "\n",
    "   # Work backwards from the end\n",
    "   i = len(messages) - 1\n",
    "   while i >= 0 and len(context_messages) < (MAX_MESSAGES + 2):\n",
    "      msg = messages[i]\n",
    "      context_messages.insert(0, msg)\n",
    "      \n",
    "      # If this is a ToolMessage, make sure to include the AIMessage that called it\n",
    "      if isinstance(msg, ToolMessage) and i > 0:\n",
    "         prev_msg = messages[i-1]\n",
    "         if isinstance(prev_msg, AIMessage) and hasattr(prev_msg, 'additional_kwargs'):\n",
    "               if 'tool_calls' in prev_msg.additional_kwargs and prev_msg not in context_messages:\n",
    "                  context_messages.insert(0, prev_msg)\n",
    "      i -= 1\n",
    "\n",
    "      return {\"messages\": [llm_with_tools.invoke([sys_msg_reduced] + context_messages)]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Graph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"sre-agent\", sreAgent_reducedInput)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"sre-agent\")\n",
    "builder.add_conditional_edges(\n",
    "    \"sre-agent\",\n",
    "    # If the latest message (result) from sre-agent is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from sre-agent is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"sre-agent\")\n",
    "react_graph_reduced = builder.compile()\n",
    "\n",
    "# Show\n",
    "display(Image(react_graph_reduced.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = input(\"Enter experiment name: \")\n",
    "\n",
    "if experiment_name.strip() == \"\":\n",
    "    experiment_name = \"SRE ReAct agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "human = \"\"\n",
    "\n",
    "result_reduced = await test_async_graph(react_graph_reduced, human, trace_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final AI answer\n",
    "result_reduced[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveExperiment(experiment_name, result_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Use custom structured schema\n",
    "\n",
    "This approach improves token efficiency by replacing the full message history with a structured summary. Instead of passing all previous messages to the LLM, the agent maintains:\n",
    "\n",
    "- **`insights`**: Key findings from each investigation step\n",
    "- **`prev_steps`**: Concise descriptions of actions taken\n",
    "- **`response`**: Final diagnosis when investigation is complete\n",
    "\n",
    "After each tool execution, a `summarise` node extracts the most important information and updates the structured state. This allows the agent to maintain context while significantly reducing token usage compared to the baseline approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Literal, Annotated\n",
    "from langgraph.graph.message import add_messages, AnyMessage\n",
    "import operator\n",
    "\n",
    "class SREAgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    app_summary: str\n",
    "    insights: Annotated[list[str], operator.add]\n",
    "    prev_steps: Annotated[list[str], operator.add]\n",
    "    response: str\n",
    "    final_output: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "sre_agent_prompt = \"\"\"\n",
    "    You are an expert DevOps engineer who has been tasked with detecting anomalies in a deployed service.\n",
    "\n",
    "    The service you are working with today is described below:\n",
    "    {app_summary}\n",
    "\n",
    "    You will use an MCP server which will provide you access to the Kubernetes cluster.\n",
    "\n",
    "    Context:\n",
    "\n",
    "    *Previous Steps:*\n",
    "    {prev_steps}\n",
    "\n",
    "    *Insights:*\n",
    "    {insights}\n",
    "\n",
    "    Your task:\n",
    "        1. Begin by analyzing the service's state and telemetry using kubectl tools\n",
    "        2. When you have identified the issue, call the submit_final_diagnosis tool with:\n",
    "            - diagnosis: Describe the issue you have identified (without fixing it)\n",
    "            - reasoning: Explain your reasoning and thought process behind the solution\n",
    "\n",
    "    IMPORTANT: You must call submit_final_diagnosis when you're ready to conclude your investigation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarise_prompt = \"\"\"\n",
    "    You are an autonomous SRE agent for Kubernetes incident diagnosis.\n",
    "\n",
    "    Context:\n",
    "\n",
    "    Previous Insights: \n",
    "    {insights}\n",
    "    \n",
    "    Previous Steps:\n",
    "    {prev_steps}\n",
    "\n",
    "    Below are the latest two messages:\n",
    "    {last_two_messages}\n",
    "\n",
    "    Instructions:\n",
    "    1. From the latest two messages, extract the most important new insight relevant for incident diagnosis or mitigation. Summarize it concisely.\n",
    "    2. Write a concise description of only the most recent action taken including the tool used (not the whole list).  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class UpdateAgentData(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents a step performed by the SRE agent.\n",
    "    \"\"\"\n",
    "    insight: str = Field(..., description=\"Most important new finding\")\n",
    "    prev_step: str = Field(..., description=\"Concise description of the most recent action taken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_strct_output = gpt5mini.with_structured_output(UpdateAgentData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_insights_str(state):\n",
    "    \"\"\"Return a string with the formatted list of insights gathered during exploration\"\"\"\n",
    "    if len(state[\"insights\"]) > 0:\n",
    "        return \"\\n- \".join([\"\"] + state[\"insights\"])\n",
    "    else:\n",
    "        return \"No insights yet\"\n",
    "    \n",
    "def get_prev_steps_str(state):\n",
    "    \"\"\"Return a string with the formatted list of previous steps performed during exploration\"\"\"\n",
    "    if len(state[\"prev_steps\"]) > 0:\n",
    "        return \"\\n- \".join([\"\"] + state[\"prev_steps\"])\n",
    "    else:\n",
    "        return \"No previous steps yet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Node used to summarise the infos given the two previous messages\n",
    "async def summarise(state: SREAgentState):\n",
    "\n",
    "    # Gather last two messages (tool call + tool response)\n",
    "    last_messages = state[\"messages\"][-2:]\n",
    "\n",
    "    insights_str = get_insights_str(state)\n",
    "    prev_step_str = get_prev_steps_str(state)\n",
    "\n",
    "    prompt = HumanMessage(content=summarise_prompt.format(\n",
    "        prev_steps = prev_step_str,\n",
    "        insights=insights_str,\n",
    "        last_two_messages=last_messages))\n",
    "\n",
    "    data = llm_with_strct_output.invoke([prompt])\n",
    "\n",
    "    return {\"insights\" : [data.insight], \"prev_steps\" : [data.prev_step]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.types import Command\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import tool, InjectedToolCallId\n",
    "\n",
    "#Â Tool used to submit the final response\n",
    "@tool\n",
    "def submit_final_diagnosis(\n",
    "    diagnosis: str, \n",
    "    reasoning: str,\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> Command:\n",
    "    \"\"\"\n",
    "    Submit the final diagnosis when investigation is complete.\n",
    "    \n",
    "    Args:\n",
    "        diagnosis: The issue you have identified (without fixing it)\n",
    "        reasoning: Your reasoning and thought process behind the diagnosis\n",
    "    \n",
    "    Returns:\n",
    "        Command to update state and end workflow\n",
    "    \"\"\"\n",
    "    final_response = f\"Diagnosis:\\n{diagnosis}\\n\\nReasoning:\\n{reasoning}\"\n",
    "    \n",
    "    return Command(\n",
    "        update={\n",
    "            \"response\": final_response, # Add in the final graph state the final answer\n",
    "            \"messages\": [\n",
    "                ToolMessage(\n",
    "                    content=\"Final diagnosis submitted successfully. Investigation complete.\",\n",
    "                    tool_call_id=tool_call_id\n",
    "                )\n",
    "            ]\n",
    "        },\n",
    "        goto=\"format-output\" # End the loop cycle\n",
    "    )\n",
    "\n",
    "# Append the tool for submission to the list of tools (MCP servers)\n",
    "completion_tool = submit_final_diagnosis\n",
    "tools_with_completion = tools + [completion_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def sreAgent(state: SREAgentState):\n",
    "\n",
    "    insights_str = get_insights_str(state)\n",
    "    prev_step_str = get_prev_steps_str(state)\n",
    "\n",
    "    prompt = HumanMessage(content=sre_agent_prompt.format(\n",
    "        prev_steps=prev_step_str, \n",
    "        insights=insights_str, \n",
    "        app_summary=state[\"app_summary\"]\n",
    "    ))\n",
    "\n",
    "    # Use tools with completion (for the submission)\n",
    "    llm_with_completion_tools = gpt5mini.bind_tools(tools_with_completion, parallel_tool_calls=False)\n",
    "    return {\"messages\": [llm_with_completion_tools.invoke([prompt])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def format_response(state: SREAgentState):\n",
    "\n",
    "    insights_str = get_insights_str(state)\n",
    "    prev_step_str = get_prev_steps_str(state)\n",
    "\n",
    "    message = \"**Results of the analysis**\\n\\n\"\n",
    "\n",
    "    message += \"Steps performed:\\n\"\n",
    "    message += prev_step_str\n",
    "\n",
    "    message += \"\\n\\nInsights gathered:\\n\"\n",
    "    message += insights_str\n",
    "\n",
    "    message += \"\\n\\nFinal report\\n\"\n",
    "\n",
    "    message += state[\"response\"]\n",
    "\n",
    "    return {\"final_output\" : message}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Build the graph\n",
    "builder = StateGraph(SREAgentState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"sre-agent\", sreAgent)\n",
    "builder.add_node(\"tools\", ToolNode(tools_with_completion)) # Tool node is executing the tool called in the previous message\n",
    "builder.add_node(\"summarise\", summarise) # Node to reduce the raw data into a schema\n",
    "builder.add_node(\"format-output\", format_response)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"sre-agent\")\n",
    "\n",
    "# Conditional edge from sre-agent\n",
    "builder.add_conditional_edges(\n",
    "    \"sre-agent\",\n",
    "    #Use in the conditional_edge to route to the ToolNode if the last message has tool calls. Otherwise, route to the end.\n",
    "    tools_condition,\n",
    ")\n",
    "\n",
    "# After tools, decide whether to summarise or end\n",
    "def after_tools_condition(state: SREAgentState):\n",
    "    # If response is filled, investigation is complete (end of the workflow)\n",
    "    if state.get(\"response\"):\n",
    "        return \"format-output\"\n",
    "    return \"summarise\"\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"tools\",\n",
    "    after_tools_condition,\n",
    "    {\n",
    "        \"summarise\": \"summarise\",\n",
    "        \"format-output\": \"format-output\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# After summarise, continue investigation (go to sre-agent)\n",
    "builder.add_edge(\"summarise\", \"sre-agent\")\n",
    "builder.add_edge(\"format-output\", END)\n",
    "\n",
    "# Compile the graph\n",
    "structured_graph = builder.compile()\n",
    "\n",
    "# Show the graph\n",
    "display(Image(structured_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(structured_graph.get_graph(xray=True).draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "async def test_structured_graph(graph, app_summary:str, human_message: str = \"\", trace_name: str = None):\n",
    "    \"\"\"Test the structured graph with SREAgentState\"\"\"\n",
    "    # Create initial state with SREAgentState structure\n",
    "    initial_state = {\n",
    "        \"messages\": [HumanMessage(content=human_message)] if human_message else [],\n",
    "        \"insights\": [],\n",
    "        \"prev_steps\": [],\n",
    "        \"response\": \"\",\n",
    "        \"final_output\" : \"\",\n",
    "        \"app_summary\" : app_summary\n",
    "    }\n",
    "    \n",
    "    # Start time tracking\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Configuration for the graph execution\n",
    "    config = {\"recursion_limit\": 50}\n",
    "    if trace_name:\n",
    "        config[\"run_name\"] = trace_name\n",
    "    \n",
    "    # Invoke the graph asynchronously\n",
    "    result = await graph.ainvoke(initial_state, config)\n",
    "\n",
    "    # Calculate execution time\n",
    "    execution_time = time.time() - start_time\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = input(\"Enter experiment name: \")\n",
    "\n",
    "if experiment_name.strip() == \"\":\n",
    "    experiment_name = \"SRE structured format agent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_summary = \"\"\"\n",
    "    The application implements a hotel reservation service, build with Go and gRPC, and starting from the open-source project https://github.com/harlow/go-micro-services. The initial project is extended in several ways, including adding back-end in-memory and persistent databases, adding a recommender system for obtaining hotel recommendations, and adding the functionality to place a hotel reservation. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "human = \"\"\n",
    "structured_result = await test_structured_graph(structured_graph, app_summary, human, experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(structured_result[\"final_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in structured_result[\"prev_steps\"]:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "for insight in structured_result[\"insights\"]:\n",
    "    print(insight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(structured_result[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in structured_result[\"messages\"]:\n",
    "    message.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sre-agent-35UqMg2y-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
