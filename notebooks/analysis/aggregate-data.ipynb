{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Aggregate results\n",
    "\n",
    "This notebook aggregates experiment results from multiple batch folders into a single deduplicated dataset. It:\n",
    "\n",
    "1. **Loads all experiment batches** from the results directory\n",
    "2. **Imports all JSON result files** across all batches into a unified DataFrame\n",
    "3. **Identifies and resolves duplicates** based on agent_id, fault_name, and scenario\n",
    "4. **Creates a clean aggregate folder** containing only unique experiments (one per agent-fault-scenario combination)\n",
    "\n",
    "This is useful when you have run the same experiments across multiple batches and want to consolidate them for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "# Get the path to the root directory of the repository\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "# Load environment variables from .env file in the root directory\n",
    "load_dotenv(os.path.join(root_dir, '.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Load resutls batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.environ.get(\"RESULTS_PATH\")\n",
    "print(f\"The results are stored in the directory {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_batches = [d for d in os.listdir(RESULTS_DIR) if os.path.isdir(os.path.join(RESULTS_DIR, d))]\n",
    "for i, d in enumerate(experiment_batches, 1):\n",
    "    dir_path = os.path.join(RESULTS_DIR, d)\n",
    "    json_count = sum(\n",
    "        1 for f in os.listdir(dir_path)\n",
    "        if f.endswith('.json') and os.path.isfile(os.path.join(dir_path, f))\n",
    "    )\n",
    "    print(f\"{i}) {d} ({json_count} result{'s' if json_count != 1 else ''})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Load faults and agent configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAULTS_CONF_DIR = os.environ.get(\"FAULTS_CONF_DIR\")\n",
    "\n",
    "faults_conf_files = [f for f in os.listdir(FAULTS_CONF_DIR) if f.endswith('.json')]\n",
    "\n",
    "for f in faults_conf_files:\n",
    "    print(f\"- {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name_dict = {}\n",
    "\n",
    "AGENTS_CONF_DIR = os.environ.get(\"AGENTS_CONF_DIR\")\n",
    "\n",
    "agents_conf_files = [f for f in os.listdir(AGENTS_CONF_DIR) if f.endswith('.json')]\n",
    "\n",
    "for f in agents_conf_files:\n",
    "    with open(os.path.join(AGENTS_CONF_DIR,f), 'r') as file:\n",
    "        data = json.load(file)\n",
    "        agent_name_dict[data.get(\"name\")] = data.get(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Import all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_df = pd.DataFrame()\n",
    "\n",
    "for batch in experiment_batches:\n",
    "    batch_path = os.path.join(RESULTS_DIR, batch)\n",
    "    experiments = [f for f in os.listdir(batch_path) if f.endswith('.json')]\n",
    "    for experiment in experiments:\n",
    "        try:\n",
    "            with open(os.path.join(batch_path, experiment), 'r') as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "                localization = data.get(\"final_report\", {}).get(\"localization\", [])\n",
    "                if isinstance(localization, list):\n",
    "                    localization_str = \", \".join(localization)\n",
    "                else:\n",
    "                    localization_str = None\n",
    "\n",
    "            record = {\n",
    "                \"batch\": batch,\n",
    "                \"experiment_file\": experiment,\n",
    "                \"agent_id\" : data.get(\"agent_id\", None),\n",
    "                \"agent_conf_name\" : data.get(\"agent_configuration_name\", None),\n",
    "                \"scenario\": data.get(\"app_name\", None).lower(),\n",
    "                \"fault_name\": data.get(\"testbed\", {}).get(\"fault_name\", None),\n",
    "                \"target_namespace\": data.get(\"target_namespace\", None),\n",
    "                \"trace_service_starting_point\": data.get(\"trace_service_starting_point\", None),\n",
    "                \"rca_tasks_per_iteration\": data.get(\"testbed\", {}).get(\"rca_tasks_per_iteration\", 0),\n",
    "                \"max_tool_calls\": data.get(\"testbed\", {}).get(\"max_tool_calls\", 0),\n",
    "                \"execution_time_seconds\": data.get(\"stats\", {}).get(\"execution_time_seconds\", 0),\n",
    "                \"total_tokens\": data.get(\"stats\", {}).get(\"total_tokens\", 0),\n",
    "                \"tokens_triage\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"triage_agent\", {}).get(\"total_tokens\", 0),\n",
    "                \"tokens_planner\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"planner_agent\", {}).get(\"total_tokens\", 0),\n",
    "                \"tokens_rca_worker\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"rca_agent\", {}).get(\"total_tokens\", 0),\n",
    "                \"runs_count_rca\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"rca_agent\", {}).get(\"runs_count\", 0),\n",
    "                \"tokens_supervisor\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"supervisor_agent\", {}).get(\"total_tokens\", 0),\n",
    "                \"runs_count_supervisor\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"supervisor_agent\", {}).get(\"runs_count\", 0),\n",
    "                \"detection\": data.get(\"final_report\", {}).get(\"detection\", None),\n",
    "                \"localization\": localization_str, \n",
    "                \"root_cause\": data.get(\"final_report\", {}).get(\"root_cause\", None),\n",
    "                \"eval_detection\" : data.get(\"evaluation\", {}).get(\"detection\", None),\n",
    "                \"eval_localization\" : data.get(\"evaluation\", {}).get(\"localization\", None),\n",
    "                \"eval_rca_score\" : data.get(\"evaluation\", {}).get(\"rca_score\", None),\n",
    "                \"eval_rca_motivation\" : data.get(\"evaluation\", {}).get(\"rca_motivation\", None),\n",
    "            }\n",
    "\n",
    "            if not record['agent_id']:\n",
    "                record['agent_id'] = next((id for conf, id in agent_name_dict.items() if conf in record['agent_conf_name']), None)\n",
    "\n",
    "            # Append record to dataframe\n",
    "            experiments_df = pd.concat([experiments_df, pd.DataFrame([record])], ignore_index=True)\n",
    "        \n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Warning: Error processing {experiment}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "experiments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Deduplicate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by agent_id and fault_name, aggregating count and list of batches\n",
    "grouped_df = experiments_df.groupby(['agent_id', 'fault_name']).agg(\n",
    "    count=('experiment_file', 'count'),\n",
    "    batches=('batch', lambda x: ', '.join(sorted(set(x))))\n",
    ").reset_index()\n",
    "\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique scenarios and let user choose\n",
    "scenarios = sorted(experiments_df['scenario'].unique())\n",
    "print(f\"\\nAvailable apps/scenarios:\")\n",
    "for i, scenario in enumerate(scenarios, 1):\n",
    "    count = len(experiments_df[experiments_df['scenario'] == scenario])\n",
    "    print(f\"{i}) {scenario} ({count} results)\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        choice = int(input(f\"\\nSelect an app (1-{len(scenarios)}): \"))\n",
    "        if 1 <= choice <= len(scenarios):\n",
    "            selected_scenario = scenarios[choice - 1]\n",
    "            break\n",
    "        else:\n",
    "            print(f\"Invalid choice. Please enter a number between 1 and {len(scenarios)}\")\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter a valid number.\")\n",
    "\n",
    "print(f\"\\nSelected: {selected_scenario}\")\n",
    "\n",
    "# Filter dataframe to selected scenario\n",
    "filtered_df = experiments_df[experiments_df['scenario'] == selected_scenario].copy()\n",
    "print(f\"Found {len(filtered_df)} results for {selected_scenario}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def resolve_duplicates(df, automatic=False):\n",
    "    # Find duplicates based on agent_id, fault_name\n",
    "    duplicate_mask = df.duplicated(subset=['agent_id', 'fault_name'], keep=False)\n",
    "    unique_rows = []\n",
    "    \n",
    "    for _, group in df[duplicate_mask].groupby(['agent_id', 'fault_name']):\n",
    "        if automatic:\n",
    "            # Automatically select the most recent JSON file (by modification time)\n",
    "            group_with_mtime = group.copy()\n",
    "            group_with_mtime['mtime'] = group_with_mtime.apply(\n",
    "                lambda row: os.path.getmtime(os.path.join(RESULTS_DIR, row['batch'], row['experiment_file'])),\n",
    "                axis=1\n",
    "            )\n",
    "            newest = group_with_mtime.loc[group_with_mtime['mtime'].idxmax()]\n",
    "            unique_rows.append(newest)\n",
    "        else:\n",
    "            print(f\"\\nDuplicate found for: Agent {group.iloc[0]['agent_id']} - {group.iloc[0]['fault_name']}\")\n",
    "            print(f\"\\n{'Index':<7} {'Batch':<40}\")\n",
    "            print(\"-\" * 50)\n",
    "            for idx, row in enumerate(group.itertuples()):\n",
    "                print(f\"{idx:<7} {row.batch:<40}\")\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    idx = int(input(f\"\\nSelect the index of the row to keep (0-{len(group)-1}): \"))\n",
    "                    if 0 <= idx < len(group):\n",
    "                        break\n",
    "                    else:\n",
    "                        print(f\"Invalid index. Please enter a number between 0 and {len(group)-1}\")\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input. Please enter a valid number.\")\n",
    "            \n",
    "            unique_rows.append(group.iloc[idx])\n",
    "            clear_output()\n",
    "    \n",
    "    # Non-duplicates\n",
    "    non_duplicates = df[~duplicate_mask]\n",
    "    \n",
    "    # Combine\n",
    "    result_df = pd.concat([non_duplicates, pd.DataFrame(unique_rows)], ignore_index=True)\n",
    "    return result_df\n",
    "\n",
    "unique_experiments_df = resolve_duplicates(filtered_df, automatic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_experiments_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Store results in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dir_aggregate_name = input(\"Enter a name for the aggregate results folder: \")\n",
    "\n",
    "# Create a new folder in RESULTS_DIR\n",
    "unique_results_dir = os.path.join(RESULTS_DIR, dir_aggregate_name)\n",
    "os.makedirs(unique_results_dir, exist_ok=True)\n",
    "\n",
    "# Copy all files listed in unique_experiments_df['experiment_file'] from their batch folders\n",
    "for _, row in unique_experiments_df.iterrows():\n",
    "    src = os.path.join(RESULTS_DIR, row['batch'], row['experiment_file'])\n",
    "    dst = os.path.join(unique_results_dir, row['experiment_file'])\n",
    "    shutil.copy2(src, dst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sre-agent-35UqMg2y-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
