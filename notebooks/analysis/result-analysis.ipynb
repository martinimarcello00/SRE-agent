{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Get the path to the root directory of the repository\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "# Load environment variables from .env file in the root directory\n",
    "load_dotenv(os.path.join(root_dir, '.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Load experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.environ.get(\"RESULTS_PATH\")\n",
    "EXPERIMENT_DIR = None\n",
    "print(f\"The results are stored in the directory {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_batches = [d for d in os.listdir(RESULTS_DIR) if os.path.isdir(os.path.join(RESULTS_DIR, d))]\n",
    "\n",
    "for i, dir in enumerate(experiment_batches, 1):\n",
    "    print(f\"{i}) {dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = input(\"Choose the experiment ID (the number)\")\n",
    "\n",
    "try:\n",
    "    id = int(experiment_id) - 1\n",
    "    if id < 0 or id >= len(experiment_batches):\n",
    "        print(f\"Error: Invalid experiment ID. Please choose a number between 1 and {len(experiment_batches)}\")\n",
    "        id = 0\n",
    "        print(f\"Using first experiment: {experiment_batches[id]}\") \n",
    "except ValueError:\n",
    "    print(f\"Error: Invalid input. Please enter a number between 1 and {len(experiment_batches)}\")\n",
    "    id = 0\n",
    "    print(f\"Using first experiment: {experiment_batches[id]}\")\n",
    "\n",
    "exp_folder_name = experiment_batches[id]\n",
    "EXPERIMENT_DIR = os.path.join(RESULTS_DIR,exp_folder_name)\n",
    "\n",
    "print(f\"The experiment batch selected is {EXPERIMENT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_results_files = [f for f in os.listdir(EXPERIMENT_DIR) if f.endswith('.json')]\n",
    "print(f\"Found {len(exp_results_files)} experiments:\")\n",
    "for file in exp_results_files:\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "experiment = exp_results_files[0]\n",
    "with open(os.path.join(EXPERIMENT_DIR,experiment), 'r') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "experiments_df = pd.DataFrame()\n",
    "\n",
    "for experiment in exp_results_files:\n",
    "    try:\n",
    "        with open(os.path.join(EXPERIMENT_DIR, experiment), 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "            localization = data.get(\"final_report\", {}).get(\"localization\", [])\n",
    "            if isinstance(localization, list):\n",
    "                localization_str = \", \".join(localization)\n",
    "            else:\n",
    "                localization_str = None\n",
    "    \n",
    "        record = {\n",
    "            \"experiment_file\": experiment,\n",
    "            \"agent_id\" : data.get(\"agent_id\", None),\n",
    "            \"agent_conf_name\" : data.get(\"agent_configuration_name\", None),\n",
    "            \"scenario\": data.get(\"app_name\", None),\n",
    "            \"fault_name\": data.get(\"testbed\", {}).get(\"fault_name\", None),\n",
    "            \"target_namespace\": data.get(\"target_namespace\", None),\n",
    "            \"trace_service_starting_point\": data.get(\"trace_service_starting_point\", None),\n",
    "            \"rca_tasks_per_iteration\": data.get(\"testbed\", {}).get(\"rca_tasks_per_iteration\", 0),\n",
    "            \"max_tool_calls\": data.get(\"testbed\", {}).get(\"max_tool_calls\", 0),\n",
    "            \"execution_time_seconds\": data.get(\"stats\", {}).get(\"execution_time_seconds\", 0),\n",
    "            \"total_tokens\": data.get(\"stats\", {}).get(\"total_tokens\", 0),\n",
    "            \"tokens_triage\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"triage_agent\", {}).get(\"total_tokens\", 0),\n",
    "            \"tokens_planner\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"planner_agent\", {}).get(\"total_tokens\", 0),\n",
    "            \"tokens_rca_worker\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"rca_agent\", {}).get(\"total_tokens\", 0),\n",
    "            \"runs_count_rca\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"rca_agent\", {}).get(\"runs_count\", 0),\n",
    "            \"tokens_supervisor\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"supervisor_agent\", {}).get(\"total_tokens\", 0),\n",
    "            \"runs_count_supervisor\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"supervisor_agent\", {}).get(\"runs_count\", 0),\n",
    "            \"detection\": data.get(\"final_report\", {}).get(\"detection\", None),\n",
    "            \"localization\": localization_str, \n",
    "            \"root_cause\": data.get(\"final_report\", {}).get(\"root_cause\", None),\n",
    "            \"eval_detection\" : data.get(\"evaluation\", {}).get(\"detection\", None),\n",
    "            \"eval_localization\" : data.get(\"evaluation\", {}).get(\"localization\", None),\n",
    "            \"eval_rca_score\" : data.get(\"evaluation\", {}).get(\"rca_score\", None),\n",
    "            \"eval_rca_motivation\" : data.get(\"evaluation\", {}).get(\"rca_motivation\", None),\n",
    "        }\n",
    "        \n",
    "        # Append record to dataframe\n",
    "        experiments_df = pd.concat([experiments_df, pd.DataFrame([record])], ignore_index=True)\n",
    "    \n",
    "    except (json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"Warning: Error processing {experiment}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Load fault types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAULTS_CONF_DIR = os.environ.get(\"FAULTS_CONF_DIR\")\n",
    "\n",
    "faults_conf_files = [f for f in os.listdir(FAULTS_CONF_DIR) if f.endswith('.json')]\n",
    "\n",
    "print(f\"Found {len(faults_conf_files)} fault configuration files:\")\n",
    "\n",
    "for f in faults_conf_files:\n",
    "    print(f\"- {f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_target_dict = {}\n",
    "\n",
    "for f in faults_conf_files:\n",
    "    with open(os.path.join(FAULTS_CONF_DIR,f), 'r') as file:\n",
    "        data = json.load(file)\n",
    "        formatted_key = f\"{data[\"app_name\"]} - {data[\"fault_type\"]}\"\n",
    "        fault_target_dict[formatted_key] = data.get(\"target\", None)\n",
    "\n",
    "fault_target_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation helpers from repo package\n",
    "import sys, os\n",
    "\n",
    "# Ensure we can import from `sre-agent` package\n",
    "sys.path.append(os.path.abspath(os.path.join(root_dir, 'sre-agent')))\n",
    "\n",
    "from evaluation.evaluation import (\n",
    "    evaluate_detection,\n",
    "    evaluate_localization,\n",
    "    evaluate_rca_analysis,\n",
    "    evaluate_experiment,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ground-truth mapping (target, RCA_gt) from faults configs\n",
    "faults_gt_dict = {}\n",
    "try:\n",
    "    for f in faults_conf_files:\n",
    "        with open(os.path.join(FAULTS_CONF_DIR, f), 'r') as file:\n",
    "            d = json.load(file)\n",
    "            key = f\"{d['app_name']} - {d['fault_type']}\"\n",
    "            faults_gt_dict[key] = {\n",
    "                \"target\": d.get(\"target\", None),\n",
    "                \"RCA_gt\": d.get(\"RCA_gt\", \"\")\n",
    "            }\n",
    "    print(f\"Loaded GT for {len(faults_gt_dict)} scenarios.\")\n",
    "except Exception as e:\n",
    "    print(f\"[warn] Failed building faults_gt_dict: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fault_scenario(row):\n",
    "    key = f\"{row['scenario']} - {row['fault_name']}\"\n",
    "    gt = faults_gt_dict.get(key, {})\n",
    "    return {\n",
    "        \"target\": gt.get(\"target\", None),\n",
    "        \"RCA_gt\": gt.get(\"RCA_gt\", \"\")\n",
    "    }\n",
    "\n",
    "def build_report_from_row(row):\n",
    "    # Convert localization to list for evaluate_experiment compatibility\n",
    "    loc_val = row['localization']\n",
    "    if isinstance(loc_val, str):\n",
    "        loc_list = [s.strip() for s in loc_val.split(',') if s.strip()]\n",
    "    elif isinstance(loc_val, list):\n",
    "        loc_list = loc_val\n",
    "    else:\n",
    "        loc_list = []\n",
    "    return {\n",
    "        \"agent_configuration_name\": row.get(\"agent_conf_name\", \"N/A\"),\n",
    "        \"final_report\": {\n",
    "            \"detection\": bool(row.get(\"detection\", False)),\n",
    "            \"localization\": loc_list,\n",
    "            \"root_cause\": row.get(\"root_cause\", \"\") if isinstance(row.get(\"root_cause\", \"\"), str) else \"\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "def backfill_missing_evaluations(limit: Optional[int] = None, dry_run: bool = False) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    For rows where eval_* fields are missing, compute evaluation via evaluate_experiment,\n",
    "    persist to the result JSON (adds/overwrites the \"evaluation\" key), and update the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        limit: optional maximum number of rows to process.\n",
    "        dry_run: if True, do not write files; only compute and update the DataFrame in-memory.\n",
    "\n",
    "    Returns:\n",
    "        (processed_count, file_updates)\n",
    "    \"\"\"\n",
    "    required_cols = ['eval_detection', 'eval_localization', 'eval_rca_score', 'eval_rca_motivation']\n",
    "    missing_mask = experiments_df[required_cols].isna().any(axis=1)\n",
    "    idxs = experiments_df[missing_mask].index.tolist()\n",
    "    if limit is not None:\n",
    "        idxs = idxs[:limit]\n",
    "    \n",
    "    processed = 0\n",
    "    updated_files = 0\n",
    "    \n",
    "    for idx in tqdm(idxs, desc=\"Backfilling evaluations\", unit=\"row\"):\n",
    "        row = experiments_df.loc[idx]\n",
    "        # Build inputs for evaluation\n",
    "        fault_scenario = build_fault_scenario(row)\n",
    "        report = build_report_from_row(row)\n",
    "        \n",
    "        # Run evaluation (this may call the LLM for RCA scoring)\n",
    "        evaluation = evaluate_experiment(fault_scenario, report)\n",
    "        if not isinstance(evaluation, dict):\n",
    "            print(f\"[warn] Row {idx}: evaluate_experiment returned non-dict; skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Update dataframe in-memory\n",
    "        experiments_df.loc[idx, 'eval_detection'] = evaluation.get('detection', None)\n",
    "        experiments_df.loc[idx, 'eval_localization'] = evaluation.get('localization', None)\n",
    "        experiments_df.loc[idx, 'eval_rca_score'] = evaluation.get('rca_score', None)\n",
    "        experiments_df.loc[idx, 'eval_rca_motivation'] = evaluation.get('rca_motivation', None)\n",
    "        \n",
    "        # Persist to file\n",
    "        if not dry_run:\n",
    "            file_name = row['experiment_file']\n",
    "            file_path = os.path.join(EXPERIMENT_DIR, file_name)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                data['evaluation'] = evaluation\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(data, f, indent=2)\n",
    "                updated_files += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] Row {idx}: failed writing evaluation to {file_path}: {e}\")\n",
    "        \n",
    "        processed += 1\n",
    "    \n",
    "    print(f\"Backfill completed. Processed rows: {processed}. Files updated: {updated_files}.\")\n",
    "    return processed, updated_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "backfill_missing_evaluations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Get unique fault types for coloring\n",
    "fault_types = experiments_df['fault_name'].unique()\n",
    "colors = plt.cm.Accent(np.linspace(0, 1, len(fault_types)))\n",
    "color_map = {fault: colors[i] for i, fault in enumerate(fault_types)}\n",
    "\n",
    "# Plot each experiment as a dot\n",
    "for idx, row in experiments_df.iterrows():\n",
    "    x = row['execution_time_seconds']\n",
    "    y = row['total_tokens']\n",
    "    fault = row['fault_name']\n",
    "    color = color_map[fault]\n",
    "    detection = row['detection']\n",
    "    \n",
    "    # Plot the dot - filled if detection is True, only border if False\n",
    "    if detection:\n",
    "        ax.scatter(x, y, s=100, color=color, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "    else:\n",
    "        ax.scatter(x, y, s=100, facecolors='none', edgecolors=color, linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # Add label with agent configuration on top of the dot\n",
    "    label = f\"{row['agent_id']}\"\n",
    "    ax.annotate(label, (x, y), fontsize=8, ha='center', va='bottom', fontweight='bold', \n",
    "                xytext=(0, 7), textcoords='offset points')\n",
    "\n",
    "# Create legend for fault types\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map[fault], \n",
    "                       markersize=8, label=fault) for fault in fault_types]\n",
    "\n",
    "# Add detection legend\n",
    "detection_handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', \n",
    "               markersize=8, label='Detection: True', markeredgecolor='black', markeredgewidth=0.5),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='none', \n",
    "               markersize=8, label='Detection: False', markeredgecolor='gray', markeredgewidth=2)\n",
    "]\n",
    "\n",
    "# Combine all handles into a single legend\n",
    "all_handles = handles + detection_handles\n",
    "ax.legend(handles=all_handles, loc='upper left', fontsize=10)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Execution Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Total Tokens', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Hotel reservation scenario', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Grid\n",
    "ax.grid(True, alpha=0.3, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "# Get unique fault types for coloring\n",
    "fault_types = experiments_df['fault_name'].unique()\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(fault_types)))\n",
    "color_map = {fault: colors[i] for i, fault in enumerate(fault_types)}\n",
    "\n",
    "# Collect points and labels for later adjustment\n",
    "texts = []\n",
    "\n",
    "# Plot each experiment as a dot\n",
    "for idx, row in experiments_df.iterrows():\n",
    "    x = row['execution_time_seconds']\n",
    "    y = row['total_tokens']\n",
    "    fault = row['fault_name']\n",
    "    color = color_map[fault]\n",
    "    correct_localization = row['correct_localization']\n",
    "    \n",
    "    # Plot the dot - filled if correct_localization is True, X marker if False\n",
    "    if correct_localization:\n",
    "        ax.scatter(x, y, s=150, color=color, alpha=0.8, edgecolors='black', linewidth=1.2)\n",
    "    else:\n",
    "        ax.scatter(x, y, s=150, marker='x', color=color, linewidth=2.5, alpha=0.8)\n",
    "    \n",
    "    # Add label with agent configuration\n",
    "    label = f\"{row['agent_id']}\"\n",
    "    text_obj = ax.text(x, y, label, fontsize=9, ha='center', va='center', fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round,pad=0.4', facecolor='white', edgecolor='gray', alpha=0.85, linewidth=0.8),\n",
    "                       zorder=10)\n",
    "    texts.append(text_obj)\n",
    "\n",
    "# Adjust text positions to avoid overlaps\n",
    "try:\n",
    "    adjust_text(texts, arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2', \n",
    "                                       lw=0.8, color='gray', alpha=0.6),\n",
    "                ax=ax, precision=0.1, expand_points=(1.5, 1.5))\n",
    "except ImportError:\n",
    "    print(\"Note: adjustText not installed. Labels may overlap. Install with: pip install adjustText\")\n",
    "\n",
    "# Create legend for fault types\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map[fault], \n",
    "                       markersize=10, label=fault, markeredgecolor='black', markeredgewidth=1) \n",
    "           for fault in sorted(fault_types)]\n",
    "\n",
    "# Add localization legend\n",
    "localization_handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', \n",
    "               markersize=10, label='Correct Localization: True', markeredgecolor='black', markeredgewidth=1),\n",
    "    plt.Line2D([0], [0], marker='x', color='gray', linestyle='None',\n",
    "               markersize=10, label='Correct Localization: False', markeredgewidth=2.5)\n",
    "]\n",
    "\n",
    "# Combine all handles into a single legend\n",
    "all_handles = handles + localization_handles\n",
    "ax.legend(handles=all_handles, loc='upper left', fontsize=11, framealpha=0.95, edgecolor='black')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Execution Time (seconds)', fontsize=14, fontweight='bold', color='#2c3e50')\n",
    "ax.set_ylabel('Total Tokens', fontsize=14, fontweight='bold', color='#2c3e50')\n",
    "ax.set_title('Localization Accuracy: Execution Time vs Token Usage\\n(by Fault Type and Agent Configuration)', \n",
    "             fontsize=16, fontweight='bold', pad=20, color='#2c3e50')\n",
    "\n",
    "# Grid and background\n",
    "ax.set_facecolor('#f8f9fa')\n",
    "ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8, color='#bdc3c7')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('#bdc3c7')\n",
    "ax.spines['bottom'].set_color('#bdc3c7')\n",
    "ax.spines['left'].set_linewidth(1.2)\n",
    "ax.spines['bottom'].set_linewidth(1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distributions first\n",
    "global_dist = experiments_df['correct_localization'].value_counts(normalize=True).sort_index(ascending=False)\n",
    "fault_type_dist = experiments_df.groupby('fault_name')['correct_localization'].mean().sort_values(ascending=False)\n",
    "agent_config_dist = experiments_df.groupby(['rca_tasks_per_iteration', 'max_tool_calls'])['correct_localization'].mean().sort_values(ascending=False)\n",
    "\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "\n",
    "# Set modern style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure matplotlib for modern look\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = '#f8f9fa'\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "\n",
    "# ============ GLOBAL DISTRIBUTION ============\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "colors_global = ['#2ecc71', '#e74c3c']\n",
    "bars = ax.bar(['Correct', 'Incorrect'], [global_dist.get(True, 0), global_dist.get(False, 0)], \n",
    "              color=colors_global, edgecolor='white', linewidth=3, alpha=0.9, width=0.6)\n",
    "\n",
    "ax.set_title('Localization Accuracy - Overall Results', fontsize=18, fontweight='bold', pad=25, color='#2c3e50')\n",
    "ax.set_ylabel('Proportion', fontsize=14, fontweight='bold', color='#34495e')\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "ax.set_ylim(0, max(global_dist.values) * 1.2)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('#bdc3c7')\n",
    "ax.spines['bottom'].set_color('#bdc3c7')\n",
    "ax.spines['left'].set_linewidth(1)\n",
    "ax.spines['bottom'].set_linewidth(1)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.2, linewidth=0.8, color='#bdc3c7')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Add value labels\n",
    "total = experiments_df.shape[0]\n",
    "for i, (label, bar) in enumerate(zip(['Correct', 'Incorrect'], bars)):\n",
    "    value = bar.get_height()\n",
    "    count = int(value * total)\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, value + 0.045, f'{value*100:.1f}%\\n({count}/{total})', \n",
    "            ha='center', va='bottom', fontsize=13, fontweight='bold', color='#2c3e50')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============ BY FAULT TYPE ============\n",
    "fig, ax = plt.subplots(figsize=(13, 7))\n",
    "colors_fault = plt.cm.RdYlGn(np.linspace(0.3, 0.8, len(fault_type_dist)))\n",
    "bars = ax.barh(range(len(fault_type_dist)), fault_type_dist.values, \n",
    "               color=colors_fault, edgecolor='white', linewidth=2.5, alpha=0.9)\n",
    "\n",
    "ax.set_yticks(range(len(fault_type_dist)))\n",
    "ax.set_yticklabels(fault_type_dist.index, fontsize=12, fontweight='bold', color='#2c3e50')\n",
    "ax.set_xlabel('Localization Accuracy Rate', fontsize=14, fontweight='bold', color='#34495e')\n",
    "ax.set_title('Localization Accuracy by Fault Type', fontsize=18, fontweight='bold', pad=25, color='#2c3e50')\n",
    "ax.xaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "ax.set_xlim(0, 1.2)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('#bdc3c7')\n",
    "ax.spines['bottom'].set_color('#bdc3c7')\n",
    "ax.spines['left'].set_linewidth(1)\n",
    "ax.spines['bottom'].set_linewidth(1)\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.2, linewidth=0.8, color='#bdc3c7')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Add value labels\n",
    "for i, (fault, rate) in enumerate(fault_type_dist.items()):\n",
    "    total_fault = (experiments_df['fault_name'] == fault).sum()\n",
    "    correct_fault = ((experiments_df['fault_name'] == fault) & (experiments_df['correct_localization'])).sum()\n",
    "    ax.text(rate + 0.03, i, f'{rate*100:.1f}% ({correct_fault}/{total_fault})', \n",
    "            va='center', fontsize=11, fontweight='bold', color='#2c3e50')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============ BY AGENT CONFIGURATION ============\n",
    "fig, ax = plt.subplots(figsize=(11, 6))\n",
    "config_labels = [f\"P{p}-T{t}\" for p, t in agent_config_dist.index]\n",
    "colors_config = plt.cm.viridis(np.linspace(0.2, 0.9, len(agent_config_dist)))\n",
    "bars = ax.bar(range(len(agent_config_dist)), agent_config_dist.values,\n",
    "              color=colors_config, edgecolor='white', linewidth=2.5, alpha=0.9, width=0.65)\n",
    "\n",
    "ax.set_xticks(range(len(agent_config_dist)))\n",
    "ax.set_xticklabels(config_labels, fontsize=12, fontweight='bold', color='#2c3e50', rotation=45, ha='right')\n",
    "ax.set_ylabel('Localization Accuracy Rate', fontsize=14, fontweight='bold', color='#34495e')\n",
    "ax.set_title('Localization Accuracy by Agent Configuration', fontsize=18, fontweight='bold', pad=25, color='#2c3e50')\n",
    "ax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "ax.set_ylim(0, max(agent_config_dist.values) * 1.2)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('#bdc3c7')\n",
    "ax.spines['bottom'].set_color('#bdc3c7')\n",
    "ax.spines['left'].set_linewidth(1)\n",
    "ax.spines['bottom'].set_linewidth(1)\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.2, linewidth=0.8, color='#bdc3c7')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Add value labels\n",
    "for i, ((p, t), rate) in enumerate(agent_config_dist.items()):\n",
    "    mask = (experiments_df['rca_tasks_per_iteration'] == str(p)) & (experiments_df['max_tool_calls'] == str(t))\n",
    "    total_cfg = mask.sum()\n",
    "    correct_cfg = (mask & experiments_df['correct_localization']).sum()\n",
    "    ax.text(i, rate + 0.048, f'{rate*100:.1f}%\\n({correct_cfg}/{total_cfg})', \n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold', color='#2c3e50')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_tokens = experiments_df[[\n",
    "    'tokens_triage',\n",
    "    'tokens_planner', \n",
    "    'tokens_rca_worker',\n",
    "    'tokens_supervisor'\n",
    "]].sum()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "agent_tokens.plot(kind='bar', ax=ax, color=['#FF9999', '#66B2FF', '#99FF99', '#FFD700'])\n",
    "ax.set_ylabel('Total Tokens', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Token Distribution Across Agents', fontsize=14, fontweight='bold')\n",
    "ax.set_xticklabels(['Triage', 'Planner', 'RCA Worker', 'Supervisor'], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "iterations = experiments_df['rca_tasks_per_iteration'].unique()\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(iterations)))\n",
    "\n",
    "for i, iter_val in enumerate(sorted(iterations)):\n",
    "    mask = experiments_df['rca_tasks_per_iteration'] == iter_val\n",
    "    ax.scatter(experiments_df[mask]['execution_time_seconds'], \n",
    "              experiments_df[mask]['total_tokens'],\n",
    "              label=f'RCA Tasks: {int(iter_val)}',\n",
    "              s=100, alpha=0.7, color=colors[i])\n",
    "\n",
    "ax.set_xlabel('Execution Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Total Tokens', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Impact of RCA Tasks per Iteration on Performance', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sre-agent-35UqMg2y-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
