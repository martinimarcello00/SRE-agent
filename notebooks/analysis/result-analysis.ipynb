{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Result analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Get the path to the root directory of the repository\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "# Load environment variables from .env file in the root directory\n",
    "load_dotenv(os.path.join(root_dir, '.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "SAVE_IMAGES = True\n",
    "OVERRIDE_EVAL = False\n",
    "DRIRUN_EVAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Load experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = os.environ.get(\"RESULTS_PATH\")\n",
    "print(f\"The results are stored in the directory {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_batches = [d for d in os.listdir(RESULTS_DIR) if os.path.isdir(os.path.join(RESULTS_DIR, d))]\n",
    "\n",
    "# Print each folder with the number of JSON result files it contains\n",
    "for i, d in enumerate(experiment_batches, 1):\n",
    "    dir_path = os.path.join(RESULTS_DIR, d)\n",
    "    json_count = sum(\n",
    "        1 for f in os.listdir(dir_path)\n",
    "        if f.endswith('.json') and os.path.isfile(os.path.join(dir_path, f))\n",
    "    )\n",
    "    print(f\"{i}) {d} ({json_count} result{'s' if json_count != 1 else ''})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_ids_input = input(\"Choose experiment ID(s) (single number or comma-separated list, e.g., 1 or 1,2,3): \")\n",
    "\n",
    "selected_ids = []\n",
    "try:\n",
    "    # Parse input - handle both single number and comma-separated list\n",
    "    ids_str = experiment_ids_input.replace(\" \", \"\")  # Remove spaces\n",
    "    id_list = [int(x.strip()) for x in ids_str.split(\",\")]\n",
    "    \n",
    "    # Validate all IDs\n",
    "    for id_val in id_list:\n",
    "        if id_val < 1 or id_val > len(experiment_batches):\n",
    "            print(f\"Error: ID {id_val} is out of range (1-{len(experiment_batches)}). Skipping.\")\n",
    "        else:\n",
    "            selected_ids.append(id_val - 1)  # Convert to 0-indexed\n",
    "    \n",
    "    if not selected_ids:\n",
    "        print(f\"Error: No valid IDs provided. Using first experiment.\")\n",
    "        selected_ids = [0]\n",
    "    \n",
    "except ValueError:\n",
    "    print(f\"Error: Invalid input format. Please enter numbers separated by commas.\")\n",
    "    selected_ids = [0]\n",
    "\n",
    "# Get selected folder names and their base directories\n",
    "selected_experiment_dirs = [os.path.join(RESULTS_DIR, experiment_batches[id]) for id in selected_ids]\n",
    "exp_folder_names = [experiment_batches[id] for id in selected_ids]\n",
    "\n",
    "print(f\"\\nSelected {len(selected_ids)} experiment batch(es):\")\n",
    "for i, folder_name in enumerate(exp_folder_names, 1):\n",
    "    print(f\"  {i}) {folder_name}\")\n",
    "\n",
    "# Set EXPERIMENT_DIR to the first selected directory\n",
    "EXPERIMENT_DIR = selected_experiment_dirs[0]\n",
    "print(f\"\\nEXPERIMENT_DIR set to: {EXPERIMENT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all experiment files from selected directories\n",
    "all_exp_results_files = {}  # Dictionary: dir_path -> list of files\n",
    "\n",
    "for exp_dir in selected_experiment_dirs:\n",
    "    exp_results_files = [f for f in os.listdir(exp_dir) if f.endswith('.json')]\n",
    "    all_exp_results_files[exp_dir] = exp_results_files\n",
    "    print(f\"{exp_dir}: Found {len(exp_results_files)} JSON experiments\")\n",
    "\n",
    "total_experiments = sum(len(files) for files in all_exp_results_files.values())\n",
    "print(f\"\\nTotal: {total_experiments} experiments across all selected folders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "LOAD_FROM_CSV = False\n",
    "experiments_df = pd.DataFrame()\n",
    "folders_load_method = {}  # Track which load method (CSV or JSON) for each folder\n",
    "\n",
    "# Process each selected folder\n",
    "for exp_dir in selected_experiment_dirs:\n",
    "    folder_name = os.path.basename(exp_dir)\n",
    "    csv_files = [f for f in os.listdir(exp_dir) if f.endswith('.csv')]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing folder: {folder_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if csv_files:\n",
    "        print(f\"Found {len(csv_files)} CSV file(s):\")\n",
    "        for i, csv_file in enumerate(csv_files, 1):\n",
    "            print(f\"  {i}) {csv_file}\")\n",
    "        print(f\"  0) Load from JSON files (default)\")\n",
    "        \n",
    "        choice = input(f\"Choose data source for {folder_name} (0 for JSON, or CSV file number): \")\n",
    "        \n",
    "        try:\n",
    "            choice_id = int(choice)\n",
    "            if choice_id > 0 and choice_id <= len(csv_files):\n",
    "                csv_file = csv_files[choice_id - 1]\n",
    "                csv_path = os.path.join(exp_dir, csv_file)\n",
    "                print(f\"Loading from CSV: {csv_file}\")\n",
    "                df_temp = pd.read_csv(csv_path)\n",
    "                experiments_df = pd.concat([experiments_df, df_temp], ignore_index=True)\n",
    "                folders_load_method[folder_name] = f\"CSV ({csv_file})\"\n",
    "                LOAD_FROM_CSV = True\n",
    "            else:\n",
    "                print(f\"Loading from JSON files...\")\n",
    "                folders_load_method[folder_name] = \"JSON\"\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Loading from JSON files...\")\n",
    "            folders_load_method[folder_name] = \"JSON\"\n",
    "    else:\n",
    "        print(f\"No CSV files found. Will load from JSON files...\")\n",
    "        folders_load_method[folder_name] = \"JSON\"\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Summary of load methods:\")\n",
    "for folder, method in folders_load_method.items():\n",
    "    print(f\"  {folder}: {method}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# If CSV was loaded, we're done. If not, JSON will be loaded in the next cell.\n",
    "if LOAD_FROM_CSV:\n",
    "    print(f\"Loaded {len(experiments_df)} experiments from CSV files\")\n",
    "else:\n",
    "    print(\"Will load experiments from JSON files...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Check if any folders need to be loaded from JSON\n",
    "folders_to_load_json = [folder for folder, method in folders_load_method.items() if method == \"JSON\"]\n",
    "\n",
    "if folders_to_load_json:\n",
    "    # Iterate over all selected experiment directories\n",
    "    for exp_dir in selected_experiment_dirs:\n",
    "        folder_name = os.path.basename(exp_dir)\n",
    "        # Only load JSON for folders where user selected JSON option\n",
    "        if folders_load_method.get(folder_name) != \"JSON\":\n",
    "            continue\n",
    "        \n",
    "        exp_results_files = all_exp_results_files[exp_dir]\n",
    "        print(f\"\\nLoading experiments from {folder_name}...\")\n",
    "        \n",
    "        for experiment in exp_results_files:\n",
    "            try:\n",
    "                with open(os.path.join(exp_dir, experiment), 'r') as file:\n",
    "                    data = json.load(file)\n",
    "\n",
    "                    localization = data.get(\"final_report\", {}).get(\"localization\", [])\n",
    "                    if isinstance(localization, list):\n",
    "                        localization_str = \", \".join(localization)\n",
    "                    else:\n",
    "                        localization_str = None\n",
    "            \n",
    "                record = {\n",
    "                    \"experiment_file\": experiment,\n",
    "                    \"agent_id\" : data.get(\"agent_id\", None),\n",
    "                    \"agent_conf_name\" : data.get(\"agent_configuration_name\", None),\n",
    "                    \"scenario\": data.get(\"app_name\", None),\n",
    "                    \"fault_name\": data.get(\"testbed\", {}).get(\"fault_name\", None),\n",
    "                    \"target_namespace\": data.get(\"target_namespace\", None),\n",
    "                    \"trace_service_starting_point\": data.get(\"trace_service_starting_point\", None),\n",
    "                    \"rca_tasks_per_iteration\": data.get(\"testbed\", {}).get(\"rca_tasks_per_iteration\", 0),\n",
    "                    \"max_tool_calls\": data.get(\"testbed\", {}).get(\"max_tool_calls\", 0),\n",
    "                    \"execution_time_seconds\": data.get(\"stats\", {}).get(\"execution_time_seconds\", 0),\n",
    "                    \"total_tokens\": data.get(\"stats\", {}).get(\"total_tokens\", 0),\n",
    "                    \"tokens_triage\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"triage_agent\", {}).get(\"total_tokens\", 0),\n",
    "                    \"tokens_planner\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"planner_agent\", {}).get(\"total_tokens\", 0),\n",
    "                    \"tokens_rca_worker\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"rca_agent\", {}).get(\"total_tokens\", 0),\n",
    "                    \"runs_count_rca\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"rca_agent\", {}).get(\"runs_count\", 0),\n",
    "                    \"tokens_supervisor\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"supervisor_agent\", {}).get(\"total_tokens\", 0),\n",
    "                    \"runs_count_supervisor\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"supervisor_agent\", {}).get(\"runs_count\", 0),\n",
    "                    \"detection\": data.get(\"final_report\", {}).get(\"detection\", None),\n",
    "                    \"localization\": localization_str, \n",
    "                    \"root_cause\": data.get(\"final_report\", {}).get(\"root_cause\", None),\n",
    "                    \"eval_detection\" : data.get(\"evaluation\", {}).get(\"detection\", None),\n",
    "                    \"eval_localization\" : data.get(\"evaluation\", {}).get(\"localization\", None),\n",
    "                    \"eval_rca_score\" : data.get(\"evaluation\", {}).get(\"rca_score\", None),\n",
    "                    \"eval_rca_motivation\" : data.get(\"evaluation\", {}).get(\"rca_motivation\", None),\n",
    "                }\n",
    "                \n",
    "                # Append record to dataframe\n",
    "                experiments_df = pd.concat([experiments_df, pd.DataFrame([record])], ignore_index=True)\n",
    "            \n",
    "            except (json.JSONDecodeError, KeyError) as e:\n",
    "                print(f\"Warning: Error processing {experiment}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"Loaded {len(exp_results_files)} experiments from {os.path.basename(exp_dir)}\")\n",
    "    \n",
    "    print(f\"\\nTotal experiments loaded: {len(experiments_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the main app scenario\n",
    "scenarions = experiments_df[\"scenario\"].str.lower().unique()\n",
    "\n",
    "SCENARIOS_NAME = \", \".join(scenarions)\n",
    "\n",
    "SCENARIOS_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Load fault types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "FAULTS_CONF_DIR = os.environ.get(\"FAULTS_CONF_DIR\")\n",
    "\n",
    "faults_conf_files = [f for f in os.listdir(FAULTS_CONF_DIR) if f.endswith('.json')]\n",
    "\n",
    "print(f\"Found {len(faults_conf_files)} fault configuration files:\")\n",
    "\n",
    "for f in faults_conf_files:\n",
    "    print(f\"- {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fault_target_dict = {}\n",
    "\n",
    "for f in faults_conf_files:\n",
    "    with open(os.path.join(FAULTS_CONF_DIR,f), 'r') as file:\n",
    "        data = json.load(file)\n",
    "        formatted_key = f\"{data[\"app_name\"]} - {data[\"fault_type\"]}\"\n",
    "        fault_target_dict[formatted_key] = data.get(\"target\", None)\n",
    "\n",
    "fault_target_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Load agent configutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name_dict = {}\n",
    "agent_desc_dict = {}\n",
    "\n",
    "AGENTS_CONF_DIR = os.environ.get(\"AGENTS_CONF_DIR\")\n",
    "\n",
    "agents_conf_files = [f for f in os.listdir(AGENTS_CONF_DIR) if f.endswith('.json')]\n",
    "\n",
    "for f in agents_conf_files:\n",
    "    with open(os.path.join(AGENTS_CONF_DIR,f), 'r') as file:\n",
    "        data = json.load(file)\n",
    "        agent_name_dict[data.get(\"name\")] = data.get(\"id\")\n",
    "        agent_desc_dict[data.get(\"id\")] ={\n",
    "            \"name\" : data.get(\"name\"),\n",
    "            \"description\" : data.get(\"description\", \"\")\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add agent_id if not present in the results\n",
    "\n",
    "if experiments_df['agent_id'].isna().any():\n",
    "    experiments_df['agent_id'] = experiments_df['agent_conf_name'].map(\n",
    "        lambda name: next((id for conf, id in agent_name_dict.items() if conf in name), None)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import evaluation helpers from repo package\n",
    "import sys, os\n",
    "\n",
    "# Ensure we can import from `sre-agent` package\n",
    "sys.path.append(os.path.abspath(os.path.join(root_dir, 'sre-agent')))\n",
    "\n",
    "from evaluation.evaluation import evaluate_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build ground-truth mapping (target, RCA_gt) from faults configs\n",
    "faults_gt_dict = {}\n",
    "try:\n",
    "    for f in faults_conf_files:\n",
    "        with open(os.path.join(FAULTS_CONF_DIR, f), 'r') as file:\n",
    "            d = json.load(file)\n",
    "            key = f\"{d['app_name']} - {d['fault_type']}\"\n",
    "            faults_gt_dict[key] = {\n",
    "                \"target\": d.get(\"target\", None),\n",
    "                \"RCA_gt\": d.get(\"RCA_gt\", \"\")\n",
    "            }\n",
    "    print(f\"Loaded GT for {len(faults_gt_dict)} scenarios.\")\n",
    "except Exception as e:\n",
    "    print(f\"[warn] Failed building faults_gt_dict: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fault_scenario(row):\n",
    "    key = f\"{row['scenario']} - {row['fault_name']}\"\n",
    "    gt = faults_gt_dict.get(key, {})\n",
    "    return {\n",
    "        \"target\": gt.get(\"target\", None),\n",
    "        \"RCA_gt\": gt.get(\"RCA_gt\", \"\")\n",
    "    }\n",
    "\n",
    "def build_report_from_row(row):\n",
    "    # Convert localization to list for evaluate_experiment compatibility\n",
    "    loc_val = row['localization']\n",
    "    if isinstance(loc_val, str):\n",
    "        loc_list = [s.strip() for s in loc_val.split(',') if s.strip()]\n",
    "    elif isinstance(loc_val, list):\n",
    "        loc_list = loc_val\n",
    "    else:\n",
    "        loc_list = []\n",
    "    return {\n",
    "        \"agent_configuration_name\": row.get(\"agent_conf_name\", \"N/A\"),\n",
    "        \"final_report\": {\n",
    "            \"detection\": bool(row.get(\"detection\", False)),\n",
    "            \"localization\": loc_list,\n",
    "            \"root_cause\": row.get(\"root_cause\", \"\") if isinstance(row.get(\"root_cause\", \"\"), str) else \"\"\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "def backfill_missing_evaluations(limit: Optional[int] = None, dry_run: bool = False, override: bool = False) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    For rows where eval_* fields are missing, compute evaluation via evaluate_experiment,\n",
    "    persist to the result JSON (adds/overwrites the \"evaluation\" key), and update the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        limit: optional maximum number of rows to process.\n",
    "        dry_run: if True, do not write files; only compute and update the DataFrame in-memory.\n",
    "        override: if True, recompute evaluation for all rows; if False, only compute for missing values.\n",
    "\n",
    "    Returns:\n",
    "        (processed_count, file_updates)\n",
    "    \"\"\"\n",
    "    required_cols = ['eval_detection', 'eval_localization', 'eval_rca_score', 'eval_rca_motivation']\n",
    "    \n",
    "    if override:\n",
    "        idxs = experiments_df.index.tolist()\n",
    "    else:\n",
    "        missing_mask = experiments_df[required_cols].isna().any(axis=1)\n",
    "        idxs = experiments_df[missing_mask].index.tolist()\n",
    "    \n",
    "    if limit is not None:\n",
    "        idxs = idxs[:limit]\n",
    "    \n",
    "    processed = 0\n",
    "    updated_files = 0\n",
    "    \n",
    "    for idx in tqdm(idxs, desc=\"Backfilling evaluations\", unit=\"row\"):\n",
    "        row = experiments_df.loc[idx]\n",
    "        # Build inputs for evaluation\n",
    "        fault_scenario = build_fault_scenario(row)\n",
    "        report = build_report_from_row(row)\n",
    "        \n",
    "        # Run evaluation (this may call the LLM for RCA scoring)\n",
    "        evaluation = evaluate_experiment(fault_scenario, report)\n",
    "        if not isinstance(evaluation, dict):\n",
    "            print(f\"[warn] Row {idx}: evaluate_experiment returned non-dict; skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Update dataframe in-memory\n",
    "        experiments_df.loc[idx, 'eval_detection'] = evaluation.get('detection', None)\n",
    "        experiments_df.loc[idx, 'eval_localization'] = evaluation.get('localization', None)\n",
    "        experiments_df.loc[idx, 'eval_rca_score'] = evaluation.get('rca_score', None)\n",
    "        experiments_df.loc[idx, 'eval_rca_motivation'] = evaluation.get('rca_motivation', None)\n",
    "        \n",
    "        # Update json file\n",
    "        if not dry_run:\n",
    "            file_name = row['experiment_file']\n",
    "            file_path = os.path.join(EXPERIMENT_DIR, file_name)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                data['evaluation'] = evaluation\n",
    "                with open(file_path, 'w') as f:\n",
    "                    json.dump(data, f, indent=2)\n",
    "                updated_files += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] Row {idx}: failed writing evaluation to {file_path}: {e}\")\n",
    "        \n",
    "        processed += 1\n",
    "    \n",
    "    if override:\n",
    "        # Format the result as date and then \"experiment_results\"\n",
    "        date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        out_file = f\"experiment_results_{date_str}.csv\"\n",
    "        experiments_df.to_csv(os.path.join(EXPERIMENT_DIR, out_file), index=False)\n",
    "\n",
    "    print(f\"Backfill completed. Processed rows: {processed}. Files updated: {updated_files}.\")\n",
    "    return processed, updated_files\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "backfill_missing_evaluations(override=OVERRIDE_EVAL, dry_run=DRIRUN_EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "# Exclude results of fault scenarios\n",
    "\n",
    "Exclude results from fault scenarios that do not work on the VM due to virtualization limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of \"scenario - fault type\" from the faults_conf_files\n",
    "scenario_fault_list = []\n",
    "\n",
    "for f in faults_conf_files:\n",
    "    with open(os.path.join(FAULTS_CONF_DIR, f), 'r') as file:\n",
    "        data = json.load(file)\n",
    "        scenario = data.get(\"app_name\")\n",
    "        fault_type = data.get(\"fault_type\")\n",
    "        scenario_fault_list.append(f\"{scenario} - {fault_type}\")\n",
    "\n",
    "scenario_fault_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios_fault_to_exclude = [\n",
    "    \"hotel reservation - container kill\",\n",
    "    \"hotel reservation - pod kill\",\n",
    "    \"hotel reservation - redeploy without pv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out excluded fault scenarios from experiments_df\n",
    "experiments_df = experiments_df[~experiments_df.apply(\n",
    "    lambda row: f\"{row['scenario'].lower()} - {row['fault_name'].lower()}\" in scenarios_fault_to_exclude,\n",
    "    axis=1\n",
    ")]\n",
    "\n",
    "print(f\"Filtered experiments_df. Remaining rows: {len(experiments_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by fault type\n",
    "fault_group = experiments_df.groupby('fault_name').agg(\n",
    "    experiments=('experiment_file', 'count'),\n",
    "    detection_rate=('detection', 'mean'),\n",
    "    detection_accuracy=('eval_detection', 'mean'),\n",
    "    correct_detections=('eval_detection', 'sum'),\n",
    "    localization_accuracy=('eval_localization', 'mean'),\n",
    "    correct_localizations=('eval_localization', 'sum'),\n",
    "    avg_exec_time=('execution_time_seconds', 'mean'),\n",
    "    avg_tokens=('total_tokens', 'mean'),\n",
    "    avg_rca_score=('eval_rca_score', 'mean')\n",
    ")\n",
    "\n",
    "fault_group = fault_group.sort_values('localization_accuracy', ascending=False)\n",
    "fault_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by agent configuration\n",
    "agent_group = experiments_df.groupby('agent_id').agg(\n",
    "    experiments=('experiment_file', 'count'),\n",
    "    # detection_rate=('detection', 'mean'),\n",
    "    detection_accuracy=('eval_detection', 'mean'),\n",
    "    # correct_detections=('eval_detection', 'sum'),\n",
    "    localization_accuracy=('eval_localization', 'mean'),\n",
    "   # correct_localizations=('eval_localization', 'sum'),\n",
    "    avg_exec_time=('execution_time_seconds', 'mean'),\n",
    "    avg_tokens=('total_tokens', 'mean'),\n",
    "    avg_rca_score=('eval_rca_score', 'mean')\n",
    ")\n",
    "\n",
    "agent_group = agent_group.sort_values('localization_accuracy', ascending=False)\n",
    "agent_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by scenario (application) - convert to lowercase for consistent grouping\n",
    "scenario_group = experiments_df.groupby(experiments_df['scenario'].str.lower()).agg(\n",
    "    experiments=('experiment_file', 'count'),\n",
    "    # detection_rate=('detection', 'mean'),\n",
    "    detection_accuracy=('eval_detection', 'mean'),\n",
    "    localization_accuracy=('eval_localization', 'mean'),\n",
    "    avg_exec_time=('execution_time_seconds', 'mean'),\n",
    "    avg_tokens=('total_tokens', 'mean'),\n",
    "    avg_rca_score=('eval_rca_score', 'mean')\n",
    ")\n",
    "\n",
    "scenario_group = scenario_group.sort_values('localization_accuracy', ascending=False)\n",
    "scenario_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Global counter for plot numbering\n",
    "plot_counter = 0\n",
    "plots_dir = os.path.join(EXPERIMENT_DIR, 'plots')\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "if SAVE_IMAGES:\n",
    "    if not os.path.exists(plots_dir):\n",
    "        os.makedirs(plots_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the palette\n",
    "from pypalettes import load_cmap\n",
    "cmap = load_cmap(\"Color_Blind\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# Get unique fault types for coloring\n",
    "fault_types = experiments_df['fault_name'].unique()\n",
    "colors = cmap(np.linspace(0, 1, len(fault_types)))\n",
    "color_map = {fault: colors[i] for i, fault in enumerate(fault_types)}\n",
    "\n",
    "# Plot each experiment as a dot\n",
    "for idx, row in experiments_df.iterrows():\n",
    "    x = row['execution_time_seconds']\n",
    "    y = row['total_tokens']\n",
    "    fault = row['fault_name']\n",
    "    color = color_map[fault]\n",
    "    detection = row['eval_detection']\n",
    "    \n",
    "    # Plot the dot - filled if detection is True, only border if False\n",
    "    if detection:\n",
    "        ax.scatter(x, y, s=100, color=color, alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "    else:\n",
    "        ax.scatter(x, y, s=100, facecolors='none', edgecolors=color, linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # Add label with agent configuration on top of the dot\n",
    "    label = f\"{row['agent_id']}\"\n",
    "    ax.annotate(label, (x, y), fontsize=8, ha='center', va='bottom', fontweight='bold', \n",
    "                xytext=(0, 7), textcoords='offset points')\n",
    "\n",
    "# Create legend for fault types\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map[fault], \n",
    "                       markersize=8, label=fault) for fault in fault_types]\n",
    "\n",
    "# Add detection legend\n",
    "detection_handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', \n",
    "               markersize=8, label='Correct detection', markeredgecolor='black', markeredgewidth=0.5),\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='none', \n",
    "               markersize=8, label='Wrong detection', markeredgecolor='gray', markeredgewidth=2)\n",
    "]\n",
    "\n",
    "# Combine all handles into a single legend\n",
    "all_handles = handles + detection_handles\n",
    "ax.legend(handles=all_handles, loc='upper left', fontsize=10)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Execution Time (seconds)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Total Tokens', fontsize=12, fontweight='bold')\n",
    "ax.set_title(f'Detection Accuracy: Execution Time vs Token Usage - {SCENARIOS_NAME.title()}', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Grid\n",
    "ax.grid(True, alpha=0.3, linestyle='--', linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVE_IMAGES:\n",
    "    plot_counter += 1\n",
    "    filename = os.path.join(plots_dir, f'{plot_counter:03d}_scatter_detection_accuracy.pdf')\n",
    "\n",
    "    fig.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    print(f\"Saved plot to {filename}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "# Get unique fault types for coloring\n",
    "fault_types = experiments_df['fault_name'].unique()\n",
    "colors = cmap(np.linspace(0, 1, len(fault_types)))\n",
    "color_map = {fault: colors[i] for i, fault in enumerate(fault_types)}\n",
    "\n",
    "# Collect points and labels for later adjustment\n",
    "texts = []\n",
    "\n",
    "# Plot each experiment as a dot\n",
    "for idx, row in experiments_df.iterrows():\n",
    "    x = row['execution_time_seconds']\n",
    "    y = row['total_tokens']\n",
    "    fault = row['fault_name']\n",
    "    color = color_map[fault]\n",
    "    correct_localization = row['eval_localization']\n",
    "    \n",
    "    # Plot the dot - filled if correct_localization is True, X marker if False\n",
    "    if correct_localization:\n",
    "        ax.scatter(x, y, s=150, color=color, alpha=0.8, edgecolors='black', linewidth=1.2)\n",
    "    else:\n",
    "        ax.scatter(x, y, s=150, marker='x', color=color, linewidth=2.5, alpha=0.8)\n",
    "    \n",
    "    # Add label with agent configuration\n",
    "    label = f\"{row['agent_id']}\"\n",
    "    text_obj = ax.text(x, y, label, fontsize=9, ha='center', va='center', fontweight='bold',\n",
    "                       bbox=dict(boxstyle='round,pad=0.4', facecolor='white', edgecolor='gray', alpha=0.85, linewidth=0.8),\n",
    "                       zorder=10)\n",
    "    texts.append(text_obj)\n",
    "\n",
    "# Adjust text positions to avoid overlaps\n",
    "try:\n",
    "    adjust_text(texts, arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2', \n",
    "                                       lw=0.8, color='gray', alpha=0.6),\n",
    "                ax=ax, precision=0.1, expand_points=(1.5, 1.5))\n",
    "except ImportError:\n",
    "    print(\"Note: adjustText not installed. Labels may overlap. Install with: pip install adjustText\")\n",
    "\n",
    "# Create legend for fault types\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map[fault], \n",
    "                       markersize=10, label=fault, markeredgecolor='black', markeredgewidth=1) \n",
    "           for fault in sorted(fault_types)]\n",
    "\n",
    "# Add localization legend\n",
    "localization_handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='gray', \n",
    "               markersize=10, label='Correct Localization', markeredgecolor='black', markeredgewidth=1),\n",
    "    plt.Line2D([0], [0], marker='x', color='gray', linestyle='None',\n",
    "               markersize=10, label='Wrong Localization', markeredgewidth=2.5)\n",
    "]\n",
    "\n",
    "# Combine all handles into a single legend\n",
    "all_handles = handles + localization_handles\n",
    "ax.legend(handles=all_handles, loc='upper left', fontsize=11, framealpha=0.95, edgecolor='black')\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Execution Time (seconds)', fontsize=14, fontweight='bold', color='#2c3e50')\n",
    "ax.set_ylabel('Total Tokens', fontsize=14, fontweight='bold', color='#2c3e50')\n",
    "ax.set_title(f'Localization Accuracy: Execution Time vs Token Usage - {SCENARIOS_NAME.title()}\\n(by Fault Type and Agent Configuration)', \n",
    "             fontsize=16, fontweight='bold', pad=20, color='#2c3e50')\n",
    "\n",
    "# Grid and background\n",
    "ax.set_facecolor('#f8f9fa')\n",
    "ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8, color='#bdc3c7')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('#bdc3c7')\n",
    "ax.spines['bottom'].set_color('#bdc3c7')\n",
    "ax.spines['left'].set_linewidth(1.2)\n",
    "ax.spines['bottom'].set_linewidth(1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVE_IMAGES:\n",
    "    plot_counter += 1\n",
    "    filename = os.path.join(plots_dir, f'{plot_counter:03d}_scatter_localization_accuracy.pdf')\n",
    "\n",
    "    fig.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    print(f\"Saved plot to {filename}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adjustText import adjust_text\n",
    "\n",
    "# Visualization: RCA score vs Execution Time and Token Usage\n",
    "\n",
    "# Use existing variables: experiments_df, cmap, plt, np\n",
    "fig, ax = plt.subplots(figsize=(16, 9))\n",
    "\n",
    "# Color by fault type (reuse palette)\n",
    "fault_types = experiments_df['fault_name'].unique()\n",
    "colors = cmap(np.linspace(0, 1, len(fault_types)))\n",
    "color_map = {fault: colors[i] for i, fault in enumerate(fault_types)}\n",
    "\n",
    "# Point size encodes RCA score (0–5); shape encodes correctness of localization\n",
    "def size_for_score(score: int) -> float:\n",
    "    # Base size + scale; ensure visible even for low scores\n",
    "    return 80 + 60 * max(0, int(score))\n",
    "\n",
    "texts = []\n",
    "\n",
    "for _, row in experiments_df.iterrows():\n",
    "    x = row['execution_time_seconds']\n",
    "    y = row['total_tokens']\n",
    "    fault = row['fault_name']\n",
    "    score = row['eval_rca_score']\n",
    "    correct_loc = row['eval_localization']\n",
    "    color = color_map[fault]\n",
    "    s = size_for_score(score)\n",
    "\n",
    "    # Circle for correct localization, 'x' for incorrect\n",
    "    if correct_loc:\n",
    "        ax.scatter(x, y, s=s, color=color, alpha=0.8, edgecolors='black', linewidth=1.0)\n",
    "    else:\n",
    "        ax.scatter(x, y, s=s, marker='x', color=color, linewidth=2.5, alpha=0.9)\n",
    "\n",
    "    # Label with agent_id and score\n",
    "    label = f\"{row['agent_id']} (S{score})\"\n",
    "    text = ax.text(x, y, label, fontsize=9, ha='center', va='center', fontweight='bold',\n",
    "                   bbox=dict(boxstyle='round,pad=0.35', facecolor='white', edgecolor='gray', alpha=0.85, linewidth=0.8),\n",
    "                   zorder=10)\n",
    "    texts.append(text)\n",
    "\n",
    "# Optional: adjust_text if already imported above\n",
    "try:\n",
    "    adjust_text(texts, arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2',\n",
    "                                       lw=0.8, color='gray', alpha=0.6),\n",
    "                ax=ax, precision=0.1, expand_points=(1.5, 1.5))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Legend: fault type colors\n",
    "fault_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map[f],\n",
    "                            markeredgecolor='black', markeredgewidth=1, markersize=10, label=f)\n",
    "                 for f in sorted(fault_types)]\n",
    "\n",
    "# Legend: localization correctness\n",
    "loc_handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='gray',\n",
    "               markeredgecolor='black', markeredgewidth=1, markersize=10, label='Correct Localization'),\n",
    "    plt.Line2D([0], [0], marker='x', color='gray', linestyle='None',\n",
    "               markeredgewidth=2.5, markersize=10, label='Incorrect Localization'),\n",
    "]\n",
    "\n",
    "# Legend: RCA score sizes (show exemplars)\n",
    "score_examples = [0, 1, 3, 5]\n",
    "size_handles = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "                           markerfacecolor='lightgray', markeredgecolor='black',\n",
    "                           markeredgewidth=1, markersize=np.sqrt(size_for_score(s))/1.5,\n",
    "                           label=f'RCA Score: {s}') for s in score_examples]\n",
    "\n",
    "all_handles = fault_handles + loc_handles + size_handles\n",
    "ax.legend(handles=all_handles, loc='upper left', fontsize=11, framealpha=0.95, edgecolor='black')\n",
    "\n",
    "# Axes styling\n",
    "ax.set_xlabel('Execution Time (seconds)', fontsize=14, fontweight='bold', color='#2c3e50')\n",
    "ax.set_ylabel('Total Tokens', fontsize=14, fontweight='bold', color='#2c3e50')\n",
    "ax.set_title(f'RCA Score Visualization: Execution Time vs Token Usage - {SCENARIOS_NAME.title()}\\n(size encodes RCA score; color by Fault Type)',\n",
    "             fontsize=16, fontweight='bold', pad=20, color='#2c3e50')\n",
    "\n",
    "ax.set_facecolor('#f8f9fa')\n",
    "ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8, color='#bdc3c7')\n",
    "ax.set_axisbelow(True)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_color('#bdc3c7')\n",
    "ax.spines['bottom'].set_color('#bdc3c7')\n",
    "ax.spines['left'].set_linewidth(1.2)\n",
    "ax.spines['bottom'].set_linewidth(1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVE_IMAGES:\n",
    "    plot_counter += 1\n",
    "    filename = os.path.join(plots_dir, f'{plot_counter:03d}_scatter_rca_score.pdf')\n",
    "\n",
    "    fig.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    print(f\"Saved plot to {filename}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with two subplots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Prepare all data for global box plots\n",
    "all_time_data = experiments_df['execution_time_seconds'].values\n",
    "all_token_data = experiments_df['total_tokens'].values\n",
    "\n",
    "# Get unique fault types for coloring individual points\n",
    "fault_types = sorted(experiments_df['fault_name'].unique())\n",
    "colors_array = cmap(np.linspace(0, 1, len(fault_types)))\n",
    "color_map = {fault: colors_array[i] for i, fault in enumerate(fault_types)}\n",
    "\n",
    "# Box plot for execution time (global)\n",
    "bp1 = axes[0].boxplot([all_time_data], tick_labels=['All Experiments'], patch_artist=True, \n",
    "                      widths=0.4, showmeans=True,\n",
    "                      boxprops=dict(linewidth=1.5),\n",
    "                      whiskerprops=dict(linewidth=1.5),\n",
    "                      capprops=dict(linewidth=1.5),\n",
    "                      medianprops=dict(linewidth=2, color='darkred'),\n",
    "                      meanprops=dict(marker='D', markerfacecolor='white', \n",
    "                                    markeredgecolor='black', markersize=8))\n",
    "\n",
    "bp1['boxes'][0].set_facecolor('lightgray')\n",
    "bp1['boxes'][0].set_alpha(0.5)\n",
    "\n",
    "# Overlay individual experiments as dots\n",
    "for idx, row in experiments_df.iterrows():\n",
    "    fault = row['fault_name']\n",
    "    color = color_map[fault]\n",
    "    # Add small jitter to x position for visibility\n",
    "    x_pos = 1 + np.random.uniform(-0.15, 0.15)\n",
    "    axes[0].scatter(x_pos, row['execution_time_seconds'], s=60, color=color, \n",
    "                   alpha=0.7, edgecolors='black', linewidth=0.5, zorder=3)\n",
    "\n",
    "axes[0].set_ylabel('Execution Time (seconds)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Execution Time Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "axes[0].set_facecolor('#f8f9fa')\n",
    "axes[0].set_xlim(0.5, 1.5)\n",
    "\n",
    "# Box plot for token usage (global)\n",
    "bp2 = axes[1].boxplot([all_token_data], tick_labels=['All Experiments'], patch_artist=True,\n",
    "                      widths=0.4, showmeans=True,\n",
    "                      boxprops=dict(linewidth=1.5),\n",
    "                      whiskerprops=dict(linewidth=1.5),\n",
    "                      capprops=dict(linewidth=1.5),\n",
    "                      medianprops=dict(linewidth=2, color='darkred'),\n",
    "                      meanprops=dict(marker='D', markerfacecolor='white', \n",
    "                                    markeredgecolor='black', markersize=8))\n",
    "\n",
    "bp2['boxes'][0].set_facecolor('lightgray')\n",
    "bp2['boxes'][0].set_alpha(0.5)\n",
    "\n",
    "# Overlay individual experiments as dots\n",
    "for idx, row in experiments_df.iterrows():\n",
    "    fault = row['fault_name']\n",
    "    color = color_map[fault]\n",
    "    x_pos = 1 + np.random.uniform(-0.15, 0.15)\n",
    "    axes[1].scatter(x_pos, row['total_tokens'], s=60, color=color, \n",
    "                   alpha=0.7, edgecolors='black', linewidth=0.5, zorder=3)\n",
    "\n",
    "axes[1].set_ylabel('Total Tokens', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Token Usage Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "axes[1].set_facecolor('#f8f9fa')\n",
    "axes[1].set_xlim(0.5, 1.5)\n",
    "\n",
    "# Create legend for fault types (moved below the plots)\n",
    "fault_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map[fault],\n",
    "                            markeredgecolor='black', markeredgewidth=0.5, markersize=8, label=fault)\n",
    "                 for fault in fault_types]\n",
    "fig.legend(handles=fault_handles, loc='lower center', ncol=5, fontsize=10,\n",
    "           framealpha=0.95, edgecolor='black', bbox_to_anchor=(0.5, -0.02))\n",
    "\n",
    "# Add main title with scenarios\n",
    "fig.suptitle(f'Global Performance Distribution - {SCENARIOS_NAME.title()}', fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "# Leave space at the bottom for the legend\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.96])\n",
    "if SAVE_IMAGES:\n",
    "    plot_counter += 1\n",
    "    filename = os.path.join(plots_dir, f'{plot_counter:03d}_boxplot_global_distribution.pdf')\n",
    "\n",
    "    fig.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    print(f\"Saved plot to {filename}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data grouped by fault_name for per-fault box plots\n",
    "fault_types = sorted(experiments_df['fault_name'].unique())\n",
    "colors_array = cmap(np.linspace(0, 1, len(fault_types)))\n",
    "color_map = {fault: colors_array[i] for i, fault in enumerate(fault_types)}\n",
    "\n",
    "# Build per-fault arrays\n",
    "time_data = [\n",
    "    experiments_df[experiments_df['fault_name'] == fault]['execution_time_seconds'].values\n",
    "    for fault in fault_types\n",
    "]\n",
    "token_data = [\n",
    "    experiments_df[experiments_df['fault_name'] == fault]['total_tokens'].values\n",
    "    for fault in fault_types\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Create tick labels with numbers\n",
    "tick_labels = [f\"{i+1}\" for i in range(len(fault_types))]\n",
    "\n",
    "# Execution time per fault\n",
    "bp_time = axes[0].boxplot(\n",
    "    time_data,\n",
    "    tick_labels=tick_labels,\n",
    "    patch_artist=True,\n",
    "    widths=0.6,\n",
    "    showmeans=True,\n",
    "    boxprops=dict(linewidth=1.5),\n",
    "    whiskerprops=dict(linewidth=1.5),\n",
    "    capprops=dict(linewidth=1.5),\n",
    "    medianprops=dict(linewidth=2, color='darkred'),\n",
    "    meanprops=dict(marker='D', markerfacecolor='white', markeredgecolor='black', markersize=7)\n",
    ")\n",
    "# Color boxes by fault type\n",
    "for i, box in enumerate(bp_time['boxes']):\n",
    "    box.set_facecolor(color_map[fault_types[i]])\n",
    "    box.set_alpha(0.35)\n",
    "\n",
    "# Overlay individual points with jitter per fault\n",
    "for i, fault in enumerate(fault_types, start=1):\n",
    "    data = experiments_df[experiments_df['fault_name'] == fault]\n",
    "    jitter_x = i + np.random.uniform(-0.15, 0.15, size=len(data))\n",
    "    axes[0].scatter(\n",
    "        jitter_x,\n",
    "        data['execution_time_seconds'],\n",
    "        s=60,\n",
    "        color=color_map[fault],\n",
    "        alpha=0.85,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5,\n",
    "        zorder=3\n",
    "    )\n",
    "\n",
    "axes[0].set_ylabel('Execution Time (seconds)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Execution Time Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "axes[0].set_facecolor('#f8f9fa')\n",
    "\n",
    "# Tokens per fault\n",
    "bp_tokens = axes[1].boxplot(\n",
    "    token_data,\n",
    "    tick_labels=tick_labels,\n",
    "    patch_artist=True,\n",
    "    widths=0.6,\n",
    "    showmeans=True,\n",
    "    boxprops=dict(linewidth=1.5),\n",
    "    whiskerprops=dict(linewidth=1.5),\n",
    "    capprops=dict(linewidth=1.5),\n",
    "    medianprops=dict(linewidth=2, color='darkred'),\n",
    "    meanprops=dict(marker='D', markerfacecolor='white', markeredgecolor='black', markersize=7)\n",
    ")\n",
    "for i, box in enumerate(bp_tokens['boxes']):\n",
    "    box.set_facecolor(color_map[fault_types[i]])\n",
    "    box.set_alpha(0.35)\n",
    "\n",
    "for i, fault in enumerate(fault_types, start=1):\n",
    "    data = experiments_df[experiments_df['fault_name'] == fault]\n",
    "    jitter_x = i + np.random.uniform(-0.15, 0.15, size=len(data))\n",
    "    axes[1].scatter(\n",
    "        jitter_x,\n",
    "        data['total_tokens'],\n",
    "        s=60,\n",
    "        color=color_map[fault],\n",
    "        alpha=0.85,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5,\n",
    "        zorder=3\n",
    "    )\n",
    "\n",
    "axes[1].set_ylabel('Total Tokens', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Token Usage Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "axes[1].set_facecolor('#f8f9fa')\n",
    "\n",
    "# Create legend with numbers and fault names\n",
    "fault_handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color_map[fault],\n",
    "                markeredgecolor='black', markeredgewidth=0.5, markersize=8, \n",
    "                label=f\"{i+1}. {fault}\")\n",
    "    for i, fault in enumerate(fault_types)\n",
    "]\n",
    "fig.legend(handles=fault_handles, loc='lower center', ncol=3, fontsize=10,\n",
    "           framealpha=0.95, edgecolor='black', bbox_to_anchor=(0.5, -0.05))\n",
    "\n",
    "# Add main title with scenarios\n",
    "fig.suptitle(f'Performance Distribution by Fault Type - {SCENARIOS_NAME.title()}', fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.08, 1, 0.96])\n",
    "if SAVE_IMAGES:\n",
    "    plot_counter += 1\n",
    "    filename = os.path.join(plots_dir, f'{plot_counter:03d}_boxplot_by_fault_type.pdf')\n",
    "\n",
    "    fig.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    print(f\"Saved plot to {filename}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots by agent type (agent_id)\n",
    "\n",
    "# Prepare data grouped by agent_id\n",
    "agent_types = sorted(experiments_df['agent_id'].unique())\n",
    "colors_array = cmap(np.linspace(0, 1, len(agent_types)))\n",
    "agent_color_map = {agent: colors_array[i] for i, agent in enumerate(agent_types)}\n",
    "\n",
    "time_data_agent = [\n",
    "    experiments_df[experiments_df['agent_id'] == agent]['execution_time_seconds'].values\n",
    "    for agent in agent_types\n",
    "]\n",
    "token_data_agent = [\n",
    "    experiments_df[experiments_df['agent_id'] == agent]['total_tokens'].values\n",
    "    for agent in agent_types\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 10))\n",
    "\n",
    "# Execution time per agent\n",
    "bp_time = axes[0].boxplot(\n",
    "    time_data_agent,\n",
    "    tick_labels=agent_types,\n",
    "    patch_artist=True,\n",
    "    widths=0.6,\n",
    "    showmeans=True,\n",
    "    boxprops=dict(linewidth=1.5),\n",
    "    whiskerprops=dict(linewidth=1.5),\n",
    "    capprops=dict(linewidth=1.5),\n",
    "    medianprops=dict(linewidth=2, color='darkred'),\n",
    "    meanprops=dict(marker='D', markerfacecolor='white', markeredgecolor='black', markersize=7)\n",
    ")\n",
    "for i, box in enumerate(bp_time['boxes']):\n",
    "    box.set_facecolor(agent_color_map[agent_types[i]])\n",
    "    box.set_alpha(0.35)\n",
    "\n",
    "# Overlay individual points with jitter per agent\n",
    "for i, agent in enumerate(agent_types, start=1):\n",
    "    data = experiments_df[experiments_df['agent_id'] == agent]\n",
    "    jitter_x = i + np.random.uniform(-0.15, 0.15, size=len(data))\n",
    "    axes[0].scatter(\n",
    "        jitter_x,\n",
    "        data['execution_time_seconds'],\n",
    "        s=60,\n",
    "        color=agent_color_map[agent],\n",
    "        alpha=0.85,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5,\n",
    "        zorder=3\n",
    "    )\n",
    "\n",
    "axes[0].set_ylabel('Execution Time (seconds)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Execution Time Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "axes[0].set_facecolor('#f8f9fa')\n",
    "\n",
    "# Tokens per agent\n",
    "bp_tokens = axes[1].boxplot(\n",
    "    token_data_agent,\n",
    "    tick_labels=agent_types,\n",
    "    patch_artist=True,\n",
    "    widths=0.6,\n",
    "    showmeans=True,\n",
    "    boxprops=dict(linewidth=1.5),\n",
    "    whiskerprops=dict(linewidth=1.5),\n",
    "    capprops=dict(linewidth=1.5),\n",
    "    medianprops=dict(linewidth=2, color='darkred'),\n",
    "    meanprops=dict(marker='D', markerfacecolor='white', markeredgecolor='black', markersize=7)\n",
    ")\n",
    "for i, box in enumerate(bp_tokens['boxes']):\n",
    "    box.set_facecolor(agent_color_map[agent_types[i]])\n",
    "    box.set_alpha(0.35)\n",
    "\n",
    "for i, agent in enumerate(agent_types, start=1):\n",
    "    data = experiments_df[experiments_df['agent_id'] == agent]\n",
    "    jitter_x = i + np.random.uniform(-0.15, 0.15, size=len(data))\n",
    "    axes[1].scatter(\n",
    "        jitter_x,\n",
    "        data['total_tokens'],\n",
    "        s=60,\n",
    "        color=agent_color_map[agent],\n",
    "        alpha=0.85,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5,\n",
    "        zorder=3\n",
    "    )\n",
    "\n",
    "axes[1].set_ylabel('Total Tokens', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Token Usage Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "axes[1].set_facecolor('#f8f9fa')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Legend for agent types\n",
    "agent_handles = [\n",
    "    plt.Line2D([0], [0], marker='o', color='w',\n",
    "               markerfacecolor=agent_color_map[agent],\n",
    "               markeredgecolor='black', markeredgewidth=0.5,\n",
    "               markersize=8, label=f'{agent}: {agent_desc_dict[agent][\"name\"]}')\n",
    "    for agent in agent_types\n",
    "]\n",
    "fig.legend(handles=agent_handles, loc='lower center', ncol=4, fontsize=10,\n",
    "           framealpha=0.95, edgecolor='black', bbox_to_anchor=(0.5, -0.02))\n",
    "\n",
    "# Add main title with scenarios\n",
    "fig.suptitle(f'Performance Distribution by Agent Configuration - {SCENARIOS_NAME.title()}', fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.96])\n",
    "if SAVE_IMAGES:\n",
    "    plot_counter += 1\n",
    "    filename = os.path.join(plots_dir, f'{plot_counter:03d}_boxplot_by_agent_type.pdf')\n",
    "\n",
    "    fig.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "\n",
    "    print(f\"Saved plot to {filename}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Interactive Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Prepare data\n",
    "plot_df = experiments_df.copy()\n",
    "plot_df['hover_text'] = plot_df.apply(lambda row: \n",
    "    f\"<b>Agent:</b> {row['agent_id']}<br>\"\n",
    "    f\"<b>Scenario:</b> {row['scenario']}<br>\"\n",
    "    f\"<b>Fault:</b> {row['fault_name']}<br>\"\n",
    "    f\"<b>Time:</b> {row['execution_time_seconds']:.1f}s<br>\"\n",
    "    f\"<b>Tokens:</b> {int(row['total_tokens']):,}<br>\"\n",
    "    f\"<b>Detection:</b> {'✓' if row['eval_detection'] else '✗'}<br>\"\n",
    "    f\"<b>Localization:</b> {'✓' if row['eval_localization'] else '✗'}<br>\"\n",
    "    f\"<b>RCA Score:</b> {row['eval_rca_score']}/5\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Color palette from pypalettes\n",
    "fault_types = sorted(plot_df['fault_name'].unique())\n",
    "colors_array = cmap(np.linspace(0, 1, len(fault_types)))\n",
    "color_map = {fault: f'rgba({int(c[0]*255)},{int(c[1]*255)},{int(c[2]*255)},0.8)' \n",
    "             for fault, c in zip(fault_types, colors_array)}\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "for fault in fault_types:\n",
    "    data = plot_df[plot_df['fault_name'] == fault]\n",
    "\n",
    "    # Partition by localization correctness (treat NaN as incorrect/missing)\n",
    "    correct = data[data['eval_localization'] == True]\n",
    "    incorrect = data[data['eval_localization'] == False]\n",
    "\n",
    "    legend_shown = False\n",
    "\n",
    "    # Correct localization (circles)\n",
    "    if len(correct) > 0:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=correct['execution_time_seconds'],\n",
    "            y=correct['total_tokens'],\n",
    "            mode='markers+text',\n",
    "            name=fault,  # ensure one legend entry per fault\n",
    "            marker=dict(\n",
    "                size=correct['eval_rca_score'] * 8 + 15,\n",
    "                color=color_map[fault],\n",
    "                line=dict(width=2, color='white'),\n",
    "                symbol='circle'\n",
    "            ),\n",
    "            text=correct['agent_id'],\n",
    "            textposition='middle center',\n",
    "            textfont=dict(size=10, color='white', family='Arial Black'),\n",
    "            customdata=correct['hover_text'],\n",
    "            hovertemplate='%{customdata}<extra></extra>',\n",
    "            legendgroup=fault,\n",
    "            showlegend=True\n",
    "        ))\n",
    "        legend_shown = True\n",
    "\n",
    "    # Incorrect localization (X markers)\n",
    "    if len(incorrect) > 0:\n",
    "        # If no correct points exist, show this trace in legend under the fault name\n",
    "        name_for_incorrect = fault if not legend_shown else f'{fault} (incorrect)'\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=incorrect['execution_time_seconds'],\n",
    "            y=incorrect['total_tokens'],\n",
    "            mode='markers+text',\n",
    "            name=name_for_incorrect,\n",
    "            marker=dict(\n",
    "                size=incorrect['eval_rca_score'] * 8 + 15,\n",
    "                color=color_map[fault],\n",
    "                line=dict(width=3, color=color_map[fault]),\n",
    "                symbol='x',\n",
    "                opacity=0.6\n",
    "            ),\n",
    "            text=incorrect['agent_id'],\n",
    "            textposition='middle center',\n",
    "            textfont=dict(size=10, color=color_map[fault], family='Arial Black'),\n",
    "            customdata=incorrect['hover_text'],\n",
    "            hovertemplate='%{customdata}<extra></extra>',\n",
    "            legendgroup=fault,\n",
    "            showlegend=not legend_shown\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text='<b>Experiment Performance Dashboard</b><br><sub>Execution Time vs Token Usage | Size: RCA Score | Shape: Localization Accuracy</sub>',\n",
    "        x=0.5,\n",
    "        xanchor='center',\n",
    "        font=dict(size=20, color='#2c3e50')\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        title='<b>Execution Time (seconds)</b>',\n",
    "        showgrid=True,\n",
    "        gridcolor='rgba(200,200,200,0.3)',\n",
    "        zeroline=False,\n",
    "        title_font=dict(size=14, color='#34495e')\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='<b>Total Tokens</b>',\n",
    "        showgrid=True,\n",
    "        gridcolor='rgba(200,200,200,0.3)',\n",
    "        zeroline=False,\n",
    "        title_font=dict(size=14, color='#34495e')\n",
    "    ),\n",
    "    plot_bgcolor='#f8f9fa',\n",
    "    paper_bgcolor='white',\n",
    "    height=750,\n",
    "    hovermode='closest',\n",
    "    font=dict(family='Arial, sans-serif', size=12),\n",
    "    legend=dict(\n",
    "        title=dict(text='<b>Fault Types</b>', font=dict(size=13)),\n",
    "        orientation='v',\n",
    "        yanchor='top',\n",
    "        y=0.98,\n",
    "        xanchor='left',\n",
    "        x=0.01,\n",
    "        bgcolor='rgba(255,255,255,0.9)',\n",
    "        bordercolor='#bdc3c7',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    margin=dict(l=80, r=30, t=100, b=80)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Interactive Box Plot by Fault Type\n",
    "fault_types = sorted(experiments_df['fault_name'].unique())\n",
    "colors_array = cmap(np.linspace(0, 1, len(fault_types)))\n",
    "fault_color_map = {fault: f'rgba({int(c[0]*255)},{int(c[1]*255)},{int(c[2]*255)},0.8)' \n",
    "                   for fault, c in zip(fault_types, colors_array)}\n",
    "\n",
    "fig_fault = make_subplots(rows=1, cols=2, \n",
    "                          subplot_titles=['<b>Execution Time by Fault Type</b>', \n",
    "                                         '<b>Token Usage by Fault Type</b>'],\n",
    "                          horizontal_spacing=0.08)\n",
    "\n",
    "for i, fault in enumerate(fault_types):\n",
    "    data = experiments_df[experiments_df['fault_name'] == fault]\n",
    "    color = fault_color_map[fault]\n",
    "    \n",
    "    # Hover text for individual points\n",
    "    hover_texts = data.apply(lambda row: \n",
    "        f\"<b>Agent:</b> {row['agent_id']}<br>\"\n",
    "        f\"<b>Fault:</b> {row['fault_name']}<br>\"\n",
    "        f\"<b>Time:</b> {row['execution_time_seconds']:.1f}s<br>\"\n",
    "        f\"<b>Tokens:</b> {int(row['total_tokens']):,}<br>\"\n",
    "        f\"<b>Detection:</b> {'✓' if row['eval_detection'] else '✗'}<br>\"\n",
    "        f\"<b>Localization:</b> {'✓' if row['eval_localization'] else '✗'}<br>\"\n",
    "        f\"<b>RCA Score:</b> {row['eval_rca_score']}/5\",\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "    \n",
    "    # Execution time box plot\n",
    "    fig_fault.add_trace(go.Box(\n",
    "        y=data['execution_time_seconds'],\n",
    "        name=fault,\n",
    "        marker_color=color,\n",
    "        boxpoints='all',\n",
    "        jitter=0.4,\n",
    "        pointpos=0,\n",
    "        marker=dict(size=8, opacity=0.7, line=dict(width=1, color='white')),\n",
    "        hovertext=hover_texts,\n",
    "        hoverinfo='text',\n",
    "        legendgroup=fault,\n",
    "        showlegend=True\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    # Token usage box plot\n",
    "    fig_fault.add_trace(go.Box(\n",
    "        y=data['total_tokens'],\n",
    "        name=fault,\n",
    "        marker_color=color,\n",
    "        boxpoints='all',\n",
    "        jitter=0.4,\n",
    "        pointpos=0,\n",
    "        marker=dict(size=8, opacity=0.7, line=dict(width=1, color='white')),\n",
    "        hovertext=hover_texts,\n",
    "        hoverinfo='text',\n",
    "        legendgroup=fault,\n",
    "        showlegend=False\n",
    "    ), row=1, col=2)\n",
    "\n",
    "fig_fault.update_layout(\n",
    "    title=dict(\n",
    "        text='<b>Performance Distribution by Fault Type</b>',\n",
    "        x=0.5,\n",
    "        xanchor='center',\n",
    "        font=dict(size=20, color='#2c3e50')\n",
    "    ),\n",
    "    plot_bgcolor='#f8f9fa',\n",
    "    paper_bgcolor='white',\n",
    "    height=600,\n",
    "    font=dict(family='Arial, sans-serif', size=12),\n",
    "    legend=dict(\n",
    "        title=dict(text='<b>Fault Types</b>', font=dict(size=13)),\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=-0.25,\n",
    "        xanchor='center',\n",
    "        x=0.5,\n",
    "        bgcolor='rgba(255,255,255,0.9)',\n",
    "        bordercolor='#bdc3c7',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    margin=dict(l=80, r=30, t=100, b=120)\n",
    ")\n",
    "\n",
    "fig_fault.update_yaxes(title_text='<b>Execution Time (seconds)</b>', row=1, col=1,\n",
    "                       showgrid=True, gridcolor='rgba(200,200,200,0.3)')\n",
    "fig_fault.update_yaxes(title_text='<b>Total Tokens</b>', row=1, col=2,\n",
    "                       showgrid=True, gridcolor='rgba(200,200,200,0.3)')\n",
    "\n",
    "fig_fault.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Box Plot by Agent Configuration\n",
    "agent_types = sorted(experiments_df['agent_id'].unique())\n",
    "colors_array_agents = cmap(np.linspace(0, 1, len(agent_types)))\n",
    "agent_color_map = {agent: f'rgba({int(c[0]*255)},{int(c[1]*255)},{int(c[2]*255)},0.8)' \n",
    "                   for agent, c in zip(agent_types, colors_array_agents)}\n",
    "\n",
    "fig_agent = make_subplots(rows=1, cols=2, \n",
    "                          subplot_titles=['<b>Execution Time by Agent Configuration</b>', \n",
    "                                         '<b>Token Usage by Agent Configuration</b>'],\n",
    "                          horizontal_spacing=0.08)\n",
    "\n",
    "for i, agent in enumerate(agent_types):\n",
    "    data = experiments_df[experiments_df['agent_id'] == agent]\n",
    "    color = agent_color_map[agent]\n",
    "    \n",
    "    # Hover text for individual points\n",
    "    hover_texts = data.apply(lambda row: \n",
    "        f\"<b>Agent:</b> {row['agent_id']}<br>\"\n",
    "        f\"<b>Fault:</b> {row['fault_name']}<br>\"\n",
    "        f\"<b>Time:</b> {row['execution_time_seconds']:.1f}s<br>\"\n",
    "        f\"<b>Tokens:</b> {int(row['total_tokens']):,}<br>\"\n",
    "        f\"<b>Detection:</b> {'✓' if row['eval_detection'] else '✗'}<br>\"\n",
    "        f\"<b>Localization:</b> {'✓' if row['eval_localization'] else '✗'}<br>\"\n",
    "        f\"<b>RCA Score:</b> {row['eval_rca_score']}/5\",\n",
    "        axis=1\n",
    "    ).tolist()\n",
    "    \n",
    "    # Execution time box plot\n",
    "    fig_agent.add_trace(go.Box(\n",
    "        y=data['execution_time_seconds'],\n",
    "        name=f'Agent {agent}',\n",
    "        marker_color=color,\n",
    "        boxpoints='all',\n",
    "        jitter=0.4,\n",
    "        pointpos=0,\n",
    "        marker=dict(size=8, opacity=0.7, line=dict(width=1, color='white')),\n",
    "        hovertext=hover_texts,\n",
    "        hoverinfo='text',\n",
    "        legendgroup=str(agent),\n",
    "        showlegend=True\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    # Token usage box plot\n",
    "    fig_agent.add_trace(go.Box(\n",
    "        y=data['total_tokens'],\n",
    "        name=f'Agent {agent}',\n",
    "        marker_color=color,\n",
    "        boxpoints='all',\n",
    "        jitter=0.4,\n",
    "        pointpos=0,\n",
    "        marker=dict(size=8, opacity=0.7, line=dict(width=1, color='white')),\n",
    "        hovertext=hover_texts,\n",
    "        hoverinfo='text',\n",
    "        legendgroup=str(agent),\n",
    "        showlegend=False\n",
    "    ), row=1, col=2)\n",
    "\n",
    "fig_agent.update_layout(\n",
    "    title=dict(\n",
    "        text='<b>Performance Distribution by Agent Configuration</b>',\n",
    "        x=0.5,\n",
    "        xanchor='center',\n",
    "        font=dict(size=20, color='#2c3e50')\n",
    "    ),\n",
    "    plot_bgcolor='#f8f9fa',\n",
    "    paper_bgcolor='white',\n",
    "    height=600,\n",
    "    font=dict(family='Arial, sans-serif', size=12),\n",
    "    legend=dict(\n",
    "        title=dict(text='<b>Agent Configurations</b>', font=dict(size=13)),\n",
    "        orientation='h',\n",
    "        yanchor='bottom',\n",
    "        y=-0.25,\n",
    "        xanchor='center',\n",
    "        x=0.5,\n",
    "        bgcolor='rgba(255,255,255,0.9)',\n",
    "        bordercolor='#bdc3c7',\n",
    "        borderwidth=1\n",
    "    ),\n",
    "    margin=dict(l=80, r=30, t=100, b=120)\n",
    ")\n",
    "\n",
    "fig_agent.update_yaxes(title_text='<b>Execution Time (seconds)</b>', row=1, col=1,\n",
    "                       showgrid=True, gridcolor='rgba(200,200,200,0.3)')\n",
    "fig_agent.update_yaxes(title_text='<b>Total Tokens</b>', row=1, col=2,\n",
    "                       showgrid=True, gridcolor='rgba(200,200,200,0.3)')\n",
    "\n",
    "fig_agent.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sre-agent-35UqMg2y-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
