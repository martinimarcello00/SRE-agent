{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Multi-Experiment Hypothesis Test Analysis\n",
    "\n",
    "Statistical analysis comparing Agent A vs Agent F across multiple fault scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, mannwhitneyu\n",
    "import matplotlib.pyplot as plt\n",
    "from pypalettes import load_cmap\n",
    "import warnings\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get the path to the root directory of the repository\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(os.path.join(root_dir, '.env'))\n",
    "\n",
    "print(f\"Root directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "HYPOTHESIS_TEST_DIR = os.path.join(root_dir, 'Results', 'hypothesis-test')\n",
    "SAVE_IMAGES = True\n",
    "ALPHA = 0.05  # Significance level\n",
    "CMAP_NAME = \"Color_Blind\"\n",
    "\n",
    "print(f\"Hypothesis test directory: {HYPOTHESIS_TEST_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_data(agent_path):\n",
    "    \"\"\"Loads all JSON experiment files from a specific agent's folder.\"\"\"\n",
    "    json_files = [f for f in os.listdir(agent_path) if f.endswith('.json')]\n",
    "    agent_records = []\n",
    "    \n",
    "    for experiment_file in json_files:\n",
    "        try:\n",
    "            file_path = os.path.join(agent_path, experiment_file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract localization as string\n",
    "            localization = data.get(\"final_report\", {}).get(\"localization\", [])\n",
    "            localization_str = \", \".join(localization) if isinstance(localization, list) else None\n",
    "            \n",
    "            record = {\n",
    "                \"experiment_file\": experiment_file,\n",
    "                \"agent_id\": data.get(\"agent_id\", None),\n",
    "                \"agent_conf_name\": data.get(\"agent_configuration_name\", None),\n",
    "                \"scenario\": data.get(\"app_name\", None),\n",
    "                \"fault_name\": data.get(\"testbed\", {}).get(\"fault_name\", None),\n",
    "                \"execution_time_seconds\": data.get(\"stats\", {}).get(\"execution_time_seconds\", 0),\n",
    "                \"total_tokens\": data.get(\"stats\", {}).get(\"total_tokens\", 0),\n",
    "                \"eval_detection\": data.get(\"evaluation\", {}).get(\"detection\", None),\n",
    "                \"eval_localization\": data.get(\"evaluation\", {}).get(\"localization\", None),\n",
    "                \"eval_rca_score\": data.get(\"evaluation\", {}).get(\"rca_score\", None),\n",
    "            }\n",
    "            agent_records.append(record)\n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Warning: Error processing {experiment_file}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return pd.DataFrame(agent_records)\n",
    "\n",
    "def perform_hypothesis_test(data_a, data_b, metric_name, alpha=0.05, direction='less'):\n",
    "    \"\"\"\n",
    "    Perform one-sided hypothesis test (Agent A < Agent B).\n",
    "    Returns a dictionary of test results.\n",
    "    \"\"\"\n",
    "    a_clean = data_a.dropna()\n",
    "    b_clean = data_b.dropna()\n",
    "    \n",
    "    if len(a_clean) == 0 or len(b_clean) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Normality test\n",
    "    _, p_a_normal = stats.shapiro(a_clean)\n",
    "    _, p_b_normal = stats.shapiro(b_clean)\n",
    "    is_normal = (p_a_normal > alpha) and (p_b_normal > alpha)\n",
    "    \n",
    "    # Variance test\n",
    "    _, p_levene = stats.levene(a_clean, b_clean)\n",
    "    equal_var = p_levene > alpha\n",
    "    \n",
    "    # Hypothesis test\n",
    "    if is_normal:\n",
    "        t_stat, p_value = ttest_ind(a_clean, b_clean, equal_var=equal_var, alternative=direction)\n",
    "        test_type = \"T-Test\" if equal_var else \"Welch's T-Test\"\n",
    "    else:\n",
    "        t_stat, p_value = mannwhitneyu(a_clean, b_clean, alternative=direction)\n",
    "        test_type = \"Mann-Whitney U\"\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    mean_a, mean_b = a_clean.mean(), b_clean.mean()\n",
    "    std_a, std_b = a_clean.std(), b_clean.std()\n",
    "    pooled_std = np.sqrt(((len(a_clean)-1)*std_a**2 + (len(b_clean)-1)*std_b**2) / (len(a_clean) + len(b_clean) - 2))\n",
    "    cohens_d = (mean_a - mean_b) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'metric': metric_name,\n",
    "        'mean_a': mean_a,\n",
    "        'mean_b': mean_b,\n",
    "        'imp_pct': (mean_b - mean_a) / mean_b * 100 if mean_b != 0 else 0,\n",
    "        'p_value': p_value,\n",
    "        'significant': p_value < alpha,\n",
    "        'cohens_d': cohens_d,\n",
    "        'test_type': test_type\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Discovery and Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan directory identifying pairs\n",
    "all_dirs = [d for d in os.listdir(HYPOTHESIS_TEST_DIR) if os.path.isdir(os.path.join(HYPOTHESIS_TEST_DIR, d))]\n",
    "experiments = {}\n",
    "\n",
    "for d in all_dirs:\n",
    "    if d == 'plots': continue\n",
    "    \n",
    "    # Expecting format \"A - Experiment Name\" or \"F - Experiment Name\"\n",
    "    match = re.match(r\"^([AF]) - (.+)$\", d)\n",
    "    if match:\n",
    "        agent = match.group(1)\n",
    "        exp_name = match.group(2)\n",
    "        \n",
    "        if exp_name not in experiments:\n",
    "            experiments[exp_name] = {}\n",
    "        experiments[exp_name][agent] = os.path.join(HYPOTHESIS_TEST_DIR, d)\n",
    "\n",
    "print(f\"Found {len(experiments)} experiment pairs:\")\n",
    "for exp, agents in experiments.items():\n",
    "    print(f\"  - {exp} (Agents: {list(agents.keys())})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Execution Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = load_cmap(CMAP_NAME)\n",
    "colors = [cmap(0.2), cmap(0.8)]\n",
    "summary_results = []\n",
    "\n",
    "for exp_name, agent_paths in experiments.items():\n",
    "    if 'A' not in agent_paths or 'F' not in agent_paths:\n",
    "        print(f\"Skipping {exp_name}: Incomplete pair\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nProcessing: {exp_name}...\")\n",
    "    \n",
    "    # Load Data\n",
    "    df_a = load_experiment_data(agent_paths['A'])\n",
    "    df_f = load_experiment_data(agent_paths['F'])\n",
    "    \n",
    "    # Directory for plots\n",
    "    safe_name = re.sub(r'[^a-zA-Z0-9]', '_', exp_name)\n",
    "    plot_dir = os.path.join(HYPOTHESIS_TEST_DIR, 'plots', safe_name)\n",
    "    if SAVE_IMAGES:\n",
    "        if os.path.exists(plot_dir):\n",
    "             shutil.rmtree(plot_dir)\n",
    "        os.makedirs(plot_dir)\n",
    "\n",
    "    # Metrics to test\n",
    "    metrics = [\n",
    "        ('execution_time_seconds', 'Execution Time', 'less'),\n",
    "        ('total_tokens', 'Total Tokens', 'less')\n",
    "    ]\n",
    "    \n",
    "    # Run Tests\n",
    "    for metric, label, direction in metrics:\n",
    "        res = perform_hypothesis_test(df_a[metric], df_f[metric], label, ALPHA, direction)\n",
    "        if res:\n",
    "            res['experiment'] = exp_name\n",
    "            summary_results.append(res)\n",
    "            \n",
    "            # Plot Boxplot\n",
    "            if SAVE_IMAGES:\n",
    "                plt.figure(figsize=(6, 5))\n",
    "                box_data = [df_a[metric].dropna(), df_f[metric].dropna()]\n",
    "                plt.boxplot(box_data, labels=['Agent A', 'Agent F'], patch_artist=True,\n",
    "                            boxprops=dict(facecolor=colors[0], alpha=0.5),\n",
    "                            medianprops=dict(color='black'))\n",
    "                plt.title(f\"{label}\\n{exp_name}\")\n",
    "                plt.ylabel(label)\n",
    "                plt.tight_layout()\n",
    "                safe_metric = label.lower().replace(\" \", \"_\")\n",
    "                plt.savefig(os.path.join(plot_dir, f\"{safe_metric}_boxplot.pdf\"))\n",
    "                plt.close()\n",
    "    \n",
    "    print(f\"  -> Done. Results saved to {plot_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Statistical Metrics Explanation\n",
    "\n",
    "- **P-value**: The probability that the observed difference regarding the null hypothesis (Agent A >= Agent F) occurred by chance. A p-value < 0.05 indicates statistical significance (Agent A is significantly better).\n",
    "- **Cohen's d**: A measure of effect size (standardized difference between means).\n",
    "  - 0.2: Small effect\n",
    "  - 0.5: Medium effect\n",
    "  - 0.8: Large effect\n",
    "- **Imp % (Improvement Percentage)**: Relative improvement of Agent A compared to Agent F. Positive values indicate Agent A performed better (lower time/tokens)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame(summary_results)\n",
    "\n",
    "if not summary_df.empty:\n",
    "    # Prioritize columns\n",
    "    cols = ['experiment', 'metric', 'mean_a', 'mean_b', 'imp_pct', 'p_value', 'significant', 'test_type']\n",
    "    summary_df = summary_df[cols]\n",
    "    \n",
    "    print(\"\\nOverall Hypothesis Test Results (Agent A vs Agent F):\")\n",
    "    # Display formatted dataframe\n",
    "    display(summary_df.style.format({\n",
    "        'mean_a': '{:.2f}',\n",
    "        'mean_b': '{:.2f}',\n",
    "        'imp_pct': '{:.2f}%',\n",
    "        'p_value': '{:.5f}',\n",
    "        'cohens_d': '{:.3f}'\n",
    "    }).background_gradient(subset=['imp_pct'], cmap='Greens'))\n",
    "    \n",
    "    # Export to CSV\n",
    "    summary_path = os.path.join(HYPOTHESIS_TEST_DIR, 'hypothesis_results_summary.csv')\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"\\nSummary saved to: {summary_path}\")\n",
    "else:\n",
    "    print(\"No results generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary_df.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sre-agent-35UqMg2y-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
