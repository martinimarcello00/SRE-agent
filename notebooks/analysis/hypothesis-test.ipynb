{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Hypothesis Test Analysis\n",
    "\n",
    "Statistical analysis comparing agent configurations on Hotel Reservation fault scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get the path to the root directory of the repository\n",
    "root_dir = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "\n",
    "# Load environment variables from .env file in the root directory\n",
    "load_dotenv(os.path.join(root_dir, '.env'))\n",
    "\n",
    "print(f\"Root directory: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "HYPOTHESIS_TEST_DIR = os.path.join(root_dir, 'Results', 'hypothesis-test-hotel-res')\n",
    "SAVE_IMAGES = True\n",
    "ALPHA = 0.05  # Significance level for hypothesis tests\n",
    "\n",
    "print(f\"Hypothesis test directory: {HYPOTHESIS_TEST_DIR}\")\n",
    "print(f\"Alpha level: {ALPHA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Load Experiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get agent folders\n",
    "agent_folders = [d for d in os.listdir(HYPOTHESIS_TEST_DIR) \n",
    "                  if os.path.isdir(os.path.join(HYPOTHESIS_TEST_DIR, d))]\n",
    "agent_folders = sorted(agent_folders)\n",
    "\n",
    "print(f\"Found {len(agent_folders)} agent configurations:\")\n",
    "for i, folder in enumerate(agent_folders, 1):\n",
    "    print(f\"{i}) {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiments from all agent folders\n",
    "experiments_df = pd.DataFrame()\n",
    "agent_data = {}  # Track data by agent for later comparisons\n",
    "\n",
    "for agent_folder in agent_folders:\n",
    "    agent_path = os.path.join(HYPOTHESIS_TEST_DIR, agent_folder)\n",
    "    json_files = [f for f in os.listdir(agent_path) if f.endswith('.json')]\n",
    "    \n",
    "    print(f\"\\nLoading experiments from: {agent_folder}\")\n",
    "    print(f\"Found {len(json_files)} experiment files\")\n",
    "    \n",
    "    agent_records = []\n",
    "    \n",
    "    for experiment_file in json_files:\n",
    "        try:\n",
    "            file_path = os.path.join(agent_path, experiment_file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract localization as string\n",
    "            localization = data.get(\"final_report\", {}).get(\"localization\", [])\n",
    "            if isinstance(localization, list):\n",
    "                localization_str = \", \".join(localization)\n",
    "            else:\n",
    "                localization_str = None\n",
    "            \n",
    "            record = {\n",
    "                \"experiment_file\": experiment_file,\n",
    "                \"agent_id\": data.get(\"agent_id\", None),\n",
    "                \"agent_conf_name\": data.get(\"agent_configuration_name\", None),\n",
    "                \"scenario\": data.get(\"app_name\", None),\n",
    "                \"fault_name\": data.get(\"testbed\", {}).get(\"fault_name\", None),\n",
    "                \"target_namespace\": data.get(\"target_namespace\", None),\n",
    "                \"trace_service_starting_point\": data.get(\"trace_service_starting_point\", None),\n",
    "                \"rca_tasks_per_iteration\": data.get(\"testbed\", {}).get(\"rca_tasks_per_iteration\", 0),\n",
    "                \"max_tool_calls\": data.get(\"testbed\", {}).get(\"max_tool_calls\", 0),\n",
    "                \"execution_time_seconds\": data.get(\"stats\", {}).get(\"execution_time_seconds\", 0),\n",
    "                \"total_tokens\": data.get(\"stats\", {}).get(\"total_tokens\", 0),\n",
    "                \"tokens_triage\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"triage_agent\", {}).get(\"total_tokens\", 0),\n",
    "                \"tokens_planner\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"planner_agent\", {}).get(\"total_tokens\", 0),\n",
    "                \"tokens_rca_worker\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"rca_agent\", {}).get(\"total_tokens\", 0),\n",
    "                \"runs_count_rca\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"rca_agent\", {}).get(\"runs_count\", 0),\n",
    "                \"tokens_supervisor\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"supervisor_agent\", {}).get(\"total_tokens\", 0),\n",
    "                \"runs_count_supervisor\": data.get(\"stats\", {}).get(\"agent_stats\", {}).get(\"supervisor_agent\", {}).get(\"runs_count\", 0),\n",
    "                \"detection\": data.get(\"final_report\", {}).get(\"detection\", None),\n",
    "                \"localization\": localization_str,\n",
    "                \"root_cause\": data.get(\"final_report\", {}).get(\"root_cause\", None),\n",
    "                \"eval_detection\": data.get(\"evaluation\", {}).get(\"detection\", None),\n",
    "                \"eval_localization\": data.get(\"evaluation\", {}).get(\"localization\", None),\n",
    "                \"eval_rca_score\": data.get(\"evaluation\", {}).get(\"rca_score\", None),\n",
    "                \"eval_rca_motivation\": data.get(\"evaluation\", {}).get(\"rca_motivation\", None),\n",
    "            }\n",
    "            \n",
    "            agent_records.append(record)\n",
    "            experiments_df = pd.concat([experiments_df, pd.DataFrame([record])], ignore_index=True)\n",
    "        \n",
    "        except (json.JSONDecodeError, KeyError) as e:\n",
    "            print(f\"Warning: Error processing {experiment_file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    agent_data[agent_folder] = pd.DataFrame(agent_records)\n",
    "    print(f\"Loaded {len(agent_records)} experiments\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total experiments loaded: {len(experiments_df)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display loaded data\n",
    "experiments_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by agent\n",
    "summary_stats = experiments_df.groupby('agent_id').agg({\n",
    "    'experiment_file': 'count',\n",
    "    'execution_time_seconds': ['mean', 'std', 'min', 'max'],\n",
    "    'total_tokens': ['mean', 'std', 'min', 'max'],\n",
    "    'eval_detection': 'mean',\n",
    "    'eval_localization': 'mean',\n",
    "    'eval_rca_score': ['mean', 'std'],\n",
    "})\n",
    "\n",
    "summary_stats.columns = ['_'.join(col).strip() for col in summary_stats.columns.values]\n",
    "summary_stats.rename(columns={'experiment_file_count': 'num_runs'}, inplace=True)\n",
    "summary_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview by scenario and fault\n",
    "print(\"\\nScenario and Fault Information:\")\n",
    "print(f\"Scenario: {experiments_df['scenario'].unique()}\")\n",
    "print(f\"Fault Type: {experiments_df['fault_name'].unique()}\")\n",
    "print(f\"\\nAgent IDs: {experiments_df['agent_id'].unique()}\")\n",
    "print(f\"\\nAgent Configuration Names: {experiments_df['agent_conf_name'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "Compare performance metrics between the two agent configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, mannwhitneyu, normaltest\n",
    "\n",
    "# Get the two agent configurations\n",
    "agents = sorted(experiments_df['agent_id'].unique())\n",
    "if len(agents) != 2:\n",
    "    print(f\"Warning: Expected 2 agents, found {len(agents)}\")\n",
    "\n",
    "agent_a = agents[0]\n",
    "agent_b = agents[1]\n",
    "\n",
    "print(f\"Comparing Agent {agent_a} vs Agent {agent_b}\")\n",
    "print(f\"Alpha significance level: {ALPHA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for each agent\n",
    "agent_a_data = experiments_df[experiments_df['agent_id'] == agent_a]\n",
    "agent_b_data = experiments_df[experiments_df['agent_id'] == agent_b]\n",
    "\n",
    "print(f\"Agent {agent_a}: {len(agent_a_data)} runs\")\n",
    "print(f\"Agent {agent_b}: {len(agent_b_data)} runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform one-sided hypothesis test (Agent A better = lower values)\n",
    "def perform_hypothesis_test_onesided(data_a, data_b, metric_name, alpha=0.05, direction='less'):\n",
    "    \"\"\"\n",
    "    Perform one-sided hypothesis test.\n",
    "    direction='less': Tests if Agent A < Agent B (for metrics where lower is better)\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    a_clean = data_a.dropna()\n",
    "    b_clean = data_b.dropna()\n",
    "    \n",
    "    if len(a_clean) == 0 or len(b_clean) == 0:\n",
    "        print(f\"Skipping {metric_name}: Insufficient data\")\n",
    "        return None\n",
    "    \n",
    "    # Normality test (Shapiro-Wilk)\n",
    "    _, p_a_normal = stats.shapiro(a_clean)\n",
    "    _, p_b_normal = stats.shapiro(b_clean)\n",
    "    \n",
    "    is_normal_a = p_a_normal > alpha\n",
    "    is_normal_b = p_b_normal > alpha\n",
    "    \n",
    "    # Levene's test for equal variances\n",
    "    _, p_levene = stats.levene(a_clean, b_clean)\n",
    "    equal_var = p_levene > alpha\n",
    "    \n",
    "    # Choose appropriate test (one-sided)\n",
    "    if is_normal_a and is_normal_b:\n",
    "        # Parametric: Independent t-test (one-sided)\n",
    "        t_stat, p_value = ttest_ind(a_clean, b_clean, equal_var=equal_var, alternative=direction)\n",
    "        test_used = \"Welch's t-test (one-sided)\" if not equal_var else \"Student's t-test (one-sided)\"\n",
    "    else:\n",
    "        # Non-parametric: Mann-Whitney U test (one-sided)\n",
    "        u_stat, p_value = mannwhitneyu(a_clean, b_clean, alternative=direction)\n",
    "        t_stat = u_stat\n",
    "        test_used = \"Mann-Whitney U test (one-sided)\"\n",
    "    \n",
    "    mean_a = a_clean.mean()\n",
    "    mean_b = b_clean.mean()\n",
    "    std_a = a_clean.std()\n",
    "    std_b = b_clean.std()\n",
    "    \n",
    "    # Calculate effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt(((len(a_clean)-1)*std_a**2 + (len(b_clean)-1)*std_b**2) / (len(a_clean) + len(b_clean) - 2))\n",
    "    cohens_d = (mean_a - mean_b) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    # For \"better\", we need Agent A mean to be lower for these metrics\n",
    "    significant = \"YES\" if p_value < alpha else \"NO\"\n",
    "    \n",
    "    return {\n",
    "        'metric': metric_name,\n",
    "        'test_used': test_used,\n",
    "        'agent_a_mean': mean_a,\n",
    "        'agent_b_mean': mean_b,\n",
    "        'agent_a_std': std_a,\n",
    "        'agent_b_std': std_b,\n",
    "        'test_statistic': t_stat,\n",
    "        'p_value': p_value,\n",
    "        'significant': significant,\n",
    "        'cohens_d': cohens_d,\n",
    "        'agent_a_better': mean_a < mean_b,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ONE-SIDED hypothesis tests: Agent A better than Agent B (lower is better)\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ONE-SIDED HYPOTHESIS TEST: Agent A performs BETTER than Agent B\")\n",
    "print(\"(Lower execution time and tokens = better performance)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "metrics_to_test = [\n",
    "    ('execution_time_seconds', 'Execution Time (seconds)', 'less'),  # less = Agent A < Agent B is better\n",
    "    ('total_tokens', 'Total Tokens', 'less'),\n",
    "]\n",
    "\n",
    "hypothesis_results = []\n",
    "\n",
    "for metric_col, metric_name, direction in metrics_to_test:\n",
    "    result = perform_hypothesis_test_onesided(\n",
    "        agent_a_data[metric_col],\n",
    "        agent_b_data[metric_col],\n",
    "        metric_name,\n",
    "        alpha=ALPHA,\n",
    "        direction=direction\n",
    "    )\n",
    "    if result:\n",
    "        hypothesis_results.append(result)\n",
    "\n",
    "# Create results dataframe\n",
    "hypothesis_df = pd.DataFrame(hypothesis_results)\n",
    "print(\"\\nHypothesis Test Results (One-Sided: Agent A < Agent B):\")\n",
    "print(\"=\"*100)\n",
    "print(hypothesis_df[['metric', 'agent_a_mean', 'agent_b_mean', 'p_value', 'significant', 'cohens_d']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed results display\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"DETAILED ONE-SIDED HYPOTHESIS TEST RESULTS\")\n",
    "print(\"H0: Agent A >= Agent B (no improvement)\")\n",
    "print(\"H1: Agent A < Agent B (Agent A is better)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for idx, row in hypothesis_df.iterrows():\n",
    "    print(f\"\\n{row['metric'].upper()}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Agent {agent_a}: Mean = {row['agent_a_mean']:.4f}, Std = {row['agent_a_std']:.4f}\")\n",
    "    print(f\"  Agent {agent_b}: Mean = {row['agent_b_mean']:.4f}, Std = {row['agent_b_std']:.4f}\")\n",
    "    print(f\"  Difference: Agent A is {row['agent_a_mean'] - row['agent_b_mean']:.4f} {'LOWER' if row['agent_a_mean'] < row['agent_b_mean'] else 'HIGHER'}\")\n",
    "    print(f\"  Percentage difference: {(row['agent_a_mean'] - row['agent_b_mean']) / row['agent_b_mean'] * 100:.2f}%\")\n",
    "    print(f\"  Test Used: {row['test_used']}\")\n",
    "    print(f\"  Test Statistic: {row['test_statistic']:.4f}\")\n",
    "    print(f\"  P-value (one-sided): {row['p_value']:.6f}\")\n",
    "    print(f\"  Significant (α={ALPHA}): {row['significant']}\")\n",
    "    \n",
    "    if row['significant'] == 'YES' and row['agent_a_better']:\n",
    "        print(f\"  ✓ RESULT: Agent A performs SIGNIFICANTLY BETTER than Agent B\")\n",
    "    elif row['significant'] == 'YES' and not row['agent_a_better']:\n",
    "        print(f\"  ✗ RESULT: Agent A performs SIGNIFICANTLY WORSE than Agent B\")\n",
    "    else:\n",
    "        print(f\"  ○ RESULT: No significant difference detected\")\n",
    "    \n",
    "    print(f\"  Cohen's d (effect size): {row['cohens_d']:.4f}\")\n",
    "    \n",
    "    # Interpret effect size\n",
    "    d = abs(row['cohens_d'])\n",
    "    if d < 0.2:\n",
    "        effect = \"negligible\"\n",
    "    elif d < 0.5:\n",
    "        effect = \"small\"\n",
    "    elif d < 0.8:\n",
    "        effect = \"medium\"\n",
    "    else:\n",
    "        effect = \"large\"\n",
    "    print(f\"  Effect Size Interpretation: {effect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pypalettes import load_cmap\n",
    "\n",
    "# Load color palette\n",
    "cmap = load_cmap(\"Color_Blind\")\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "plots_dir = os.path.join(HYPOTHESIS_TEST_DIR, 'plots')\n",
    "if SAVE_IMAGES and not os.path.exists(plots_dir):\n",
    "    os.makedirs(plots_dir)\n",
    "\n",
    "plot_counter = 0\n",
    "print(f\"Plots will be saved to: {plots_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots comparing agents on key metrics (only numerical metrics)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(f'Resource Usage Comparison: Agent {agent_a} vs Agent {agent_b}', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = [cmap(0.2), cmap(0.8)]\n",
    "agent_labels = [f'Agent {agent_a}', f'Agent {agent_b}']\n",
    "agents_list = [agent_a, agent_b]\n",
    "\n",
    "metrics_to_plot = [\n",
    "    ('execution_time_seconds', 'Execution Time (seconds)'),\n",
    "    ('total_tokens', 'Total Tokens'),\n",
    "]\n",
    "\n",
    "for idx, (metric_col, metric_label) in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Prepare data\n",
    "    data_to_plot = [\n",
    "        agent_a_data[metric_col].dropna().values,\n",
    "        agent_b_data[metric_col].dropna().values\n",
    "    ]\n",
    "    \n",
    "    # Create box plot with individual points\n",
    "    bp = ax.boxplot(data_to_plot, tick_labels=agent_labels, patch_artist=True, widths=0.6,\n",
    "                    showmeans=True,\n",
    "                    boxprops=dict(linewidth=1.5),\n",
    "                    medianprops=dict(linewidth=2, color='darkred'),\n",
    "                    meanprops=dict(marker='D', markerfacecolor='white', \n",
    "                                  markeredgecolor='black', markersize=7))\n",
    "    \n",
    "    # Color boxes\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.6)\n",
    "    \n",
    "    # Overlay individual points\n",
    "    for i, agent in enumerate(agents_list, 1):\n",
    "        data = experiments_df[experiments_df['agent_id'] == agent][metric_col].dropna().values\n",
    "        x_pos = i + np.random.uniform(-0.1, 0.1, size=len(data))\n",
    "        ax.scatter(x_pos, data, s=50, color=colors[i-1], alpha=0.6, \n",
    "                  edgecolors='black', linewidth=0.5, zorder=3)\n",
    "    \n",
    "    ax.set_ylabel(metric_label, fontweight='bold', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "    ax.set_facecolor('#f8f9fa')\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVE_IMAGES:\n",
    "    plot_counter += 1\n",
    "    filename = os.path.join(plots_dir, f'{plot_counter:03d}_boxplot_comparison.pdf')\n",
    "    fig.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "    print(f\"Saved plot to {filename}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violin plots for better distribution visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle(f'Distribution Comparison: Agent {agent_a} vs Agent {agent_b}', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, (metric_col, metric_label) in enumerate(metrics_to_plot):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Prepare data for violin plot\n",
    "    plot_data = []\n",
    "    labels = []\n",
    "    for agent in agents_list:\n",
    "        data = experiments_df[experiments_df['agent_id'] == agent][metric_col].dropna().values\n",
    "        plot_data.append(data)\n",
    "        labels.append(f'Agent {agent}')\n",
    "    \n",
    "    parts = ax.violinplot(plot_data, positions=[1, 2], widths=0.7, \n",
    "                          showmeans=True, showmedians=True)\n",
    "    \n",
    "    # Color the violin plots\n",
    "    for i, pc in enumerate(parts['bodies']):\n",
    "        pc.set_facecolor(colors[i])\n",
    "        pc.set_alpha(0.6)\n",
    "    \n",
    "    ax.set_xticks([1, 2])\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylabel(metric_label, fontweight='bold', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "    ax.set_facecolor('#f8f9fa')\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVE_IMAGES:\n",
    "    plot_counter += 1\n",
    "    filename = os.path.join(plots_dir, f'{plot_counter:03d}_violin_comparison.pdf')\n",
    "    fig.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "    print(f\"Saved plot to {filename}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Execution Time vs Total Tokens\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "for agent, agent_subset, color in [(agent_a, agent_a_data, colors[0]), (agent_b, agent_b_data, colors[1])]:\n",
    "    ax.scatter(\n",
    "        agent_subset['execution_time_seconds'],\n",
    "        agent_subset['total_tokens'],\n",
    "        s=120,\n",
    "        alpha=0.6,\n",
    "        edgecolors='black',\n",
    "        linewidth=1,\n",
    "        label=f'Agent {agent}',\n",
    "        color=color\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Execution Time (seconds)', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Total Tokens', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Execution Time vs Token Usage by Agent', fontweight='bold', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, linestyle='--')\n",
    "ax.set_facecolor('#f8f9fa')\n",
    "ax.legend(fontsize=11, edgecolor='black', loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVE_IMAGES:\n",
    "    plot_counter += 1\n",
    "    filename = os.path.join(plots_dir, f'{plot_counter:03d}_scatter_performance.pdf')\n",
    "    fig.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "    print(f\"Saved plot to {filename}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart: Resource usage comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "metrics = ['Execution Time (s)', 'Total Tokens']\n",
    "agent_a_vals = [\n",
    "    agent_a_data['execution_time_seconds'].mean(),\n",
    "    agent_a_data['total_tokens'].mean() / 1000  # Scale down for visualization\n",
    "]\n",
    "agent_b_vals = [\n",
    "    agent_b_data['execution_time_seconds'].mean(),\n",
    "    agent_b_data['total_tokens'].mean() / 1000  # Scale down for visualization\n",
    "]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, agent_a_vals, width, label=f'Agent {agent_a}', \n",
    "               color=colors[0], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "bars2 = ax.bar(x + width/2, agent_b_vals, width, label=f'Agent {agent_b}', \n",
    "               color=colors[1], alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.1f}',\n",
    "               ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "ax.set_ylabel('Value', fontweight='bold', fontsize=12)\n",
    "ax.set_title(f'Resource Usage Comparison: Agent {agent_a} vs Agent {agent_b}\\n(Tokens in thousands)', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics, fontweight='bold', fontsize=11)\n",
    "ax.legend(fontsize=11, edgecolor='black')\n",
    "ax.grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "ax.set_facecolor('#f8f9fa')\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVE_IMAGES:\n",
    "    plot_counter += 1\n",
    "    filename = os.path.join(plots_dir, f'{plot_counter:03d}_bar_resource_comparison.pdf')\n",
    "    fig.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "    print(f\"Saved plot to {filename}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement summary visualization\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "improvements = [\n",
    "    abs((agent_a_data['execution_time_seconds'].mean() - agent_b_data['execution_time_seconds'].mean()) \n",
    "        / agent_b_data['execution_time_seconds'].mean() * 100),\n",
    "    abs((agent_a_data['total_tokens'].mean() - agent_b_data['total_tokens'].mean()) \n",
    "        / agent_b_data['total_tokens'].mean() * 100)\n",
    "]\n",
    "\n",
    "metrics_names = ['Execution Time', 'Total Tokens']\n",
    "\n",
    "bars = ax.barh(metrics_names, improvements, color=[cmap(0.2), cmap(0.8)], \n",
    "               edgecolor='black', linewidth=2, height=0.6, alpha=0.8)\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, val) in enumerate(zip(bars, improvements)):\n",
    "    ax.text(val + 1, i, f'{val:.1f}%', va='center', fontweight='bold', fontsize=12)\n",
    "\n",
    "ax.set_xlabel('Improvement (%)', fontweight='bold', fontsize=12)\n",
    "ax.set_title(f'Agent {agent_a} Performance Improvement vs Agent {agent_b}', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax.set_xlim(0, max(improvements) + 10)\n",
    "ax.grid(True, alpha=0.3, linestyle='--', axis='x')\n",
    "ax.set_facecolor('#f8f9fa')\n",
    "\n",
    "plt.tight_layout()\n",
    "if SAVE_IMAGES:\n",
    "    plot_counter += 1\n",
    "    filename = os.path.join(plots_dir, f'{plot_counter:03d}_improvement_summary.pdf')\n",
    "    fig.savefig(filename, format='pdf', bbox_inches='tight')\n",
    "    print(f\"Saved plot to {filename}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ONE-SIDED HYPOTHESIS TEST SUMMARY REPORT\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "print(f\"\\nTest Configuration:\")\n",
    "print(f\"  - Agent A (Candidate): {agent_a}\")\n",
    "print(f\"  - Agent B (Baseline): {agent_b}\")\n",
    "print(f\"  - Number of runs per agent: 30\")\n",
    "print(f\"  - Significance level (α): {ALPHA}\")\n",
    "print(f\"  - Test Type: ONE-SIDED (Agent A < Agent B = better)\")\n",
    "print(f\"  - Scenario: Hotel Reservation - Port Mismatch Geo\")\n",
    "print(f\"  - Metrics: Execution Time, Total Tokens\")\n",
    "\n",
    "print(f\"\\nKey Findings:\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "significant_results = hypothesis_df[hypothesis_df['significant'] == 'YES']\n",
    "if len(significant_results) > 0:\n",
    "    better_results = significant_results[significant_results['agent_a_better'] == True]\n",
    "    worse_results = significant_results[significant_results['agent_a_better'] == False]\n",
    "    \n",
    "    if len(better_results) > 0:\n",
    "        print(f\"\\n✓ Metrics where Agent A is SIGNIFICANTLY BETTER (p < {ALPHA}):\")\n",
    "        for idx, row in better_results.iterrows():\n",
    "            improvement = abs((row['agent_a_mean'] - row['agent_b_mean']) / row['agent_b_mean'] * 100)\n",
    "            print(f\"  • {row['metric']}: {improvement:.1f}% improvement (p = {row['p_value']:.6f}, d = {row['cohens_d']:.3f})\")\n",
    "    \n",
    "    if len(worse_results) > 0:\n",
    "        print(f\"\\n✗ Metrics where Agent A is SIGNIFICANTLY WORSE (p < {ALPHA}):\")\n",
    "        for idx, row in worse_results.iterrows():\n",
    "            degradation = abs((row['agent_a_mean'] - row['agent_b_mean']) / row['agent_b_mean'] * 100)\n",
    "            print(f\"  • {row['metric']}: {degradation:.1f}% degradation (p = {row['p_value']:.6f}, d = {row['cohens_d']:.3f})\")\n",
    "else:\n",
    "    print(f\"\\nNo statistically significant improvements found at α = {ALPHA}\")\n",
    "    print(f\"Agent A does not perform significantly better than Agent B on the tested metrics.\")\n",
    "\n",
    "print(f\"\\nDetailed Performance Comparison:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"\\nAgent {agent_a} (Candidate):\")\n",
    "print(f\"  - Execution Time: {agent_a_data['execution_time_seconds'].mean():.2f} ± {agent_a_data['execution_time_seconds'].std():.2f} seconds\")\n",
    "print(f\"  - Total Tokens: {agent_a_data['total_tokens'].mean():.0f} ± {agent_a_data['total_tokens'].std():.0f}\")\n",
    "print(f\"  - Detection Accuracy: {agent_a_data['eval_detection'].mean():.2%}\")\n",
    "print(f\"  - Localization Accuracy: {agent_a_data['eval_localization'].mean():.2%}\")\n",
    "print(f\"  - Avg RCA Score: {agent_a_data['eval_rca_score'].mean():.2f}/5.0\")\n",
    "\n",
    "print(f\"\\nAgent {agent_b} (Baseline):\")\n",
    "print(f\"  - Execution Time: {agent_b_data['execution_time_seconds'].mean():.2f} ± {agent_b_data['execution_time_seconds'].std():.2f} seconds\")\n",
    "print(f\"  - Total Tokens: {agent_b_data['total_tokens'].mean():.0f} ± {agent_b_data['total_tokens'].std():.0f}\")\n",
    "print(f\"  - Detection Accuracy: {agent_b_data['eval_detection'].mean():.2%}\")\n",
    "print(f\"  - Localization Accuracy: {agent_b_data['eval_localization'].mean():.2%}\")\n",
    "print(f\"  - Avg RCA Score: {agent_b_data['eval_rca_score'].mean():.2f}/5.0\")\n",
    "\n",
    "print(f\"\\nRelative Performance (Agent A vs Agent B):\")\n",
    "time_improvement = (agent_b_data['execution_time_seconds'].mean() - agent_a_data['execution_time_seconds'].mean()) / agent_b_data['execution_time_seconds'].mean() * 100\n",
    "token_improvement = (agent_b_data['total_tokens'].mean() - agent_a_data['total_tokens'].mean()) / agent_b_data['total_tokens'].mean() * 100\n",
    "print(f\"  - Execution Time: {time_improvement:+.2f}% (lower is better)\")\n",
    "print(f\"  - Total Tokens: {token_improvement:+.2f}% (lower is better)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sre-agent-35UqMg2y-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
