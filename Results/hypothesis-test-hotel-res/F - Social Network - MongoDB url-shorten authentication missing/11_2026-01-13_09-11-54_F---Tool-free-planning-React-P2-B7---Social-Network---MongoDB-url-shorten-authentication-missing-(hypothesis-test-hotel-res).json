{
  "app_name": "Social Network",
  "app_summary": "A social network with unidirectional follow relationships, implemented with loosely-coupled microservices, communicating with each other via Thrift RPCs. Supported Operations: Create text post (optional media: image, video, shortened URL, user tag), Read post, Read entire user timeline, Receive recommendations on which users to follow, Search database for user or post, Register/Login using user credentials, Follow/Unfollow user",
  "target_namespace": "test-social-network",
  "trace_service_starting_point": "nginx-thrift",
  "problematic_pods": {
    "problematic_pods": [
      {
        "pod_name": "url-shorten-mongodb-d57db69c5-v22kc",
        "namespace": "test-social-network",
        "pod_phase": "Running",
        "container_issues": [
          {
            "container_name": "url-shorten-mongodb",
            "issue_type": "Waiting",
            "reason": "CrashLoopBackOff",
            "message": "back-off 5m0s restarting failed container=url-shorten-mongodb pod=url-shorten-mongodb-d57db69c5-v22kc_test-social-network(a685ed3f-4dfc-4f8f-aaf9-5db4abf9a7fa)",
            "restart_count": 18
          }
        ]
      }
    ]
  },
  "slow_traces": {
    "service": "nginx-thrift",
    "traces": [],
    "info": "No traces found for service 'nginx-thrift' with a minimum duration of 2000ms in the last 15m."
  },
  "problematic_metrics": {
    "problematic_metrics": [],
    "info": "All monitored metrics look healthy; no anomalous values detected."
  },
  "problematic_traces": {
    "service": "nginx-thrift",
    "traces": [],
    "info": "No traces found for service 'nginx-thrift' with lookback '15m'."
  },
  "symptoms": [
    {
      "potential_symptom": "Container CrashLoopBackOff causing repeated restarts (pod unstable)",
      "resource_type": "pod",
      "affected_resource": "url-shorten-mongodb-d57db69c5-v22kc",
      "evidence": "Container url-shorten-mongodb is in Waiting state with reason 'CrashLoopBackOff'; message: 'back-off 5m0s restarting failed container=url-shorten-mongodb pod=url-shorten-mongodb-d57db69c5-v22kc_test-social-network(...)'; restart_count=18; pod_phase=Running. Monitored metrics show no anomalies and no error traces available for further detail."
    }
  ],
  "rca_tasks": [
    {
      "priority": 1,
      "status": "in_progress",
      "investigation_goal": "Epicenter (pod) \u2014 Domain: app. Testable hypothesis: the MongoDB process inside url-shorten-mongodb-d57db69c5-v22kc is crashing on startup (bad entrypoint, crash on DB open, OOM, permission denied). Actions: get the container exit code and recent container logs (current + previous), inspect pod events and container status to surface exitReason/exitCode and stack traces.",
      "target_resource": "url-shorten-mongodb-d57db69c5-v22kc",
      "resource_type": "pod",
      "suggested_tools": [
        "kubectl describe pod",
        "kubectl logs (including -p for previous)"
      ]
    },
    {
      "priority": 2,
      "status": "in_progress",
      "investigation_goal": "Connection check between epicenter and its Kubernetes Service \u2014 Domain: dependency/config. Testable hypothesis: the ClusterIP Service backing MongoDB is missing or has selector/port/name mismatch while one or more consumers reference the expected DNS name (two-sided check). Actions: (A) inspect any Deployment/StatefulSet/ConfigMap/Env that reference a mongodb host (search for 'url-shorten-mongodb' or 'mongodb' in deploy/statefulset/config); (B) inspect the Service object, its selectors, ports and the Endpoints object to confirm pods are selected and ports match consumer expectations.",
      "target_resource": "url-shorten-mongodb (Service)",
      "resource_type": "service",
      "suggested_tools": [
        "kubectl get deployment,statefulset -o yaml (search for consumer env/config referencing mongodb)",
        "kubectl get svc",
        "kubectl describe svc",
        "kubectl get endpoints"
      ]
    },
    {
      "priority": 3,
      "status": "pending",
      "investigation_goal": "Pod <-> storage check \u2014 Domain: platform. Testable hypothesis: the container fails because a required PVC is not bound, the volume mount path is missing, or permissions prevent mongodb from opening DB files. Actions: inspect the pod's volume mounts, the PVC referenced by the pod, PVC Bound state, and recent events mentioning mounts or permission errors.",
      "target_resource": "url-shorten-mongodb-d57db69c5-v22kc",
      "resource_type": "pod",
      "suggested_tools": [
        "kubectl describe pod",
        "kubectl get pvc",
        "kubectl describe pvc"
      ]
    },
    {
      "priority": 4,
      "status": "pending",
      "investigation_goal": "Image / startup configuration check \u2014 Domain: dependency/config (app image/config). Testable hypothesis: the pod's image tag or container startup args/command were changed or point to a missing binary resulting in immediate exit. Actions: inspect the owning Deployment/StatefulSet/ReplicaSet spec for the image and command, compare to previously known good image, and check imagePull events or ImagePullBackOff (if present).",
      "target_resource": "url-shorten-mongodb-d57db69c5-v22kc (owner Deployment/StatefulSet)",
      "resource_type": "pod",
      "suggested_tools": [
        "kubectl describe pod",
        "kubectl get deployment,statefulset,replicaset -o yaml"
      ]
    },
    {
      "priority": 5,
      "status": "pending",
      "investigation_goal": "Kubernetes probe check \u2014 Domain: platform (kubelet/probes). Testable hypothesis: a misconfigured liveness or readiness probe is failing immediately after start and triggering kubelet restarts (CrashLoopBackOff). Actions: inspect the pod's liveness/readiness probe configuration, probe logs/events and the timing relative to container restarts.",
      "target_resource": "url-shorten-mongodb-d57db69c5-v22kc",
      "resource_type": "pod",
      "suggested_tools": [
        "kubectl describe pod"
      ]
    }
  ],
  "rca_analyses_list": [
    {
      "diagnosis": "The MongoDB container in pod url-shorten-mongodb-d57db69c5-v22kc is failing to start because the TLS certificate mounted from the mongodb-tls secret is expired or not yet valid; MongoDB aborts on startup with an SSL certificate validation fatal assertion, causing Exit Code 14 and CrashLoopBackOff.",
      "reasoning": "kubectl describe pod shows the container in CrashLoopBackOff with Last State Terminated (Reason: Error) and Exit Code 14, and the pod mounts a tls-cert from secret 'mongodb-tls'. Recent container logs include a fatal assertion: \"The provided SSL certificate is expired or not yet valid.\" followed by an fassert abort (ssl_manager_openssl.cpp). These two unique data points directly link the startup crash (Exit Code 14) to an invalid/expired TLS certificate.",
      "task": {
        "priority": 1,
        "status": "completed",
        "investigation_goal": "Epicenter (pod) \u2014 Domain: app. Testable hypothesis: the MongoDB process inside url-shorten-mongodb-d57db69c5-v22kc is crashing on startup (bad entrypoint, crash on DB open, OOM, permission denied). Actions: get the container exit code and recent container logs (current + previous), inspect pod events and container status to surface exitReason/exitCode and stack traces.",
        "target_resource": "url-shorten-mongodb-d57db69c5-v22kc",
        "resource_type": "pod",
        "suggested_tools": [
          "kubectl describe pod",
          "kubectl logs (including -p for previous)"
        ]
      },
      "insights": [
        "Pod url-shorten-mongodb-d57db69c5-v22kc is in CrashLoopBackOff with Restart Count 18 and Last State Terminated (Exit Code 14) \u2014 the container fails immediately on startup",
        "Pod mounts a TLS secret (volume tls-cert from secret 'mongodb-tls') and a mongod.conf ConfigMap; the TLS secret is the relevant credential used by MongoDB at startup",
        "Container logs show a clear, reproducible fatal error: \"The provided SSL certificate is expired or not yet valid.\" followed by an fassert abort in ssl_manager_openssl.cpp, causing MongoDB to abort startup",
        "Root cause: an expired or otherwise invalid TLS certificate (from secret mongodb-tls) caused MongoDB to fail certificate validation and abort on startup, producing Exit Code 14 and triggering CrashLoopBackOff",
        "Confirmed hypotheses: the container was failing on startup with a non-zero exit code and the logs contained a clear SSL certificate validation failure explaining the crash",
        "Disproved other common causes: no evidence of OOMKilled, permission denied, missing data directory, or image pull errors as the primary cause \u2014 the decisive failure is SSL certificate validation"
      ],
      "steps_performed": [
        "Checked pod url-shorten-mongodb-d57db69c5-v22kc using kubectl_describe",
        "Analyzed pod description: container state, containerStatuses, Exit Code 14, Restart Count, CrashLoopBackOff events, and mounted volumes/secrets",
        "Checked pod logs for url-shorten-mongodb-d57db69c5-v22kc using get_logs (recent + previous)",
        "Analyzed container logs and identified an SSL fatal assertion: \"The provided SSL certificate is expired or not yet valid.\" and subsequent fassert abort",
        "Correlated data from kubectl_describe and get_logs to link CrashLoopBackOff/Exit Code 14 with the mounted tls-cert secret (mongodb-tls) and the SSL certificate validation failure",
        "Submitted final diagnosis using submit_final_diagnosis (expired/invalid TLS certificate caused MongoDB startup abort)"
      ],
      "tools_stats": {
        "kubectl_describe": 1,
        "get_logs": 1,
        "submit_final_diagnosis": 1
      },
      "message_history": [
        {
          "type": "AIMessage",
          "content": "Hypothesis 1: The MongoDB container in pod url-shorten-mongodb-d57db69c5-v22kc is failing on startup and the pod description will show a non-zero exitCode and event messages (e.g., CrashLoopBackOff, OOMKilled, or permission denied) that identify the exit reason. I'll describe the pod to capture containerStatuses and recent events.",
          "tool_calls": [
            {
              "name": "kubectl_describe",
              "args": {
                "resourceType": "pod",
                "name": "url-shorten-mongodb-d57db69c5-v22kc",
                "namespace": "test-social-network"
              },
              "id": "call_E83ivG9hyAh1G3LMmUJfh2VO",
              "type": "tool_call"
            }
          ]
        },
        {
          "type": "ToolMessage",
          "content": "Name:             url-shorten-mongodb-d57db69c5-v22kc\nNamespace:        test-social-network\nPriority:         0\nService Account:  default\nNode:             kind-worker/172.18.0.4\nStart Time:       Tue, 13 Jan 2026 07:59:17 +0000\nLabels:           app=url-shorten-mongodb\n                  pod-template-hash=d57db69c5\n                  service=url-shorten-mongodb\nAnnotations:      kubectl.kubernetes.io/restartedAt: 2026-01-13T07:58:50Z\nStatus:           Running\nIP:               10.244.1.40\nIPs:\n  IP:           10.244.1.40\nControlled By:  ReplicaSet/url-shorten-mongodb-d57db69c5\nContainers:\n  url-shorten-mongodb:\n    Container ID:  containerd://1ce27c7df988ee9c156caa24aa24618be5d23249b249d5b444fc1d4f2f05d107\n    Image:         docker.io/library/mongo:4.4.6\n    Image ID:      docker.io/library/mongo@sha256:6efa052039903e731e4a5550c68a13c4869ddc93742c716332883fd9c77eb79b\n    Port:          27017/TCP\n    Host Port:     0/TCP\n    Args:\n      --config\n      /social-network-microservices/config/mongod.conf\n    State:          Waiting\n      Reason:       CrashLoopBackOff\n    Last State:     Terminated\n      Reason:       Error\n      Exit Code:    14\n      Started:      Tue, 13 Jan 2026 09:06:41 +0000\n      Finished:     Tue, 13 Jan 2026 09:06:41 +0000\n    Ready:          False\n    Restart Count:  18\n    Environment:    <none>\n    Mounts:\n      /etc/tls from tls-cert (ro)\n      /social-network-microservices/config/mongod.conf from url-shorten-mongodb-config (rw,path=\"mongod.conf\")\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bp9nd (ro)\nConditions:\n  Type                        Status\n  PodReadyToStartContainers   True \n  Initialized                 True \n  Ready                       False \n  ContainersReady             False \n  PodScheduled                True \nVolumes:\n  url-shorten-mongodb-config:\n    Type:      ConfigMap (a volume populated by a ConfigMap)\n    Name:      url-shorten-mongodb\n    Optional:  false\n  tls-cert:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  mongodb-tls\n    Optional:    false\n  kube-api-access-bp9nd:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    Optional:                false\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type     Reason   Age                   From     Message\n  ----     ------   ----                  ----     -------\n  Normal   Pulling  2m52s (x19 over 70m)  kubelet  Pulling image \"docker.io/library/mongo:4.4.6\"\n  Warning  BackOff  2s (x323 over 70m)    kubelet  Back-off restarting failed container url-shorten-mongodb in pod url-shorten-mongodb-d57db69c5-v22kc_test-social-network(a685ed3f-4dfc-4f8f-aaf9-5db4abf9a7fa)\n"
        },
        {
          "type": "AIMessage",
          "content": "Hypothesis 2: The MongoDB process is crashing during startup and the pod's logs (including the most recent terminated instance) will contain a clear error message or stack trace such as \"permission denied\", \"data directory not found\", or configuration parse errors that explain Exit Code 14. I'll retrieve the pod logs (recent + previous) to confirm the exact failure message.",
          "tool_calls": [
            {
              "name": "get_logs",
              "args": {
                "resource_name": "url-shorten-mongodb-d57db69c5-v22kc",
                "resource_type": "pod",
                "tail": 500,
                "important": false
              },
              "id": "call_R1WYCT8OhpGWzm2JrYYzmAT6",
              "type": "tool_call"
            }
          ]
        },
        {
          "type": "ToolMessage",
          "content": "{\"t\":{\"$date\":\"2026-01-13T09:06:41.493+00:00\"},\"s\":\"F\",  \"c\":\"NETWORK\",  \"id\":28652,   \"ctx\":\"main\",\"msg\":\"The provided SSL certificate is expired or not yet valid.\"}\n{\"t\":{\"$date\":\"2026-01-13T09:06:41.493+00:00\"},\"s\":\"F\",  \"c\":\"-\",        \"id\":23091,   \"ctx\":\"main\",\"msg\":\"Fatal assertion\",\"attr\":{\"msgid\":28652,\"file\":\"src/mongo/util/net/ssl_manager_openssl.cpp\",\"line\":2269}}\n{\"t\":{\"$date\":\"2026-01-13T09:06:41.493+00:00\"},\"s\":\"F\",  \"c\":\"-\",        \"id\":23092,   \"ctx\":\"main\",\"msg\":\"\\n\\n***aborting after fassert() failure\\n\\n\"}\n"
        },
        {
          "type": "AIMessage",
          "content": "",
          "tool_calls": [
            {
              "name": "submit_final_diagnosis",
              "args": {
                "diagnosis": "The MongoDB container in pod url-shorten-mongodb-d57db69c5-v22kc is failing to start because the TLS certificate mounted from the mongodb-tls secret is expired or not yet valid; MongoDB aborts on startup with an SSL certificate validation fatal assertion, causing Exit Code 14 and CrashLoopBackOff.",
                "reasoning": "kubectl describe pod shows the container in CrashLoopBackOff with Last State Terminated (Reason: Error) and Exit Code 14, and the pod mounts a tls-cert from secret 'mongodb-tls'. Recent container logs include a fatal assertion: \"The provided SSL certificate is expired or not yet valid.\" followed by an fassert abort (ssl_manager_openssl.cpp). These two unique data points directly link the startup crash (Exit Code 14) to an invalid/expired TLS certificate."
              },
              "id": "call_jhL8unqW70Dw0QF2ndDJwa8v",
              "type": "tool_call"
            }
          ]
        },
        {
          "type": "ToolMessage",
          "content": "Final diagnosis submitted successfully. Investigation complete."
        }
      ]
    },
    {
      "diagnosis": "Service exists and matches consumer config, but the mongodb pod backing url-shorten-mongodb is NotReady so the Service has no Ready endpoints \u2014 consumers cannot connect.",
      "reasoning": "1) url-shorten-service ConfigMap references url-shorten-mongodb:27017 as the MongoDB host/port (service-config.json). 2) The url-shorten-mongodb Service exists with selector service=url-shorten-mongodb and port name/port targetPort 27017 \u2014 selector and port match consumer expectations. 3) The url-shorten-mongodb Endpoints show only a notReadyAddresses entry (pod url-shorten-mongodb-d57db69c5-v22kc at 10.244.1.40) and no readyAddresses; the Deployment status shows unavailableReplicas:1 and \"Deployment does not have minimum availability.\" Together these indicate the pod is failing readiness (not Ready) so the Service has no ready endpoints and cannot serve connections.",
      "task": {
        "priority": 2,
        "status": "completed",
        "investigation_goal": "Connection check between epicenter and its Kubernetes Service \u2014 Domain: dependency/config. Testable hypothesis: the ClusterIP Service backing MongoDB is missing or has selector/port/name mismatch while one or more consumers reference the expected DNS name (two-sided check). Actions: (A) inspect any Deployment/StatefulSet/ConfigMap/Env that reference a mongodb host (search for 'url-shorten-mongodb' or 'mongodb' in deploy/statefulset/config); (B) inspect the Service object, its selectors, ports and the Endpoints object to confirm pods are selected and ports match consumer expectations.",
        "target_resource": "url-shorten-mongodb (Service)",
        "resource_type": "service",
        "suggested_tools": [
          "kubectl get deployment,statefulset -o yaml (search for consumer env/config referencing mongodb)",
          "kubectl get svc",
          "kubectl describe svc",
          "kubectl get endpoints"
        ]
      },
      "insights": [
        "url-shorten-service configuration (service-config.json) points to url-shorten-mongodb:27017 as its MongoDB backend (confirmed)",
        "A ClusterIP Service url-shorten-mongodb exists with selector service=url-shorten-mongodb and port 27017; selector and ports match the consumer configuration (so Service mis-selection/port-mismatch is NOT the cause)",
        "The url-shorten-mongodb Deployment pod template mounts a TLS secret (secretName: mongodb-tls) and the Deployment status reports unavailableReplicas: 1 with condition: \"Deployment does not have minimum availability\"",
        "The Endpoints object for url-shorten-mongodb contains only notReadyAddresses (pod url-shorten-mongodb-d57db69c5-v22kc at 10.244.1.40) and no readyAddresses, indicating the pod is NotReady",
        "Correlation of ConfigMap + Service + Endpoints + Deployment status shows the root cause: the MongoDB pod is failing readiness (NotReady), so the Service has no Ready endpoints and consumers cannot connect to the database",
        "Confirmed hypotheses: Deployments/ConfigMap reference url-shorten-mongodb (confirmed). Disproved hypothesis that Service selector/port mismatch was causing the problem (Service matches Deployment)",
        "Endpoints metadata includes a last-change-trigger-time (2026-01-13T08:46:21Z) which pinpoints when the Endpoint transitioned to NotReady and is useful for timeline/debugging",
        "Impact: url-shorten-service (and any consumers) cannot reach MongoDB because the cluster Service has no ready endpoints \u2014 the immediate remediation path is to investigate why the mongodb pod is NotReady (readiness probe failures, crashloops, TLS secret/permission issues, init errors, etc.)"
      ],
      "steps_performed": [
        "Checked Deployments and StatefulSets in namespace test-social-network using functions.kubectl_get (parallel)",
        "Analyzed Deployments manifests and status for url-shorten-mongodb (noted pod template, volumes/secret mount and Deployment status fields including unavailableReplicas and \"Deployment does not have minimum availability\")",
        "Checked ConfigMap url-shorten-service in namespace test-social-network using functions.kubectl_get",
        "Analyzed ConfigMap service-config.json and confirmed url-shorten-mongodb:27017 is the MongoDB host/port used by url-shorten-service",
        "Checked Service url-shorten-mongodb in namespace test-social-network using functions.kubectl_get",
        "Checked Endpoints url-shorten-mongodb in namespace test-social-network using functions.kubectl_get",
        "Analyzed Service selector and ports and compared to Deployment pod labels/ports (selector=service:url-shorten-mongodb, port=27017)",
        "Analyzed Endpoints and Pod readiness (found only notReadyAddresses for pod url-shorten-mongodb-d57db69c5-v22kc at 10.244.1.40; no readyAddresses)",
        "Correlated data from ConfigMap, Service, Endpoints, and Deployment status to identify that the pod backing url-shorten-mongodb is NotReady, leaving the Service with no Ready endpoints and causing connection failures",
        "Submitted final diagnosis using functions.submit_final_diagnosis"
      ],
      "tools_stats": {
        "kubectl_get": 5,
        "submit_final_diagnosis": 1
      },
      "message_history": [
        {
          "type": "AIMessage",
          "content": "Hypothesis 1 (testable): One or more Deployments/StatefulSets in namespace test-social-network reference a MongoDB host named \"url-shorten-mongodb\" or contain \"mongodb\" in env/config; fetching their YAML will reveal these references (env vars, configMaps, command args).\n\nI'll retrieve all Deployments and StatefulSets in test-social-network in YAML to look for such references. Running both in parallel to save time.",
          "tool_calls": [
            {
              "name": "kubectl_get",
              "args": {
                "resourceType": "deployments",
                "namespace": "test-social-network",
                "output": "yaml"
              },
              "id": "call_yksU0K1TTM6uPLXPj1j3wIoy",
              "type": "tool_call"
            },
            {
              "name": "kubectl_get",
              "args": {
                "resourceType": "statefulsets",
                "namespace": "test-social-network",
                "output": "yaml"
              },
              "id": "call_JPHeC0wsB8qM6aI7PE1wjYum",
              "type": "tool_call"
            }
          ]
        },
        {
          "type": "ToolMessage",
          "content": "apiVersion: v1\nitems:\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: compose-post-service\n    name: compose-post-service\n    namespace: test-social-network\n    resourceVersion: \"1749\"\n    uid: ae04d2d0-bd98-4cd2-9705-cef14d18c2aa\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: compose-post-service\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: compose-post-service\n          service: compose-post-service\n      spec:\n        containers:\n        - command:\n          - ComposePostService\n          image: docker.io/deathstarbench/social-network-microservices:latest\n          imagePullPolicy: Always\n          name: compose-post-service\n          ports:\n          - containerPort: 9090\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: compose-post-service-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: compose-post-service-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: compose-post-service\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: compose-post-service\n          name: compose-post-service-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:08Z\"\n      lastUpdateTime: \"2026-01-13T07:57:08Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:42Z\"\n      lastUpdateTime: \"2026-01-13T07:57:08Z\"\n      message: ReplicaSet \"compose-post-service-7b495f794\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: home-timeline-redis\n    name: home-timeline-redis\n    namespace: test-social-network\n    resourceVersion: \"1696\"\n    uid: 864cbff5-54e6-46d3-a45c-21b1768e3a1e\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: home-timeline-redis\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: home-timeline-redis\n          service: home-timeline-redis\n      spec:\n        containers:\n        - args:\n          - /social-network-microservices/config/redis.conf\n          image: docker.io/library/redis:6.2.4\n          imagePullPolicy: Always\n          name: home-timeline-redis\n          ports:\n          - containerPort: 6379\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/redis.conf\n            name: home-timeline-redis-config\n            subPath: redis.conf\n        dnsPolicy: ClusterFirst\n        hostname: home-timeline-redis\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: home-timeline-redis\n          name: home-timeline-redis-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:05Z\"\n      lastUpdateTime: \"2026-01-13T07:57:05Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:41Z\"\n      lastUpdateTime: \"2026-01-13T07:57:05Z\"\n      message: ReplicaSet \"home-timeline-redis-7f7686d8f5\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: home-timeline-service\n    name: home-timeline-service\n    namespace: test-social-network\n    resourceVersion: \"1813\"\n    uid: 8ea591a6-33c2-489b-8264-2f768398982e\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: home-timeline-service\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: home-timeline-service\n          service: home-timeline-service\n      spec:\n        containers:\n        - command:\n          - HomeTimelineService\n          image: docker.io/deathstarbench/social-network-microservices:latest\n          imagePullPolicy: Always\n          name: home-timeline-service\n          ports:\n          - containerPort: 9090\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: home-timeline-service-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: home-timeline-service-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: home-timeline-service\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: home-timeline-service\n          name: home-timeline-service-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:15Z\"\n      lastUpdateTime: \"2026-01-13T07:57:15Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:43Z\"\n      lastUpdateTime: \"2026-01-13T07:57:15Z\"\n      message: ReplicaSet \"home-timeline-service-6bf55fbcd8\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: jaeger\n    name: jaeger\n    namespace: test-social-network\n    resourceVersion: \"1800\"\n    uid: 3540d071-cb03-437a-968b-19c458f7547c\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: jaeger\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: jaeger\n          service: jaeger\n      spec:\n        containers:\n        - env:\n          - name: COLLECTOR_ZIPKIN_HTTP_PORT\n            value: \"9411\"\n          image: docker.io/jaegertracing/all-in-one:1.57\n          imagePullPolicy: Always\n          name: jaeger\n          ports:\n          - containerPort: 5775\n            protocol: TCP\n          - containerPort: 6831\n            protocol: TCP\n          - containerPort: 6832\n            protocol: TCP\n          - containerPort: 5778\n            protocol: TCP\n          - containerPort: 16686\n            protocol: TCP\n          - containerPort: 14268\n            protocol: TCP\n          - containerPort: 9411\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: jaeger-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: jaeger-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: jaeger\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: jaeger\n          name: jaeger-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:13Z\"\n      lastUpdateTime: \"2026-01-13T07:57:13Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:43Z\"\n      lastUpdateTime: \"2026-01-13T07:57:13Z\"\n      message: ReplicaSet \"jaeger-7cb68977df\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: media-frontend\n    name: media-frontend\n    namespace: test-social-network\n    resourceVersion: \"1948\"\n    uid: d8769fdd-3ea9-4243-9565-0ede820ce349\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: media-frontend\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: media-frontend\n          service: media-frontend\n      spec:\n        containers:\n        - env:\n          - name: fqdn_suffix\n            value: .test-social-network.svc.cluster.local\n          image: docker.io/yg397/media-frontend:xenial\n          imagePullPolicy: Always\n          name: media-frontend\n          ports:\n          - containerPort: 8081\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /usr/local/openresty/nginx/conf/nginx.conf\n            name: media-frontend-config\n            subPath: nginx.conf\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: media-frontend-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: media-frontend-config\n            subPath: service-config.json\n          - mountPath: /usr/local/openresty/nginx/lua-scripts\n            name: lua-scripts\n        dnsPolicy: ClusterFirst\n        hostname: media-frontend\n        initContainers:\n        - args:\n          - -c\n          - git clone https://github.com/delimitrou/DeathStarBench.git /DeathStarBench\n            && cp -r /DeathStarBench/socialNetwork/media-frontend/lua-scripts/* /lua-scripts/\n          command:\n          - /bin/sh\n          image: docker.io/alpine/git:latest\n          imagePullPolicy: Always\n          name: alpine-container\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /lua-scripts\n            name: lua-scripts\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: media-frontend\n          name: media-frontend-config\n        - emptyDir: {}\n          name: lua-scripts\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:58:07Z\"\n      lastUpdateTime: \"2026-01-13T07:58:07Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:44Z\"\n      lastUpdateTime: \"2026-01-13T07:58:07Z\"\n      message: ReplicaSet \"media-frontend-7ccdcc5874\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: media-memcached\n    name: media-memcached\n    namespace: test-social-network\n    resourceVersion: \"1851\"\n    uid: 6b5c8669-75f3-4b4f-908f-4dd8b3249e41\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: media-memcached\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: media-memcached\n          service: media-memcached\n      spec:\n        containers:\n        - image: docker.io/library/memcached:1.6.7\n          imagePullPolicy: Always\n          name: media-memcached\n          ports:\n          - containerPort: 11211\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: media-memcached-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: media-memcached-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: media-memcached\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: media-memcached\n          name: media-memcached-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:19Z\"\n      lastUpdateTime: \"2026-01-13T07:57:19Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:44Z\"\n      lastUpdateTime: \"2026-01-13T07:57:19Z\"\n      message: ReplicaSet \"media-memcached-6f888776fd\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: media-mongodb\n    name: media-mongodb\n    namespace: test-social-network\n    resourceVersion: \"1620\"\n    uid: dc599a88-d066-4d07-8a7d-dd366aafddad\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: media-mongodb\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: media-mongodb\n          service: media-mongodb\n      spec:\n        containers:\n        - args:\n          - --config\n          - /social-network-microservices/config/mongod.conf\n          image: docker.io/library/mongo:4.4.6\n          imagePullPolicy: Always\n          name: media-mongodb\n          ports:\n          - containerPort: 27017\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/mongod.conf\n            name: media-mongodb-config\n            subPath: mongod.conf\n        dnsPolicy: ClusterFirst\n        hostname: media-mongodb\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: media-mongodb\n          name: media-mongodb-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:56:55Z\"\n      lastUpdateTime: \"2026-01-13T07:56:55Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:41Z\"\n      lastUpdateTime: \"2026-01-13T07:56:55Z\"\n      message: ReplicaSet \"media-mongodb-66c679cd9\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: media-service\n    name: media-service\n    namespace: test-social-network\n    resourceVersion: \"1636\"\n    uid: db9437c0-33f9-4c00-b395-0f34498e3b3c\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: media-service\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: media-service\n          service: media-service\n      spec:\n        containers:\n        - command:\n          - MediaService\n          image: docker.io/deathstarbench/social-network-microservices:latest\n          imagePullPolicy: Always\n          name: media-service\n          ports:\n          - containerPort: 9090\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: media-service-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: media-service-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: media-service\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: media-service\n          name: media-service-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:56:59Z\"\n      lastUpdateTime: \"2026-01-13T07:56:59Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:41Z\"\n      lastUpdateTime: \"2026-01-13T07:56:59Z\"\n      message: ReplicaSet \"media-service-8486767bd\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: nginx-thrift\n    name: nginx-thrift\n    namespace: test-social-network\n    resourceVersion: \"1967\"\n    uid: fa71238a-91a6-418a-bf2f-80f163ca9a4a\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: nginx-thrift\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: nginx-thrift\n          service: nginx-thrift\n      spec:\n        containers:\n        - env:\n          - name: fqdn_suffix\n            value: .test-social-network.svc.cluster.local\n          image: docker.io/yg397/openresty-thrift:xenial\n          imagePullPolicy: Always\n          name: nginx-thrift\n          ports:\n          - containerPort: 8080\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /usr/local/openresty/nginx/jaeger-config.json\n            name: nginx-thrift-config\n            subPath: jaeger-config.json\n          - mountPath: /usr/local/openresty/nginx/conf/nginx.conf\n            name: nginx-thrift-config\n            subPath: nginx.conf\n          - mountPath: /usr/local/openresty/nginx/lua-scripts\n            name: lua-scripts\n          - mountPath: /usr/local/openresty/lualib/thrift\n            name: lua-thrift\n          - mountPath: /usr/local/openresty/nginx/pages\n            name: pages\n          - mountPath: /gen-lua\n            name: gen-lua\n          - mountPath: /keys\n            name: keys\n        dnsPolicy: ClusterFirst\n        hostname: nginx-thrift\n        initContainers:\n        - args:\n          - -c\n          - git clone https://github.com/delimitrou/DeathStarBench.git /DeathStarBench\n            && cp -r /DeathStarBench/socialNetwork/gen-lua/* /gen-lua/ && cp -r /DeathStarBench/socialNetwork/docker/openresty-thrift/lua-thrift/*\n            /lua-thrift/ && cp -r /DeathStarBench/socialNetwork/nginx-web-server/lua-scripts/*\n            /lua-scripts/ && cp -r /DeathStarBench/socialNetwork/nginx-web-server/pages/*\n            /pages/ && cp /DeathStarBench/socialNetwork/keys/* /keys/\n          command:\n          - /bin/sh\n          image: docker.io/alpine/git:latest\n          imagePullPolicy: Always\n          name: alpine-container\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /lua-scripts\n            name: lua-scripts\n          - mountPath: /lua-thrift\n            name: lua-thrift\n          - mountPath: /pages\n            name: pages\n          - mountPath: /gen-lua\n            name: gen-lua\n          - mountPath: /keys\n            name: keys\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: nginx-thrift\n          name: nginx-thrift-config\n        - emptyDir: {}\n          name: lua-scripts\n        - emptyDir: {}\n          name: pages\n        - emptyDir: {}\n          name: gen-lua\n        - emptyDir: {}\n          name: lua-thrift\n        - emptyDir: {}\n          name: keys\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:58:13Z\"\n      lastUpdateTime: \"2026-01-13T07:58:13Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:44Z\"\n      lastUpdateTime: \"2026-01-13T07:58:13Z\"\n      message: ReplicaSet \"nginx-thrift-7fbb6d9b65\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: post-storage-memcached\n    name: post-storage-memcached\n    namespace: test-social-network\n    resourceVersion: \"1600\"\n    uid: a3f67d5c-3dfb-4498-804f-f42715ccb20a\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: post-storage-memcached\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: post-storage-memcached\n          service: post-storage-memcached\n      spec:\n        containers:\n        - image: docker.io/library/memcached:1.6.7\n          imagePullPolicy: Always\n          name: post-storage-memcached\n          ports:\n          - containerPort: 11211\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: post-storage-memcached-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: post-storage-memcached-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: post-storage-memcached\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: post-storage-memcached\n          name: post-storage-memcached-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:56:49Z\"\n      lastUpdateTime: \"2026-01-13T07:56:49Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:41Z\"\n      lastUpdateTime: \"2026-01-13T07:56:49Z\"\n      message: ReplicaSet \"post-storage-memcached-6dcc4b4c66\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: post-storage-mongodb\n    name: post-storage-mongodb\n    namespace: test-social-network\n    resourceVersion: \"1777\"\n    uid: 127b028a-9ef5-4e88-959c-f6038f7a54e2\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: post-storage-mongodb\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: post-storage-mongodb\n          service: post-storage-mongodb\n      spec:\n        containers:\n        - args:\n          - --config\n          - /social-network-microservices/config/mongod.conf\n          image: docker.io/library/mongo:4.4.6\n          imagePullPolicy: Always\n          name: post-storage-mongodb\n          ports:\n          - containerPort: 27017\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/mongod.conf\n            name: post-storage-mongodb-config\n            subPath: mongod.conf\n        dnsPolicy: ClusterFirst\n        hostname: post-storage-mongodb\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: post-storage-mongodb\n          name: post-storage-mongodb-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:10Z\"\n      lastUpdateTime: \"2026-01-13T07:57:10Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:43Z\"\n      lastUpdateTime: \"2026-01-13T07:57:10Z\"\n      message: ReplicaSet \"post-storage-mongodb-6db994db57\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: post-storage-service\n    name: post-storage-service\n    namespace: test-social-network\n    resourceVersion: \"1701\"\n    uid: c9b42131-1ac6-4ac4-b40b-adbeae7aac62\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: post-storage-service\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: post-storage-service\n          service: post-storage-service\n      spec:\n        containers:\n        - command:\n          - PostStorageService\n          image: docker.io/deathstarbench/social-network-microservices:latest\n          imagePullPolicy: Always\n          name: post-storage-service\n          ports:\n          - containerPort: 9090\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: post-storage-service-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: post-storage-service-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: post-storage-service\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: post-storage-service\n          name: post-storage-service-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:05Z\"\n      lastUpdateTime: \"2026-01-13T07:57:05Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:41Z\"\n      lastUpdateTime: \"2026-01-13T07:57:05Z\"\n      message: ReplicaSet \"post-storage-service-5f4d46cbb9\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: social-graph-mongodb\n    name: social-graph-mongodb\n    namespace: test-social-network\n    resourceVersion: \"1836\"\n    uid: da16764b-8ae0-4e3b-bb22-15cb86607ccb\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: social-graph-mongodb\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: social-graph-mongodb\n          service: social-graph-mongodb\n      spec:\n        containers:\n        - args:\n          - --config\n          - /social-network-microservices/config/mongod.conf\n          image: docker.io/library/mongo:4.4.6\n          imagePullPolicy: Always\n          name: social-graph-mongodb\n          ports:\n          - containerPort: 27017\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/mongod.conf\n            name: social-graph-mongodb-config\n            subPath: mongod.conf\n        dnsPolicy: ClusterFirst\n        hostname: social-graph-mongodb\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: social-graph-mongodb\n          name: social-graph-mongodb-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:17Z\"\n      lastUpdateTime: \"2026-01-13T07:57:17Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:44Z\"\n      lastUpdateTime: \"2026-01-13T07:57:17Z\"\n      message: ReplicaSet \"social-graph-mongodb-8485c8fc7\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: social-graph-redis\n    name: social-graph-redis\n    namespace: test-social-network\n    resourceVersion: \"1703\"\n    uid: 4510e446-f54c-436a-8bfe-7efa2fa8cd26\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: social-graph-redis\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: social-graph-redis\n          service: social-graph-redis\n      spec:\n        containers:\n        - args:\n          - /social-network-microservices/config/redis.conf\n          image: docker.io/library/redis:6.2.4\n          imagePullPolicy: Always\n          name: social-graph-redis\n          ports:\n          - containerPort: 6379\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/redis.conf\n            name: social-graph-redis-config\n            subPath: redis.conf\n        dnsPolicy: ClusterFirst\n        hostname: social-graph-redis\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: social-graph-redis\n          name: social-graph-redis-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:05Z\"\n      lastUpdateTime: \"2026-01-13T07:57:05Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:41Z\"\n      lastUpdateTime: \"2026-01-13T07:57:05Z\"\n      message: ReplicaSet \"social-graph-redis-66688cfbd8\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: social-graph-service\n    name: social-graph-service\n    namespace: test-social-network\n    resourceVersion: \"1814\"\n    uid: 0f1a32a0-b4b4-4b1f-bb51-9c202131e257\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: social-graph-service\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: social-graph-service\n          service: social-graph-service\n      spec:\n        containers:\n        - command:\n          - SocialGraphService\n          image: docker.io/deathstarbench/social-network-microservices:latest\n          imagePullPolicy: Always\n          name: social-graph-service\n          ports:\n          - containerPort: 9090\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: social-graph-service-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: social-graph-service-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: social-graph-service\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: social-graph-service\n          name: social-graph-service-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:15Z\"\n      lastUpdateTime: \"2026-01-13T07:57:15Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:43Z\"\n      lastUpdateTime: \"2026-01-13T07:57:15Z\"\n      message: ReplicaSet \"social-graph-service-65b8ddbdbc\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: text-service\n    name: text-service\n    namespace: test-social-network\n    resourceVersion: \"1677\"\n    uid: 0cb17ce3-dd43-471f-8c62-1aa9de73bd79\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: text-service\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: text-service\n          service: text-service\n      spec:\n        containers:\n        - command:\n          - TextService\n          image: docker.io/deathstarbench/social-network-microservices:latest\n          imagePullPolicy: Always\n          name: text-service\n          ports:\n          - containerPort: 9090\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: text-service-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: text-service-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: text-service\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: text-service\n          name: text-service-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:04Z\"\n      lastUpdateTime: \"2026-01-13T07:57:04Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:41Z\"\n      lastUpdateTime: \"2026-01-13T07:57:04Z\"\n      message: ReplicaSet \"text-service-774b6cf48f\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: unique-id-service\n    name: unique-id-service\n    namespace: test-social-network\n    resourceVersion: \"1856\"\n    uid: 212a27f5-4968-4bf4-aa86-77051223d138\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: unique-id-service\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: unique-id-service\n          service: unique-id-service\n      spec:\n        containers:\n        - command:\n          - UniqueIdService\n          image: docker.io/deathstarbench/social-network-microservices:latest\n          imagePullPolicy: Always\n          name: unique-id-service\n          ports:\n          - containerPort: 9090\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: unique-id-service-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: unique-id-service-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: unique-id-service\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: unique-id-service\n          name: unique-id-service-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:20Z\"\n      lastUpdateTime: \"2026-01-13T07:57:20Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:44Z\"\n      lastUpdateTime: \"2026-01-13T07:57:20Z\"\n      message: ReplicaSet \"unique-id-service-fc74bdf74\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: url-shorten-memcached\n    name: url-shorten-memcached\n    namespace: test-social-network\n    resourceVersion: \"1653\"\n    uid: f077440d-de9f-4a6b-854f-a65e2f99dab1\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: url-shorten-memcached\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: url-shorten-memcached\n          service: url-shorten-memcached\n      spec:\n        containers:\n        - image: docker.io/library/memcached:1.6.7\n          imagePullPolicy: Always\n          name: url-shorten-memcached\n          ports:\n          - containerPort: 11211\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: url-shorten-memcached-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: url-shorten-memcached-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: url-shorten-memcached\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: url-shorten-memcached\n          name: url-shorten-memcached-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:00Z\"\n      lastUpdateTime: \"2026-01-13T07:57:00Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:41Z\"\n      lastUpdateTime: \"2026-01-13T07:57:00Z\"\n      message: ReplicaSet \"url-shorten-memcached-566bf679b4\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"2\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 2\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: url-shorten-mongodb\n    name: url-shorten-mongodb\n    namespace: test-social-network\n    resourceVersion: \"7624\"\n    uid: a8d73e88-938e-4143-a10a-16e980399502\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: url-shorten-mongodb\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        annotations:\n          kubectl.kubernetes.io/restartedAt: \"2026-01-13T07:58:50Z\"\n        creationTimestamp: null\n        labels:\n          app: url-shorten-mongodb\n          service: url-shorten-mongodb\n      spec:\n        containers:\n        - args:\n          - --config\n          - /social-network-microservices/config/mongod.conf\n          image: docker.io/library/mongo:4.4.6\n          imagePullPolicy: Always\n          name: url-shorten-mongodb\n          ports:\n          - containerPort: 27017\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/mongod.conf\n            name: url-shorten-mongodb-config\n            subPath: mongod.conf\n          - mountPath: /etc/tls\n            name: tls-cert\n            readOnly: true\n        dnsPolicy: ClusterFirst\n        hostname: url-shorten-mongodb\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: url-shorten-mongodb\n          name: url-shorten-mongodb-config\n        - name: tls-cert\n          secret:\n            defaultMode: 420\n            secretName: mongodb-tls\n  status:\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:56:43Z\"\n      lastUpdateTime: \"2026-01-13T07:59:24Z\"\n      message: ReplicaSet \"url-shorten-mongodb-d57db69c5\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    - lastTransitionTime: \"2026-01-13T08:46:21Z\"\n      lastUpdateTime: \"2026-01-13T08:46:21Z\"\n      message: Deployment does not have minimum availability.\n      reason: MinimumReplicasUnavailable\n      status: \"False\"\n      type: Available\n    observedGeneration: 2\n    replicas: 1\n    unavailableReplicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: url-shorten-service\n    name: url-shorten-service\n    namespace: test-social-network\n    resourceVersion: \"1999\"\n    uid: c45ff475-348c-4882-8760-f127200ac49b\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: url-shorten-service\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: url-shorten-service\n          service: url-shorten-service\n      spec:\n        containers:\n        - command:\n          - UrlShortenService\n          image: docker.io/deathstarbench/social-network-microservices:latest\n          imagePullPolicy: Always\n          name: url-shorten-service\n          ports:\n          - containerPort: 9090\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: url-shorten-service-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: url-shorten-service-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: url-shorten-service\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: url-shorten-service\n          name: url-shorten-service-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:56:41Z\"\n      lastUpdateTime: \"2026-01-13T07:57:00Z\"\n      message: ReplicaSet \"url-shorten-service-598df5754\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    - lastTransitionTime: \"2026-01-13T07:58:17Z\"\n      lastUpdateTime: \"2026-01-13T07:58:17Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: user-memcached\n    name: user-memcached\n    namespace: test-social-network\n    resourceVersion: \"1750\"\n    uid: 1bb1ab7a-b52d-4492-bf6a-76e0c1b8715f\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: user-memcached\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: user-memcached\n          service: user-memcached\n      spec:\n        containers:\n        - image: docker.io/library/memcached:1.6.7\n          imagePullPolicy: Always\n          name: user-memcached\n          ports:\n          - containerPort: 11211\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: user-memcached-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: user-memcached-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: user-memcached\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: user-memcached\n          name: user-memcached-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:08Z\"\n      lastUpdateTime: \"2026-01-13T07:57:08Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:42Z\"\n      lastUpdateTime: \"2026-01-13T07:57:08Z\"\n      message: ReplicaSet \"user-memcached-849988fd5b\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: user-mention-service\n    name: user-mention-service\n    namespace: test-social-network\n    resourceVersion: \"1767\"\n    uid: 47dd2217-40fe-4710-8418-d90e94de82e3\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: user-mention-service\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: user-mention-service\n          service: user-mention-service\n      spec:\n        containers:\n        - command:\n          - UserMentionService\n          image: docker.io/deathstarbench/social-network-microservices:latest\n          imagePullPolicy: Always\n          name: user-mention-service\n          ports:\n          - containerPort: 9090\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: user-mention-service-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: user-mention-service-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: user-mention-service\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: user-mention-service\n          name: user-mention-service-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:09Z\"\n      lastUpdateTime: \"2026-01-13T07:57:09Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:43Z\"\n      lastUpdateTime: \"2026-01-13T07:57:09Z\"\n      message: ReplicaSet \"user-mention-service-59cf6d5b8f\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: user-mongodb\n    name: user-mongodb\n    namespace: test-social-network\n    resourceVersion: \"1720\"\n    uid: e574c227-91b7-43b9-b63d-0d9915d56f82\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: user-mongodb\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: user-mongodb\n          service: user-mongodb\n      spec:\n        containers:\n        - args:\n          - --config\n          - /social-network-microservices/config/mongod.conf\n          image: docker.io/library/mongo:4.4.6\n          imagePullPolicy: Always\n          name: user-mongodb\n          ports:\n          - containerPort: 27017\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/mongod.conf\n            name: user-mongodb-config\n            subPath: mongod.conf\n        dnsPolicy: ClusterFirst\n        hostname: user-mongodb\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: user-mongodb\n          name: user-mongodb-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:06Z\"\n      lastUpdateTime: \"2026-01-13T07:57:06Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:42Z\"\n      lastUpdateTime: \"2026-01-13T07:57:06Z\"\n      message: ReplicaSet \"user-mongodb-5cf7c6cd7b\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: user-service\n    name: user-service\n    namespace: test-social-network\n    resourceVersion: \"1787\"\n    uid: a90e520d-6253-4c97-9bdd-bc8104414eeb\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: user-service\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: user-service\n          service: user-service\n      spec:\n        containers:\n        - command:\n          - UserService\n          image: docker.io/deathstarbench/social-network-microservices:latest\n          imagePullPolicy: Always\n          name: user-service\n          ports:\n          - containerPort: 9090\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: user-service-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: user-service-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: user-service\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: user-service\n          name: user-service-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:11Z\"\n      lastUpdateTime: \"2026-01-13T07:57:11Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:43Z\"\n      lastUpdateTime: \"2026-01-13T07:57:11Z\"\n      message: ReplicaSet \"user-service-5cf866746f\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: user-timeline-mongodb\n    name: user-timeline-mongodb\n    namespace: test-social-network\n    resourceVersion: \"1719\"\n    uid: f2987949-5900-4e9d-91a2-a67cc4d1743e\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: user-timeline-mongodb\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: user-timeline-mongodb\n          service: user-timeline-mongodb\n      spec:\n        containers:\n        - args:\n          - --config\n          - /social-network-microservices/config/mongod.conf\n          image: docker.io/library/mongo:4.4.6\n          imagePullPolicy: Always\n          name: user-timeline-mongodb\n          ports:\n          - containerPort: 27017\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/mongod.conf\n            name: user-timeline-mongodb-config\n            subPath: mongod.conf\n        dnsPolicy: ClusterFirst\n        hostname: user-timeline-mongodb\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: user-timeline-mongodb\n          name: user-timeline-mongodb-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:06Z\"\n      lastUpdateTime: \"2026-01-13T07:57:06Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:42Z\"\n      lastUpdateTime: \"2026-01-13T07:57:06Z\"\n      message: ReplicaSet \"user-timeline-mongodb-c895ffcc8\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: user-timeline-redis\n    name: user-timeline-redis\n    namespace: test-social-network\n    resourceVersion: \"1667\"\n    uid: c7584f2f-002c-4bee-ad31-2eec265d95d2\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: user-timeline-redis\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: user-timeline-redis\n          service: user-timeline-redis\n      spec:\n        containers:\n        - args:\n          - /social-network-microservices/config/redis.conf\n          image: docker.io/library/redis:6.2.4\n          imagePullPolicy: Always\n          name: user-timeline-redis\n          ports:\n          - containerPort: 6379\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/redis.conf\n            name: user-timeline-redis-config\n            subPath: redis.conf\n        dnsPolicy: ClusterFirst\n        hostname: user-timeline-redis\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: user-timeline-redis\n          name: user-timeline-redis-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:03Z\"\n      lastUpdateTime: \"2026-01-13T07:57:03Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:41Z\"\n      lastUpdateTime: \"2026-01-13T07:57:03Z\"\n      message: ReplicaSet \"user-timeline-redis-84dc9b4599\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\n- apiVersion: apps/v1\n  kind: Deployment\n  metadata:\n    annotations:\n      deployment.kubernetes.io/revision: \"1\"\n      meta.helm.sh/release-name: social-network\n      meta.helm.sh/release-namespace: test-social-network\n    creationTimestamp: \"2026-01-13T07:56:41Z\"\n    generation: 1\n    labels:\n      app.kubernetes.io/managed-by: Helm\n      service: user-timeline-service\n    name: user-timeline-service\n    namespace: test-social-network\n    resourceVersion: \"1734\"\n    uid: 368ec845-df99-4690-a64a-1a4923904390\n  spec:\n    progressDeadlineSeconds: 600\n    replicas: 1\n    revisionHistoryLimit: 10\n    selector:\n      matchLabels:\n        service: user-timeline-service\n    strategy:\n      rollingUpdate:\n        maxSurge: 25%\n        maxUnavailable: 25%\n      type: RollingUpdate\n    template:\n      metadata:\n        creationTimestamp: null\n        labels:\n          app: user-timeline-service\n          service: user-timeline-service\n      spec:\n        containers:\n        - command:\n          - UserTimelineService\n          image: docker.io/deathstarbench/social-network-microservices:latest\n          imagePullPolicy: Always\n          name: user-timeline-service\n          ports:\n          - containerPort: 9090\n            protocol: TCP\n          resources: {}\n          terminationMessagePath: /dev/termination-log\n          terminationMessagePolicy: File\n          volumeMounts:\n          - mountPath: /social-network-microservices/config/jaeger-config.yml\n            name: user-timeline-service-config\n            subPath: jaeger-config.yml\n          - mountPath: /social-network-microservices/config/service-config.json\n            name: user-timeline-service-config\n            subPath: service-config.json\n        dnsPolicy: ClusterFirst\n        hostname: user-timeline-service\n        restartPolicy: Always\n        schedulerName: default-scheduler\n        securityContext: {}\n        terminationGracePeriodSeconds: 30\n        volumes:\n        - configMap:\n            defaultMode: 420\n            name: user-timeline-service\n          name: user-timeline-service-config\n  status:\n    availableReplicas: 1\n    conditions:\n    - lastTransitionTime: \"2026-01-13T07:57:07Z\"\n      lastUpdateTime: \"2026-01-13T07:57:07Z\"\n      message: Deployment has minimum availability.\n      reason: MinimumReplicasAvailable\n      status: \"True\"\n      type: Available\n    - lastTransitionTime: \"2026-01-13T07:56:42Z\"\n      lastUpdateTime: \"2026-01-13T07:57:07Z\"\n      message: ReplicaSet \"user-timeline-service-647bd67bfb\" has successfully progressed.\n      reason: NewReplicaSetAvailable\n      status: \"True\"\n      type: Progressing\n    observedGeneration: 1\n    readyReplicas: 1\n    replicas: 1\n    updatedReplicas: 1\nkind: List\nmetadata:\n  resourceVersion: \"\"\n"
        },
        {
          "type": "ToolMessage",
          "content": "apiVersion: v1\nitems: []\nkind: List\nmetadata:\n  resourceVersion: \"\"\n"
        },
        {
          "type": "AIMessage",
          "content": "Hypothesis 2: The url-shorten-service's ConfigMap (service-config.json) contains a MongoDB connection pointing to \"url-shorten-mongodb\" (host/port). I'll fetch the url-shorten-service ConfigMap to check its service-config.json.\n\nThen Hypothesis 3: The ClusterIP Service object url-shorten-mongodb either has mismatched selector/ports or is missing; I'll fetch the Service YAML and its Endpoints to verify selectors and whether pods are selected and ports match the consumer expectation. I'll retrieve the Service and Endpoints next.",
          "tool_calls": [
            {
              "name": "kubectl_get",
              "args": {
                "resourceType": "configmaps",
                "name": "url-shorten-service",
                "namespace": "test-social-network",
                "output": "yaml"
              },
              "id": "call_XPxsi42nJVCsuHUrS7UK4ZRs",
              "type": "tool_call"
            }
          ]
        },
        {
          "type": "ToolMessage",
          "content": "apiVersion: v1\ndata:\n  jaeger-config.yml: |\n    disabled: false\n    reporter:\n      logSpans: false\n      localAgentHostPort: \"jaeger:6831\"\n      queueSize: 1000000\n      bufferFlushInterval: 10\n    sampler:\n      type: \"probabilistic\"\n      param: 0.01\n  service-config.json: |\n    {\n        \"secret\": \"secret\",\n        \"social-graph-service\": {\n          \"addr\": \"social-graph-service\",\n          \"port\": 9090,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"social-graph-mongodb\": {\n          \"addr\": \"social-graph-mongodb\",\n          \"port\": 27017,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"social-graph-redis\": {\n          \"addr\": \"social-graph-redis\",\n          \"port\": 6379,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000,\n          \"use_cluster\": 0,\n          \"use_replica\": 0\n        },\n        \"write-home-timeline-service\": {\n          \"addr\": \"write-home-timeline-service\",\n          \"port\": 9090,\n          \"workers\": 32,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"write-home-timeline-rabbitmq\": {\n          \"addr\": \"write-home-timeline-rabbitmq\",\n          \"port\": 5672,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"home-timeline-redis\": {\n          \"addr\": \"home-timeline-redis\",\n          \"port\": 6379,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000,\n          \"use_cluster\": 0,\n          \"use_replica\": 0\n        },\n        \"compose-post-service\": {\n          \"addr\": \"compose-post-service\",\n          \"port\": 9090,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"compose-post-redis\": {\n          \"addr\": \"compose-post-redis\",\n          \"port\": 6379,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000,\n          \"use_cluster\": 0,\n          \"use_replica\": 0\n        },\n        \"user-timeline-service\": {\n          \"addr\": \"user-timeline-service\",\n          \"port\": 9090,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"user-timeline-mongodb\": {\n          \"addr\": \"user-timeline-mongodb\",\n          \"port\": 27017,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"user-timeline-redis\": {\n          \"addr\": \"user-timeline-redis\",\n          \"port\": 6379,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000,\n          \"use_cluster\": 0,\n          \"use_replica\": 0\n        },\n        \"post-storage-service\": {\n          \"addr\": \"post-storage-service\",\n          \"port\": 9090,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"post-storage-mongodb\": {\n          \"addr\": \"post-storage-mongodb\",\n          \"port\": 27017,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"post-storage-memcached\": {\n          \"addr\": \"post-storage-memcached\",\n          \"port\": 11211,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000,\n          \"binary_protocol\": 1\n        },\n        \"unique-id-service\": {\n          \"addr\": \"unique-id-service\",\n          \"port\": 9090,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000,\n          \"netif\": \"eth0\"\n        },\n        \"media-service\": {\n          \"addr\": \"media-service\",\n          \"port\": 9090,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"media-mongodb\": {\n          \"addr\": \"media-mongodb\",\n          \"port\": 27017,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"media-memcached\": {\n          \"addr\": \"media-memcached\",\n          \"port\": 11211,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000,\n          \"binary_protocol\": 1\n        },\n        \"media-frontend\": {\n          \"addr\": \"media-frontend\",\n          \"port\": 8081,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"text-service\": {\n          \"addr\": \"text-service\",\n          \"port\": 9090,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"user-mention-service\": {\n          \"addr\": \"user-mention-service\",\n          \"port\": 9090,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"url-shorten-service\": {\n          \"addr\": \"url-shorten-service\",\n          \"port\": 9090,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"url-shorten-memcached\": {\n          \"addr\": \"url-shorten-memcached\",\n          \"port\": 11211,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000,\n          \"binary_protocol\": 1\n        },\n        \"url-shorten-mongodb\": {\n          \"addr\": \"url-shorten-mongodb\",\n          \"port\": 27017,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"user-service\": {\n          \"addr\": \"user-service\",\n          \"port\": 9090,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000,\n          \"netif\": \"eth0\"\n        },\n        \"user-memcached\": {\n          \"addr\": \"user-memcached\",\n          \"port\": 11211,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000,\n          \"binary_protocol\": 1\n        },\n        \"user-mongodb\": {\n          \"addr\": \"user-mongodb\",\n          \"port\": 27017,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"home-timeline-service\": {\n          \"addr\": \"home-timeline-service\",\n          \"port\": 9090,\n          \"connections\": 512,\n          \"timeout_ms\": 10000,\n          \"keepalive_ms\": 10000\n        },\n        \"ssl\": {\n          \"enabled\": false,\n          \"caPath\": \"/keys/CA.pem\",\n          \"ciphers\": \"ALL:!ADH:!LOW:!EXP:!MD5:@STRENGTH\",\n          \"serverKeyPath\": \"/keys/server.key\",\n          \"serverCertPath\": \"/keys/server.crt\"\n        },\n        \"redis-primary\": {\n          \"keepalive_ms\": 10000,\n          \"addr\": \"redis-master\",\n          \"timeout_ms\": 10000,\n          \"port\": 6379,\n          \"connections\": 512\n        },\n        \"redis-replica\": {\n          \"keepalive_ms\": 10000,\n          \"addr\": \"redis-replicas\",\n          \"timeout_ms\": 10000,\n          \"port\": 6379,\n          \"connections\": 512\n        }\n      }\nkind: ConfigMap\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: social-network\n    meta.helm.sh/release-namespace: test-social-network\n  creationTimestamp: \"2026-01-13T07:56:41Z\"\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    socialnetwork/service: url-shorten-service\n  name: url-shorten-service\n  namespace: test-social-network\n  resourceVersion: \"1108\"\n  uid: d6fdf523-4c93-4d95-8d66-06f39c5b1fda\n"
        },
        {
          "type": "AIMessage",
          "content": "Hypothesis 4: The ClusterIP Service url-shorten-mongodb exists but its selector or port mapping does not match the url-shorten-mongodb Deployment's pod labels/ports, causing no endpoints. I'll fetch the Service YAML to check its selector and ports.\n\nI'll also fetch the Endpoints object to confirm whether any Pod IPs are selected.",
          "tool_calls": [
            {
              "name": "kubectl_get",
              "args": {
                "resourceType": "svc",
                "name": "url-shorten-mongodb",
                "namespace": "test-social-network",
                "output": "yaml"
              },
              "id": "call_JB7bRbqXLb8PE4hs15P2AjXr",
              "type": "tool_call"
            }
          ]
        },
        {
          "type": "ToolMessage",
          "content": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    meta.helm.sh/release-name: social-network\n    meta.helm.sh/release-namespace: test-social-network\n  creationTimestamp: \"2026-01-13T07:56:41Z\"\n  labels:\n    app.kubernetes.io/managed-by: Helm\n  name: url-shorten-mongodb\n  namespace: test-social-network\n  resourceVersion: \"1204\"\n  uid: 7357f108-9682-4a3c-afa3-9378032136f7\nspec:\n  clusterIP: 10.96.148.4\n  clusterIPs:\n  - 10.96.148.4\n  internalTrafficPolicy: Cluster\n  ipFamilies:\n  - IPv4\n  ipFamilyPolicy: SingleStack\n  ports:\n  - name: \"27017\"\n    port: 27017\n    protocol: TCP\n    targetPort: 27017\n  selector:\n    service: url-shorten-mongodb\n  sessionAffinity: None\n  type: ClusterIP\nstatus:\n  loadBalancer: {}\n"
        },
        {
          "type": "AIMessage",
          "content": "",
          "tool_calls": [
            {
              "name": "kubectl_get",
              "args": {
                "resourceType": "endpoints",
                "name": "url-shorten-mongodb",
                "namespace": "test-social-network",
                "output": "yaml"
              },
              "id": "call_ErZz2dUz6alswcdJ0zVWC8di",
              "type": "tool_call"
            }
          ]
        },
        {
          "type": "ToolMessage",
          "content": "apiVersion: v1\nkind: Endpoints\nmetadata:\n  annotations:\n    endpoints.kubernetes.io/last-change-trigger-time: \"2026-01-13T08:46:21Z\"\n  creationTimestamp: \"2026-01-13T07:56:41Z\"\n  labels:\n    app.kubernetes.io/managed-by: Helm\n  name: url-shorten-mongodb\n  namespace: test-social-network\n  resourceVersion: \"7622\"\n  uid: dc374ddc-8a89-4bd9-af85-5011e58af6cc\nsubsets:\n- notReadyAddresses:\n  - ip: 10.244.1.40\n    nodeName: kind-worker\n    targetRef:\n      kind: Pod\n      name: url-shorten-mongodb-d57db69c5-v22kc\n      namespace: test-social-network\n      uid: a685ed3f-4dfc-4f8f-aaf9-5db4abf9a7fa\n  ports:\n  - name: \"27017\"\n    port: 27017\n    protocol: TCP\n"
        },
        {
          "type": "AIMessage",
          "content": "",
          "tool_calls": [
            {
              "name": "submit_final_diagnosis",
              "args": {
                "diagnosis": "Service exists and matches consumer config, but the mongodb pod backing url-shorten-mongodb is NotReady so the Service has no Ready endpoints \u2014 consumers cannot connect.",
                "reasoning": "1) url-shorten-service ConfigMap references url-shorten-mongodb:27017 as the MongoDB host/port (service-config.json). 2) The url-shorten-mongodb Service exists with selector service=url-shorten-mongodb and port name/port targetPort 27017 \u2014 selector and port match consumer expectations. 3) The url-shorten-mongodb Endpoints show only a notReadyAddresses entry (pod url-shorten-mongodb-d57db69c5-v22kc at 10.244.1.40) and no readyAddresses; the Deployment status shows unavailableReplicas:1 and \"Deployment does not have minimum availability.\" Together these indicate the pod is failing readiness (not Ready) so the Service has no ready endpoints and cannot serve connections."
              },
              "id": "call_48WzOZRQIbLFtOzkNwi2cDN5",
              "type": "tool_call"
            }
          ]
        },
        {
          "type": "ToolMessage",
          "content": "Final diagnosis submitted successfully. Investigation complete."
        }
      ]
    }
  ],
  "tasks_to_be_executed": [],
  "final_report": {
    "root_cause": "The MongoDB process in pod url-shorten-mongodb-d57db69c5-v22kc aborted on startup because it failed SSL certificate validation: the TLS certificate mounted from the kubernetes secret 'mongodb-tls' was expired or otherwise not valid. MongoDB raises a fatal assertion on invalid certificates, exited with Exit Code 14 and entered CrashLoopBackOff. Because the pod never reached Ready, the url-shorten-mongodb Service had no Ready endpoints and consumers (e.g., url-shorten-service) could not connect.",
    "affected_resources": [
      "pod/url-shorten-mongodb-d57db69c5-v22kc",
      "deployment/url-shorten-mongodb",
      "service/url-shorten-mongodb",
      "endpoints/url-shorten-mongodb",
      "configmap/url-shorten-service (service-config.json referencing url-shorten-mongodb:27017)",
      "secret/mongodb-tls",
      "deployment/url-shorten-service (consumer of MongoDB)",
      "namespace/test-social-network"
    ],
    "evidence_summary": "1) Pod url-shorten-mongodb-d57db69c5-v22kc is CrashLoopBackOff with RestartCount=18 and Last State Terminated (Exit Code 14). 2) Container logs contain a reproducible fatal assertion: \"The provided SSL certificate is expired or not yet valid.\" followed by an fassert abort in ssl_manager_openssl.cpp. 3) Pod spec mounts a TLS secret (secretName: mongodb-tls) used by MongoDB at startup. 4) The url-shorten-mongodb Endpoints show only notReadyAddresses (the failing pod) and the Deployment reports unavailableReplicas: 1 and \"Deployment does not have minimum availability.\" 5) No evidence was found for alternate primary causes (no OOMKilled, no imagePull errors, service selector/ports match consumer config). These items together link the expired/invalid TLS cert in secret 'mongodb-tls' to the MongoDB startup abort and downstream service outage.",
    "investigation_summary": "Priority 1 investigation (completed) focused on the failing pod. Actions: kubectl describe pod, collected container logs (including previous), and correlated exitCode and fatal log assertions. Findings: explicit SSL certificate validation failure in logs and Exit Code 14. Priority 2 investigation (completed) checked ClusterIP service and endpoints: Service selector and ports match consumer config but Endpoints contain only notReadyAddresses because the pod never became Ready. Other planned checks (storage, image/startup args, probes) were considered lower priority and were not required because the logs provide a decisive, causal error. The causal chain established: expired/invalid TLS certificate in secret 'mongodb-tls' -> MongoDB aborts on startup (fassert) -> container exits (Exit Code 14) -> pod CrashLoopBackOff and NotReady -> Service has no Ready endpoints -> consumers cannot connect.",
    "detection": true,
    "localization": [
      "secret/mongodb-tls",
      "pod/url-shorten-mongodb-d57db69c5-v22kc"
    ]
  },
  "prompts_config": {
    "triage_agent": "You are an expert Site Reliability Engineer. Your mission is to triage a Kubernetes application by analyzing the provided data.\n\nYour analysis must adhere to the following rules:\n1.  **Focus**: Identify symptoms at the **pod or service level only**. Do not provide cluster-wide analysis or generalizations.\n2.  **Aggregation**: For each pod or service that has issues, create **at most one symptom entry**. Aggregate all related evidence (from pods, metrics, traces) into that single entry.\n3.  **Action**: Synthesize the information to identify and list potential symptoms. For each symptom, pinpoint the affected resource (pod or service) and cite the specific evidence.\n4.  **Resource Naming**: In the `affected_resource` field, provide ONLY the exact resource name without any decorators, prefixes, or namespace qualifiers (e.g., use \"geo-6b4b89b5f5-rsrh7\" NOT \"test-hotel-reservation/geo-6b4b89b5f5-rsrh7\").\n5.  **Trace-Only Evidence**: If error traces are the only signals, still produce symptoms by identifying the service (or pod) that owns the failing span and summarizing the suspected issue using the trace error message. Avoid generic \"trace failed\" statements\u2014make the hypothesis explicit (e.g., \"checkout-service may have invalid credentials because trace X shows `401 Unauthorized` calling payment-service\").\n6.  **Empty State**: If the provided data contains no issues, it is correct to return an empty list of symptoms.",
    "planner_agent": "You are an expert Site Reliability Engineer. Produce a concise, de-duplicated investigation plan where each task inspects a precise part of the infrastructure to surface the most likely root-cause signals and converge on the true RCA quickly.\n\n**Planning Rules**\n1. For every symptom, classify the dominant failure domain (app, latency, dependency/config, or platform) and craft a single, testable hypothesis per resource.\n2. Use the `data_dependencies` and `infra_dependencies` JSON to ground every hypothesis. Merge overlapping symptoms into one task per resource.\n3. **Connections (non-negotiable):** Always create at least one task that inspects the connection between every pair of affected resources or the epicenter and its downstream dependents. These tasks must perform explicit two-sided checks (e.g., verify `service-a`'s config for `service-b`'s URL **and** inspect `service-b`'s Kubernetes service definition for the matching port/name) to catch login/URL/port misconfigurations.\n\n**Tool Selection**\nYou are obligated to fill the field with \"the smallest set of tools required to accomplish the task\"\n\n**Priority Policy**\n- Assign unique priorities (1..N).\n- Priority 1 is the most direct epicenter investigation. Immediately after that, schedule the connection-check tasks described above; treat them as top-tier because they frequently surface misconfigurations.\n- Remaining priorities should rank other high-impact hypotheses (shared dependencies, severe crashes) ahead of narrow or low-scope checks.\n",
    "rca_agent": "You are an expert DevOps engineer performing focused Root Cause Analysis on a Kubernetes service.\n\nInstructions:\n1. You have full autonomy to select the necessary tools from the toolkit. Decide which tools are required to verify your hypotheses efficiently.\n2. For each tool call, first formulate a clear, testable hypothesis about a possible root cause that can be answered by the result. Avoid broad or exploratory queries.\n3. Each tool call must provide unique, non-overlapping information. Never repeat requests with similar parameters or investigate the same aspect repeatedly in slightly different ways.\n4. Stop investigating, even if you have not reached the budget limit, when you have:\n   - Clear evidence that directly identifies a root cause (or definitively rules one out)\n   - Multiple data points indicating the same failure/cause\n   - Sufficient information to answer the investigation goal\n5. DO NOT:\n   - Repeat or re-run tools unless you are testing a truly new and justified hypothesis\n   - Investigate unrelated resources or expand scope beyond the investigation goal\n6. When you have gathered sufficient, non-redundant evidence (typically after 2-3 targeted tool calls), call submit_final_diagnosis with:\n   - diagnosis: State the precise root cause as it pertains to the investigation goal\n   - reasoning: Support your diagnosis by referencing unique findings from your tool calls\n\nREMEMBER: Quality over quantity. Focus on unique and conclusive findings rather than exhaustive or repetitive investigation\n",
    "supervisor_agent": "You are an expert Site Reliability Engineer analyzing RCA findings to determine the root cause of an incident.\n\nAnalyze all symptoms and investigation findings to:\n1. Identify patterns and correlations across findings\n2. Determine the primary root cause\n3. List all affected resources\n4. Summarize key evidence\n\n**Understanding Task Priority:**\nEach RCA investigation task has been assigned a priority level:\n- Priority 1 = Most important / Most likely to reveal root cause (investigated first)\n- Priority 2, 3, ... = Progressively lower importance\n- Tasks with lower priority numbers should generally have higher weight in your analysis\n- The priority reflects the likelihood that investigating that task/resource would uncover the root cause\n\nUse the priority information to contextualize findings: results from priority 1 tasks are typically more significant for determining root cause than lower priority investigations.\n\n**Detection & Localization Requirements:**\n- **detection**: Set to `true` if any problem/anomaly was detected in the cluster based on the evidence. Set to `false` only if no issues are found.\n- **localization**: Provide a list of ONLY the faulty/problematic components (service names or pod names) directly identified as the root cause. This should be a minimal, precise list - extract ONLY the specific resource(s) that caused the incident, not all affected resources.\n  - Example: If a service \"user-service\" has a misconfiguration causing downstream failures, localization = [\"user-service\"]\n  - Example: If a pod \"database-pod-xyz\" is failing, localization = [\"database-pod-xyz\"]\n  - Leave empty/null if the root cause cannot be localized to a specific service or pod.\n\n**Root Cause Expectations:**\n- Build a causal chain that connects symptoms, investigation evidence, and the precise failure mechanism\n- Cite concrete configuration or runtime details (e.g., \"service expects port 5432 but database listens on 5433\") when diagnosing misconfigurations or integration issues\n- If evidence stops at the symptom level, identify the missing proof and pursue it before finalizing\n\n**Strict Iteration Policy:**\nOnly request another RCA iteration when the existing evidence is insufficient to produce a confident final diagnosis. Never re-run or re-request tasks already marked as completed or currently in progress. When additional work is unavoidable, list only the minimal set of pending task priorities in `tasks_to_be_executed` and clearly justify why each is required. If the current findings support a solid root cause, leave `tasks_to_be_executed` empty and finalize the report.\n\nWhen you need more evidence, ask for the most targeted pending tasks that can close the causal gap (for example, verifying port mappings, credentials, or configuration values at both ends of a failing connection).\n\nProvide a clear, specific root cause statement that explains what caused the incident and why it happened now."
  },
  "experiment_name": "F - Tool-free planning React P2 B7 - Social Network - MongoDB url-shorten authentication missing (hypothesis-test-hotel-res)",
  "agent_id": "F",
  "agent_configuration_name": "F - Tool-free planning React P2 B7",
  "stats": {
    "run_id": "f9919f79-8687-45c7-a390-3e6e0621554f",
    "experiment_name": "F - Tool-free planning React P2 B7 - Social Network - MongoDB url-shorten authentication missing (hypothesis-test-hotel-res)",
    "status": "success",
    "execution_time_seconds": 216.005339,
    "total_tokens": 154296,
    "total_cost": 0.02911,
    "langsmith_url": "https://smith.langchain.com/o/abfc6d67-41f7-486b-956b-7589404f3bd1/projects/p/2264bd80-45a8-4ab7-b5b3-23b64980ca6f/r/f9919f79-8687-45c7-a390-3e6e0621554f?trace_id=f9919f79-8687-45c7-a390-3e6e0621554f&start_time=2026-01-13T09:08:03.538275",
    "agent_stats": {
      "supervisor_agent": {
        "total_tokens": 5498,
        "input_tokens": 3922,
        "output_tokens": 1576,
        "cost": 0.003152,
        "runs_count": 1
      },
      "rca_agent": {
        "total_tokens": 142477,
        "input_tokens": 133107,
        "output_tokens": 9370,
        "cost": 0.01874,
        "runs_count": 2
      },
      "schedule_rca_tasks": {
        "total_tokens": 0,
        "input_tokens": 0,
        "output_tokens": 0,
        "cost": 0.0,
        "runs_count": 1
      },
      "planner_agent": {
        "total_tokens": 4954,
        "input_tokens": 1829,
        "output_tokens": 3125,
        "cost": 0.00625,
        "runs_count": 1
      },
      "triage_agent": {
        "total_tokens": 1367,
        "input_tokens": 883,
        "output_tokens": 484,
        "cost": 0.000968,
        "runs_count": 1
      }
    }
  },
  "testbed": {
    "application_name": [
      "Social Network"
    ],
    "fault_name": "MongoDB url-shorten authentication missing",
    "target_namespace": "test-social-network",
    "trace_service_starting_point": "nginx-thrift",
    "rca_tasks_per_iteration": "2",
    "max_tool_calls": "7"
  },
  "evaluation": {
    "detection": true,
    "localization": true,
    "rca_score": 2,
    "rca_motivation": "The agent correctly focuses on the url-shorten-mongodb component in the correct namespace and notes it is failing to become Ready, but the diagnosed root cause (invalid/expired TLS certificate causing MongoDB startup failure) is unrelated to the actual injected issue (missing/disabled MongoDB authentication configuration). This is the right component with a wrong, unrelated root cause."
  }
}